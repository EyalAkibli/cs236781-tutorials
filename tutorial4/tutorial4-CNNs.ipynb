{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "# CS236605: Deep Learning\n",
    "# Tutorial 4: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- Convolutional layers\n",
    "- Pooling layers\n",
    "- Network architecture\n",
    "- Spatial classification with fully-convolutional nets\n",
    "- Residual nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Model\n",
    "![img](https://qph.fs.quoracdn.net/main-qimg-330e8b2941bc0164211bbdc7d5c693f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Composed of multiple **layers**.\n",
    "\n",
    "Each layer $j$ consists of $n_j$ regular perceptrons (\"neurons\") which calculate:\n",
    "$$\n",
    "\\vec{y}_j = \\varphi\\left( \\mat{W}_j \\vec{y}_{j-1} + \\vec{b}_j \\right),~\n",
    "\\mat{W}_j\\in\\set{R}^{n_{j}\\times n_{j-1}},~ \\vec{b}_j\\in\\set{R}^{n_j}.\n",
    "$$\n",
    "\n",
    "- Note that both input and output are **vetors**. We can think of the above equation as describing a layer of **multiple perceptrons**.\n",
    "- We'll henceforth refer to such layers as **fully-connected** or FC layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Given an input sample $\\vec{x}^i$, the computed function of an $L$-layer MLP is:\n",
    "$$\n",
    "\\vec{y}_L^i= \\varphi \\left(\n",
    "\\mat{W}_L \\varphi \\left( \\cdots\n",
    "\\varphi \\left( \\mat{W}_1 \\vec{x}^i + \\vec{b}_1 \\right)\n",
    "\\cdots \\right)\n",
    "+ \\vec{b}_L \\right)\n",
    "$$\n",
    "\n",
    "- Universal approximator theorem: an MLP with $L>1$, can approximate (almost) any function given enough parameters (Cybenko, 1989).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Regression\n",
    "\n",
    "<img src=\"img/regression.png\" alt=\"regression\" width=\"600\"/>\n",
    "\n",
    "- Output: $\\hat{\\vec{y}^i} = \\vec{y}^i_L$\n",
    "- Quadratic loss: $\\sum_i (\\vec{y}^i - \\hat{\\vec{y}^i})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Classification\n",
    "\n",
    "<img src=\"img/classification.png\" width=\"900\" alt=\"classification\">\n",
    "\n",
    "- Output: $\\hat{\\vec{y}^i} = \\mathrm{softmax}(\\vec{y}^i_L)$ (class probabilities)\n",
    "- Cross entropy loss: $\\sum_i - {\\vectr{y}}^i \\log(\\hat{\\vec{y}^i})$\n",
    "\n",
    "To explore ConvNets we'll now be focusing our attention mainly on the task of classifying images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Limitations of MLPs for image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Number of parameters increases quadratically with image size due to connectivity.\n",
    "    - 28x28 MNIST image: 784 weights per neuron in the first layer\n",
    "    - 1000x1000x3 color image: 3M weights **per neuron**\n",
    "    \n",
    "    <img src=\"img/vanilla_dnn_scale.png\" width=\"700\" alt=\"scale\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Huge number of parameters greatly increases risk of overfitting\n",
    "    - MLP with 1 hidden layer, 3, 6 and 20 Neurons\n",
    "    \n",
    "    <img src=\"img/overfit_1HL_3-6-20N.jpg\" width=\"600\" alt=\"overfit1\">\n",
    "    \n",
    "    - MLP with 1, 2 and 4 hidden layers, 3 neurons each\n",
    "    \n",
    "    <img src=\"img/overfit_1-2-4HL_3N.jpg\" width=\"600\" alt=\"overfit1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- FC layers are highly sensitivity to translation, while image features are inherently translation-invariant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Despite all these limitations we still want to use deep netural nets because they allow us to **learn hierarchical features** from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Structural view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A convolutional layer is similar to an MLP FC layer but with three improtant distinctions:\n",
    "1. Each neuron is only **connected to a small region** of the previous layer's output.\n",
    "1. The neurons are stacked in a **3D** grid (insead of 1D).\n",
    "1. Neurons that are at the same depth in the grid **share the same weights** (parameters $\\mat{W},~\\vec{b}$).\n",
    "\n",
    "<img src=\"img/cnn_layer.jpeg\" width=\"400\"/>\n",
    "\n",
    "In the above image, the colors of the neurons represent their weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two important things to understand about convolutional layers:\n",
    "- They operate on and produce **volumes** (3D tensors).\n",
    "\n",
    "   <img src=\"img/cnn_layers.jpeg\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Each neuron is spatially local, but operates on the **full depth** dimension of it's input layer.\n",
    "\n",
    "   <img src=\"img/depthcol.jpeg\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Filter-based view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since each neuron in a given depth-slice of operates on a small region of the input layer, we can think of the combined **output of that depth-slice** as the **convolution between a filter and the input volume**.\n",
    "\n",
    "<img src=\"img/cnn_filters.png\" width=\"700\" />\n",
    "<img src=\"img/filter_resp.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since we have multiple depth-slices per convolutional layer, the layer computes multiple convolutions of the same input with different kernels (filters).\n",
    "\n",
    "Each 2D slice of an input and output volume is known as **feature map** or a **channel**.\n",
    "\n",
    "[Visualization of a convolutional filter](http://cs231n.github.io/assets/conv-demo/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Hyperparameters & dimentions\n",
    "\n",
    "Assume an input volume of shape $(C_{\\mathrm{in}}, H_{\\mathrm{in}}, W_{\\mathrm{in}})$, i.e. channels, height, width.\n",
    "Define,\n",
    "\n",
    "1. Number of kernels, $K \\geq 1$.\n",
    "2. Spatial extent (size) of each kernel, $F \\geq 1$. \n",
    "3. Stride $S\\geq 1$: spatial distance between consecutive applications of a kernel.\n",
    "4. Padding $P\\geq 0$: Number of \"pixels\" to zero-pad around each input feature map.\n",
    "5. Dilation $D \\geq 1$: Spacing between kernel elements when applying to input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the following animations, **blue** maps are inputs,\n",
    "**green** maps are outputs and\n",
    "the **shaded** area is the kernel with $F=3$.\n",
    "\n",
    "| $P=0,~S=1,~D=1$ | $P=1,~S=1,~D=1$ | $P=1,~S=2,~D=1$ | $P=0,~S=1,~D=2$ |\n",
    "|-----------------|-----------------|-----------------| --------------- |\n",
    "|<img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif\" width=\"250\"/>| <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides.gif\" width=\"250\"/> | <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides.gif\" width=\"250\"/> | <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif\" width=\"250\"/> |\n",
    "\n",
    "\n",
    "We can see that the second combination, $F=3,~P=1,~S=1,~D=1$, leads to identical sizes of input and output feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then, given a set of hyperparameters,\n",
    "\n",
    "- Each convolution kernel will be a tensor of shape $(C_{\\mathrm{in}}, F, F)$.\n",
    "- The ouput volume dimensions will be (ignoring dilation):\n",
    "\n",
    "  $$\\begin{align}\n",
    "  H_{\\mathrm{out}} &= \\left\\lfloor \\frac{H_{\\mathrm{in}} + 2P - D\\cdot(F-1) -1}{S} \\right\\rfloor + 1\\\\\n",
    "  W_{\\mathrm{out}} &= \\left\\lfloor \\frac{W_{\\mathrm{in}} + 2P - D\\cdot(F-1) -1}{S} \\right\\rfloor + 1\\\\\n",
    "  C_{\\mathrm{out}} &= K\\\\\n",
    "  \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The number of parameters in the convolutional layer will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\underbrace{K}_{\\mathrm{kernels}} \\cdot \\left(\n",
    "\\underbrace{C_{\\mathrm{in}} \\cdot F^2}_{\\mathrm{kernel\\ size}} + \\underbrace{1}_{\\mathrm{bias\\ term}}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Example**: Input image is 1000x1000x3, and the first conv layer has $10$ kernels of size 5x5.\n",
    "The number of parameters in the first layer will be: $ 10 \\cdot 3 \\cdot 5^2 + 10 = 760 $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pytorch `Conv2d` layer example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as tvtf\n",
    "\n",
    "tf = tvtf.Compose([tvtf.ToTensor()])\n",
    "ds_cifar10 = torchvision.datasets.CIFAR10(data_dir, download=True, train=True, transform=tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 shape with batch dim: torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Load first CIFAR10 image\n",
    "x0,y0 = ds_cifar10[0]\n",
    "\n",
    "# add batch dimension\n",
    "x0 = x0.unsqueeze(0)\n",
    "\n",
    "print('x0 shape with batch dim:', x0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# A function to count the number of parameters in an nn.Module.\n",
    "def num_params(layer):\n",
    "    return sum([p.numel() for p in layer.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: 280 parameters\n",
      "Input image shape:       torch.Size([1, 3, 32, 32])\n",
      "After first conv layer:  torch.Size([1, 10, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# First conv layer: works on input image volume\n",
    "conv1 = nn.Conv2d(in_channels=x0.size(1), out_channels=10, padding=1, kernel_size=3, stride=1)\n",
    "\n",
    "print(f'conv1: {num_params(conv1)} parameters')\n",
    "print(f'{\"Input image shape:\":25s}{x0.shape}')\n",
    "print(f'{\"After first conv layer:\":25s}{conv1(x0).shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2: 7220 parameters\n",
      "After second conv layer: torch.Size([1, 20, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "# Second conv layer: works on output volume of first layer\n",
    "conv2 = nn.Conv2d(in_channels=10, out_channels=20, padding=0, kernel_size=6, stride=2)\n",
    "\n",
    "print(f'conv2: {num_params(conv2)} parameters')\n",
    "print(f'{\"After second conv layer:\":25s}{conv2(conv1(x0)).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Note**: observe that the width and height dimensions of the input image were never specified!\n",
    "more on the significance of that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In addition to strides, another way to reduce the size of feature maps between the convolutional layers,\n",
    "is by adding **pooling** layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A pooling layer has the following hyperparameters (but **no trainable parameters**):\n",
    "\n",
    "1. Spatial extent (size) of each pooling kernel, $F \\geq 2$. \n",
    "1. Stride $S\\geq 2$: spatial distance between consecutive applications.\n",
    "1. Operation (e.g. max, average, $p$-norm)\n",
    "\n",
    "**Example**: $\\max$-pooling with $F=2,~S=2$ performing a factor-2 downsample:\n",
    "\n",
    "<img src=\"img/maxpool.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why downsample feature maps after convolutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Obviously, to reduce the number of features to process in the next layer.\n",
    "\n",
    "But more crucially,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- To increase the **receptive field** of the original image that each layer works with.\n",
    "- We want successive conv layers to be affected by increasingly larger parts of the input image.\n",
    "- This allows us to learn a hierarchy of visual features.\n",
    "\n",
    "<img src=\"img/feature_hierarchy.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PyTorch `Pool2d` layer example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After second conv layer: torch.Size([1, 20, 14, 14])\n",
      "After max-pool:          torch.Size([1, 20, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "print(f'{\"After second conv layer:\":25s}{conv2(conv1(x0)).shape}')\n",
    "print(f'{\"After max-pool:\":25s}{pool(conv2(conv1(x0))).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The basic way to build an architecture of a deep convolutional neural net, is to repeat groups of **conv-relu** layers, sprinkle in some **pooling** in between and top it all off with a nice **FC-softmax** combo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/arch.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the above image,\n",
    "\n",
    "- all the **conv** blocks are actually **conv-relu** (or some other nonlinearity).\n",
    "- The rightmost architecture is called VGG, and used to be a relevant architecture for ImageNet classification.\n",
    "- Other types of layers, such as normalization layers are usually also added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are many other things to consider as part of the architecture:\n",
    "- Size of conv kernels\n",
    "- Number of consecutive convolutions\n",
    "- Use of batch norm to speed up training\n",
    "- Dropout for improved generalization\n",
    "- Pointwise convolutions instead of FC layers\n",
    "- Skip connections (we'll see later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many different network architectures exist, made famous mainly by repeated improvements on the ImageNet classification challenge since 2012.\n",
    "\n",
    "<img src=\"img/net_archs.png\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notable ImageNet-winning architectures:\n",
    "\n",
    "- AlexNet, 5 layers (2012): Based on LeNet, deeper, with ReLU, trained with GPUs\n",
    "- Inception/GoogLeNet, 22 layers (2014): Multiple (small) kernel sizes at same depth\n",
    "- ResNet, 152 (!) layers (2015): Skip connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PyTorch network architecture example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's implement **LeNet**, arguably the first successful CNN model for MNIST (LeCun, 1998).\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*1TI1aGBZ4dybR6__DI9dzA.png\" width=\"1000\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        class_scores = self.classifier(features)\n",
    "        return class_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "LeNet(x0)= tensor([[-0.0121, -0.0345,  0.0950,  0.0754,  0.0334,  0.0507,  0.0379,  0.0738,\n",
      "          0.1052, -0.0774]], grad_fn=<AddmmBackward>)\n",
      "shape= torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "net = LeNet()\n",
    "print(net)\n",
    "print('LeNet(x0)=', net(x0))\n",
    "print('shape=', net(x0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fully-convolutional Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notice how we never actually specified the input image size when implementing the network.\n",
    "\n",
    "**Does this mean we can use the network on images of any size**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**No**, because of the FC layers at the end.\n",
    "\n",
    "Here, let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "size mismatch, m1: [1 x 2704], m2: [400 x 120] at /Users/soumith/mc3build/conda-bld/pytorch_1549597882250/work/aten/src/TH/generic/THTensorMath.cpp:940\n"
     ]
    }
   ],
   "source": [
    "large_image = torch.randn(1,3,32*2,32*2)\n",
    "try:\n",
    "    net(large_image)\n",
    "except RuntimeError as e:\n",
    "    print(e, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However,\n",
    "- Only the FC layers at the end require actual knowledge of exact image sizes.\n",
    "- We can replace them with... Convolutions, of course\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class LeNetFullyConv(LeNet):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Remember: the last feature map volume has shape (16,5,5)\n",
    "        # Override the classifier with 5x5 then 1x1 convolutions\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(16, 120, 5), # note: no padding or strides!\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(120, 84, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(84, 10, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        # note: no need to reshape the features now\n",
    "        class_scores = self.classifier(features)\n",
    "        return class_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNetFullyConv(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(120, 84, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(84, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net_fully_conv = LeNetFullyConv()\n",
    "print(net_fully_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's forward the original-sized image and the larger image through the network and observe the output shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular image output shape: torch.Size([1, 10, 1, 1])\n",
      "large   image output shape: torch.Size([1, 10, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "print('regular image output shape:', net_fully_conv(x0).shape)\n",
    "print('large   image output shape:', net_fully_conv(large_image).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**What's the meaning of the output after conversion to fully convolutional?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It's now a **spatial classification map**.\n",
    "\n",
    "<img src=\"img/fully_conv.png\" width=\"800\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Residual Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- For image-related tasks it seems that **deeper is better**: learn more complex features\n",
    "    <img src=\"img/deeper_meme.jpeg\"/>\n",
    "\n",
    "- In practice there are two major problems with adding depth:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. More difficult convergence: vanishing gradients\n",
    "1. More difficult optimization: parameter space increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In theory, adding an addition layer should provide **at least** the same accuracy as before, since it could always just be an identity map.\n",
    "\n",
    "In practice, not so:\n",
    "\n",
    "<img src=\"img/resnet_plain_deep_error.png\" width=\"800\"/>\n",
    "\n",
    "I.e., even if the same solution (or better) exists, SGD optimization can't find it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "ResNets attempt to address these issues by building a network architecture composed of convolutional blocks with added **shortcut-connections**:\n",
    "\n",
    "<img src=\"img/resnet_block.png\" width=\"400\"/>\n",
    "\n",
    "Here the weight layers are `3x3` or `1x1` convolutions followed by batch-normalization.\n",
    "These shortcuts create two key advantages:\n",
    "- Allow gradients to flow freely backwards\n",
    "- Each block only learns the \"residual mapping\", i.e. some delta from the identity map which is easier to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### PyTorch ResNet example\n",
    "\n",
    "Let's implement the smallest resifual network from the original paper, ResNet18.\n",
    "\n",
    "<img src=\"img/resnet_arch_table.png\" width=\"1000\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, first_stride=1):\n",
    "        super().__init__()\n",
    "        self.main_path = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=first_stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.shortcut_path = nn.Sequential()\n",
    "        # Check if spatial or channel dimentions changed along main path\n",
    "        # If so, we need to adjust the dimensions of the shortcut connection\n",
    "        if first_stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut_path = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=first_stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = self.main_path(x)\n",
    "        out += self.shortcut_path(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.input_group = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        # Add 4 groups of 2 blocks, each group adding more output channels\n",
    "        in_channels_curr = 64\n",
    "        out_channels_per_group = [64, 128, 256, 512]\n",
    "        for group_idx, out_channels in enumerate(out_channels_per_group):\n",
    "            first_stride = 1 if group_idx == 0 else 2\n",
    "            group = nn.Sequential(\n",
    "                ResNetBlock(in_channels_curr, out_channels, first_stride),\n",
    "                ResNetBlock(out_channels, out_channels, 1),\n",
    "            )\n",
    "            in_channels_curr = out_channels\n",
    "            setattr(self, f'group{group_idx}', group)\n",
    "            \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # kernel is adaptive s.t. 1x1 is output size\n",
    "        self.fc = nn.Linear(out_channels_per_group[-1], num_classes) \n",
    "    def forward(self, x):\n",
    "        out = self.input_group(x)\n",
    "        for group_idx in range(4):\n",
    "            group = getattr(self, f'group{group_idx}')\n",
    "            out = group(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet18(\n",
       "  (input_group): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (group0): Sequential(\n",
       "    (0): ResNetBlock(\n",
       "      (main_path): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut_path): Sequential()\n",
       "    )\n",
       "    (1): ResNetBlock(\n",
       "      (main_path): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut_path): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (group1): Sequential(\n",
       "    (0): ResNetBlock(\n",
       "      (main_path): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut_path): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResNetBlock(\n",
       "      (main_path): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut_path): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (group2): Sequential(\n",
       "    (0): ResNetBlock(\n",
       "      (main_path): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut_path): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResNetBlock(\n",
       "      (main_path): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut_path): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (group3): Sequential(\n",
       "    (0): ResNetBlock(\n",
       "      (main_path): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut_path): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResNetBlock(\n",
       "      (main_path): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut_path): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = ResNetBlock(in_channels=3, out_channels=64, first_stride=2)\n",
    "net = ResNet18(num_classes=10)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First block output shape:  torch.Size([1, 64, 112, 112])\n",
      "ResNet output shape:  torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "test_image = torch.randn(1,3,224,224)\n",
    "\n",
    "print('First block output shape: ', block(test_image).shape)\n",
    "print('ResNet output shape: ', net(test_image).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Image credits**\n",
    "\n",
    "Images in this tutorial were taken and/or adapted from:\n",
    "\n",
    "- Sebastian Raschka, https://sebastianraschka.com/\n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017\n",
    "- Deep Learning with Python, Francios Chollet, Manning 2018\n",
    "- Stanford cs231n course notes by Andrej Karpathy\n",
    "- https://github.com/vdumoulin/conv_arithmetic\n",
    "- Long, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition.\n",
    "- Canziani, A., Paszke, A., & Culurciello, E. (2016). An analysis of deep neural network models for practical applications.\n",
    "- He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
