{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\Tr}[0]{^\\top}\n",
    "\\newcommand{\\softmax}[1]{\\mathrm{softmax}\\left({#1}\\right)}\n",
    "$$\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "# Tutorial 7: Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- Sequence to sequence models for machine translation\n",
    "- Attention mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the context of learning from **sequences** of inputs, we have seen RNNs as a model capable of learning a transformation of one sequence into another.\n",
    "\n",
    "<center><img src=\"img/rnn_unrolled.png\" width=\"1000\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\forall t \\geq 0:\\\\\n",
    "\\vec{h}_t &= \\varphi_h\\left( \\mat{W}_{hh} \\vec{h}_{t-1} + \\mat{W}_{xh} \\vec{x}_t + \\vec{b}_h\\right) \\\\\n",
    "\\vec{y}_t &= \\varphi_y\\left(\\mat{W}_{hy}\\vec{h}_t + \\vec{b}_y \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, RNNs (even the fancy ones) have some major drawbacks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Input must be processed sequentially.\n",
    "2. Hard to train on long sequences (needs BPTT).\n",
    "3. Difficult to learn long-term dependencies, e.g. between late outputs and early inputs. The **hidden state** has the burden of \"remembering\" the \"meaning\" of the entire sequence so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over time \"fancy\" versions of RNNs became popular, mainly LSTMs and more recently **GRU**s:\n",
    "\n",
    "<center><img src=\"img/GRU.png\" width=800 /></center>\n",
    "\n",
    "The main idea is to have \"soft-gates\" ($\\vec{r}_t$ and $\\vec{z}_t$) that control how much of the previous state ($\\vec{h}_{t-1}$) affects the next state $\\vec{h}_t$ relative to the proposed next state $\\tilde{\\vec{h}}_{t}$.\n",
    "\n",
    "In practice these model can be trained more effectively on longer sequences and therefore can model longer dependencies compared to vanilla RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-decoder architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common architecture used in many tasks is the encoder-decoder pattern.\n",
    "\n",
    "- The **encoder** maps the input to some latent representation, usually of a low dimension.\n",
    "- The **decoder** applies a different mapping, from the latent space to some other space (sometimes back to the input space)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/enc_dec.png\" width=700 /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some common use cases are e.g.\n",
    "- Nonlinear dimentionality reduction: Autoencoders\n",
    "- Generative models: VAEs, GANs\n",
    "- Machine translation: Seq2Seq models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning contexts, **attention** is a term used for a family of related mechanisms which, in general, learn to predict some probability distribution over a sequence of elements.\n",
    "\n",
    "Intuitively, this allows a model to \"pay more attention\" to elements from the sequence which get a higher probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent versions of attention mechanisms can be defined formally as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given:\n",
    "- $n$ **key-value** pairs: $\\left\\{\\left(\\vec{k}_i, \\vec{v}_i\\right)\\right\\}_{i=1}^{n}$, where $\\vec{k}_i\\in\\set{R}^{d_k}$, $\\vec{v}_i\\in\\set{R}^{d_v}$\n",
    "- A **query**, $\\vec{q} \\in\\set{R}^{d_q}$\n",
    "- Some similarity (sometimes called *energy*) function between keys and queries, $s: \\set{R}^{d_k}\\times \\set{R}^{d_q} \\mapsto \\set{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *soft*-attention mechanism computes a weighted sum of the **values**,\n",
    "\n",
    "$$\n",
    "\\vec{o} = \\sum_{i=1}^{n} a_i \\vec{v}_i\\ \\in \\set{R}^{d_v},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where attention weights $a_i$ are computed according the the similarity between the **query** and each **key**:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "b_i &= s(\\vec{k}_i, \\vec{q}) \\\\\n",
    "\\vec{b} &= \\left[  b_1, \\dots, b_n \\right]\\Tr \\\\\n",
    "\\vec{a} &= \\softmax{\\vec{b}}.\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to define *hard*-attention by using the weights $\\vec{a}$ as a discrete distribution over the values, an sample a single $\\vec{v}_i$ from this distribution:\n",
    "\n",
    "$$\n",
    "\\vec{o} \\sim \\mathrm{Multinoulli}\\left(\\vec{a}; \\left\\{\\vec{v}_i\\right\\}_{i=1}^{n}\\right)\n",
    "$$\n",
    "\n",
    "In this case the attention output is stochastic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A famous example of attention from an image-captioning paper ([Xu et al. 2015](http://proceedings.mlr.press/v37/xuc15.pdf)):\n",
    "<center><img src=\"img/xu2015_1.png\" width=\"1100\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core ideas of this model:\n",
    "- Use pre-trained CNN  to extract `14x14` 512-dimensional convolutional feature maps from an image: \"location annotations\"\n",
    "- Train LSTM to generate image caption word by word\n",
    "- Learn to apply attention to the annotations (keys/values) based on LSTM hidden state (query)\n",
    "- Include the attention-weighted annotations in the next hidden state\n",
    "\n",
    "<center><img src=\"img/xu2015_2.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplicative attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One basic type of attention mechanism uses a simple **dot product** as the similarity function.\n",
    "\n",
    "Widely-used by models based on the **Transformer** architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume $d_k=d_q=d$, then\n",
    "\n",
    "$$\n",
    "s(\\vec{k},\\vec{q})= \\frac{\\vectr{k}\\vec{q}}{\\sqrt{d}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why scale by $\\sqrt{d}$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the factor at which the dot-product grows due to the dimensionality. E.g.,\n",
    "\n",
    "$$\n",
    "\\norm{\\vec{1}_d}_2 = \\norm{[1,\\dots,1]\\Tr}_2 = \\sqrt{d\\cdot 1^2} =\\sqrt{d}.\n",
    "$$\n",
    "\n",
    "This helps keep the softmax values from becoming very small when the dimension is large, and therefore helps prevent tiny gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now deal with $m$ queries simultaneously by stacking them in a matrix $\\mat{Q} \\in \\set{R}^{m\\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we'll stack the keys and values in their own matrices, $\\mat{K}\\in\\set{R}^{n\\times d}$, $\\mat{V}\\in\\set{R}^{n\\times d_v}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can compute the attention weights for all queries in parallel:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mat{B} &= \\frac{1}{\\sqrt{d}} \\mat{Q}\\mattr{K}  \\ \\in\\set{R}^{m\\times n} \\\\\n",
    "\\mat{A} &= \\softmax{\\mat{B}},\\ \\mathrm{dim}=1 \\\\\n",
    "\\mat{O} &= \\mat{A}\\mat{V} \\ \\in\\set{R}^{m\\times d_v}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the softmax is applied per-row, and so each row $i$ of $\\mat{A}$ contains the attention weights for the $i$th query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also notice that in this formulation, we **input a sequence** of $m$ queries and get an **output sequence** of $m$ weighed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common type of attention mechanism uses an MLP to **learn** the similarity function $s(\\vec{k},\\vec{q})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this type of attention, the similarity function is \n",
    "\n",
    "$$\n",
    "s(\\vec{k},\\vec{q}) = \\vectr{v} \\tanh(\\mat{W}_k\\vec{k} + \\mat{W}_q\\vec{q}),\n",
    "$$\n",
    "\n",
    "where $\\mat{W}_k\\in\\set{R}^{h\\times d_k}$, $\\mat{W}_q\\in\\set{R}^{h\\times d_q}$ and $\\vec{v}\\in\\set{R}^{h}$ are trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice that we're adding projected versions of the key and query and applying a 2-layer MLP.\n",
    "- Both projections and the output layer are trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention refers to applying attention on a **single sequence**  $\\left\\{\\vec{x}_i\\right\\}_{i=1}^{n}$ of elements.\n",
    "\n",
    "The keys, values and queries are either:\n",
    "- The elements themselves\n",
    "- Computed from them with **learned** linear projections, i.e.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\vec{q}_{i} &= \\mat{W}_{xq}\\vec{x}_{i} &\n",
    "\\vec{k}_{i} &= \\mat{W}_{xk}\\vec{x}_{i} &\n",
    "\\vec{v}_{i} &= \\mat{W}_{xv}\\vec{x}_{i} \n",
    "\\end{align*}\n",
    "$$\n",
    "This is what Transformer models do for Seq2Seq, instead of using RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/self_attn_transformer.svg\" width=600></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Sequence-to-sequence machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we'll translate text from German to English.\n",
    "\n",
    "The general approach using RNNs is to design a Sequence-to-sequence (**Seq2Seq**) Encoder-Decoder architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/seq2seq1.png\" width=\"900\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field\n",
    "\n",
    "# Common args for field objects\n",
    "field_args = dict(tokenize='spacy',\n",
    "                  init_token='<sos>',\n",
    "                  eos_token='<eos>',\n",
    "                  include_lengths=True,\n",
    "                  lower=True) \n",
    "\n",
    "# Field for processing German source\n",
    "src_field = Field(tokenizer_language=\"de_core_news_sm\", **field_args)\n",
    "\n",
    "# Field for processing English target\n",
    "tgt_field = Field(tokenizer_language=\"en_core_web_sm\", **field_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_valid, ds_test = Multi30k.splits(\n",
    "    root=data_dir, exts=('.de', '.en'), fields=(src_field, tgt_field)\n",
    ")\n",
    "\n",
    "VOCAB_MIN_FREQ = 2\n",
    "src_field.build_vocab(ds_train, min_freq=VOCAB_MIN_FREQ)\n",
    "tgt_field.build_vocab(ds_train, min_freq=VOCAB_MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source vocab size: 7855\n",
      "target vocab size: 5893\n"
     ]
    }
   ],
   "source": [
    "V_src = len(src_field.vocab)\n",
    "print(f'source vocab size: {V_src}')\n",
    "\n",
    "V_tgt = len(tgt_field.vocab)\n",
    "print(f'target vocab size: {V_tgt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample#0001:\n",
      "\tDE: mehrere männer mit schutzhelmen bedienen ein antriebsradsystem .\n",
      "\tEN: several men in hard hats are operating a giant pulley system .\n",
      "\n",
      "sample#0010:\n",
      "\tDE: eine ballettklasse mit fünf mädchen , die nacheinander springen .\n",
      "\tEN: a ballet class of five girls jumping in sequence .\n",
      "\n",
      "sample#0100:\n",
      "\tDE: männliches kleinkind in einem roten hut , das sich an einem geländer festhält .\n",
      "\tEN: toddler boy in a red hat holding on to some railings .\n",
      "\n",
      "sample#1000:\n",
      "\tDE: ein junger mann in einem weißen hemd , der tomaten schneidet .\n",
      "\tEN: a young man in a white shirt cutting tomatoes .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in ([1, 10, 100, 1000]):\n",
    "    example = ds_train[i]\n",
    "    src = str.join(\" \", example.src)\n",
    "    tgt = str.join(\" \", example.trg)\n",
    "    print(f'sample#{i:04d}:\\n\\tDE: {src}\\n\\tEN: {tgt}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<sos>', '<eos>', 'a', '.', 'in', 'the', 'on', 'man']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_field.vocab.itos[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<sos>', '<eos>', '.', 'ein', 'einem', 'in', 'eine', ',']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_field.vocab.itos[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>=0, <pad>=1\n"
     ]
    }
   ],
   "source": [
    "UNK_TOKEN = tgt_field.vocab.stoi['<unk>']\n",
    "PAD_TOKEN = tgt_field.vocab.stoi['<pad>']\n",
    "\n",
    "print(f'<unk>={UNK_TOKEN}, <pad>={PAD_TOKEN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "dl_train, dl_valid, dl_test = BucketIterator.splits((ds_train, ds_valid, ds_test), batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0:\n",
      " tensor([[   2,    2,    2,    2],\n",
      "        [   8,    5,    8,    5],\n",
      "        [ 253,  518,   16, 3173],\n",
      "        [ 709,  676,    7,   38],\n",
      "        [  67,   25,    6,   11],\n",
      "        [   7,   38,   51,    6],\n",
      "        [   6,   20,  128,   51],\n",
      "        [  50,   88,   37,   92],\n",
      "        [ 128,    4,  332,    7],\n",
      "        [   9,    3,   10,    6],\n",
      "        [  17,    1,   29,  135],\n",
      "        [  14,    1,   27,   11],\n",
      "        [  71,    1,  105,  380],\n",
      "        [2721,    1, 3885,  118],\n",
      "        [ 582,    1,    4,    4],\n",
      "        [   7,    1,    3,    3],\n",
      "        [ 134,    1,    1,    1],\n",
      "        [ 388,    1,    1,    1],\n",
      "        [3654,    1,    1,    1],\n",
      "        [ 519,    1,    1,    1],\n",
      "        [   4,    1,    1,    1],\n",
      "        [   3,    1,    1,    1]]) torch.Size([22, 4])\n",
      "x0_len:\n",
      " tensor([22, 10, 16, 16]) torch.Size([4])\n",
      "y0:\n",
      " tensor([[   2,    2,    2,    2],\n",
      "        [   4,    4,    4,    4],\n",
      "        [  25,   24,   14, 2455],\n",
      "        [ 183,   33,    6,  137],\n",
      "        [   6,   22,    4,   13],\n",
      "        [   4,   52,   25,    4],\n",
      "        [  31,   10,  117,   25],\n",
      "        [ 117,   37,   45,   68],\n",
      "        [2655,   57,  367,    6],\n",
      "        [   4,    5,   36,    4],\n",
      "        [  26,    3,    6,   85],\n",
      "        [ 183,    1,   43,   12],\n",
      "        [ 642,    1,   12,   52],\n",
      "        [  69,    1, 1146,   96],\n",
      "        [  44,    1, 3007,    5],\n",
      "        [  25,    1,    5,    3],\n",
      "        [ 624,    1,    3,    1],\n",
      "        [ 117,    1,    1,    1],\n",
      "        [   5,    1,    1,    1],\n",
      "        [   3,    1,    1,    1]]) torch.Size([20, 4])\n",
      "y0_len:\n",
      " tensor([20, 11, 17, 16]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# dataloader returns a Batch object with .src and .trg attributes\n",
    "b0 = next(iter(dl_train))\n",
    "\n",
    "# The .src/.trg attributes contain tuples of sequences and their lengths\n",
    "# Get batches of sequences \n",
    "x0, x0_len = b0.src\n",
    "y0, y0_len =  b0.trg\n",
    "\n",
    "print('x0:\\n', x0, x0.shape)\n",
    "print('x0_len:\\n', x0_len, x0_len.shape)\n",
    "print('y0:\\n', y0, y0.shape)\n",
    "print('y0_len:\\n', y0_len, y0_len.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Seq2SeqEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_layers, h_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.Embedding converts from token index to dense tensor\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # PyTorch multilayer GRU RNN\n",
    "        self.rnn = nn.GRU(embedding_dim, h_dim, num_layers=num_layers, dropout=dropout)\n",
    "        \n",
    "        # Encoder has no output layer; we just return hidden states.\n",
    "        \n",
    "    def forward(self, x, **kw):\n",
    "        # x shape: (S, B) Note batch dim is not first!\n",
    "        S, B = x.shape\n",
    "        embedded = self.embedding(x) # embedded shape: (S, B, E)\n",
    "        \n",
    "        # GRU first  output returns all hidden states from last layer (S, B, H)\n",
    "        # GRU second output returns last hidden state from each layer (L, B, H)\n",
    "        h, ht = self.rnn(embedded)\n",
    "        return h, ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the encoder with a batch of German sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h: torch.Size([22, 4, 64])\n",
      "ht: torch.Size([2, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "EMB_DIM = 128\n",
    "HID_DIM = 64\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "enc = Seq2SeqEncoder(V_src, EMB_DIM, NUM_LAYERS, HID_DIM)\n",
    "h, ht = enc(x0)\n",
    "print(f'h: {h.shape}')\n",
    "print(f'ht: {ht.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_layers, h_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.Embedding converts from token index to dense tensor\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # PyTorch multilayer GRU RNN\n",
    "        self.rnn = nn.GRU(embedding_dim, h_dim, num_layers=num_layers, dropout=dropout)\n",
    "        \n",
    "        # Output layer, note the output dimension!\n",
    "        self.out_fc = nn.Linear(h_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, context, **kw):\n",
    "        # x shape: (S, B)\n",
    "        # context: (L, B, H) the last hidden state from the encoder\n",
    "        S, B = x.shape\n",
    "        embedded = self.embedding(x) # embedded shape: (S, B, E)\n",
    "        \n",
    "        # Note initial hidden state is the input context vector\n",
    "        # h:  all hidden states from last layer (S, B, H)\n",
    "        # ht: last hidden state from each layer (L, B, H)\n",
    "        h, ht = self.rnn(embedded, context)\n",
    "        \n",
    "        # Project H back to the vocab size V, to get a score per word\n",
    "        out = self.out_fc(h)\n",
    "        \n",
    "        # Out shapes: (S, B, V) and (L, B, H)\n",
    "        return out, ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the decoder with the corresponding batch of German sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat: torch.Size([20, 4, 5893])\n"
     ]
    }
   ],
   "source": [
    "dec = Seq2SeqDecoder(V_tgt, EMB_DIM, NUM_LAYERS, HID_DIM)\n",
    "yhat, _ = dec(y0, ht) # note different S\n",
    "print(f'yhat: {yhat.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder: Seq2SeqEncoder, decoder: Seq2SeqDecoder):\n",
    "        super().__init__()\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "    \n",
    "    def forward(self, x_src, x_tgt, p_tf=1., **kw):\n",
    "        # input shapes: (S1, B), (S2, B)\n",
    "        S2, B = x_tgt.shape\n",
    "        \n",
    "        # Forward pass through encoder\n",
    "        # context is (L, B, H)\n",
    "        enc_h, context = self.enc(x_src, **kw)\n",
    "        \n",
    "        # First input is first target token\n",
    "        dec_input = x_tgt[[0], :] # (1, B)\n",
    "        # Loop over tokens in target sequence and feed them to the decoder\n",
    "        dec_outputs = []\n",
    "        for t in range(1, S2):\n",
    "            # Feed the decoder sequences of length 1 & save new context\n",
    "            dec_output, context = self.dec(dec_input, context, enc_h=enc_h, **kw) # dec_output is (1, B, V)\n",
    "            dec_outputs.append(dec_output)\n",
    "            \n",
    "            # For next input, take either:\n",
    "            # - next target token (AKA \"teacher forcing\"), with proba p_tf\n",
    "            # - highest scoring output (greedy prediction of next token), with proba 1-p_tf\n",
    "            if p_tf > torch.rand(1).item():\n",
    "                dec_input = x_tgt[[t], :] # (1, B)\n",
    "            else:\n",
    "                dec_input = torch.argmax(dec_output, dim=2) # (1,B,V) -> (1, B)\n",
    "            \n",
    "        # Stack decoder outputs from all timesteps\n",
    "        y_hat = torch.cat(dec_outputs, dim=0) # (S-1)x(1,B,V) -> (S-1,B,V)\n",
    "        \n",
    "        # Output shape: (S-1, B, V)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19, 4, 5893])\n"
     ]
    }
   ],
   "source": [
    "seq2seq_model = Seq2Seq(enc, dec)\n",
    "yhat = seq2seq_model(x0, y0)\n",
    "print(yhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(model, dl_train, optimizer, loss_fn, p_tf=1., clip_grad=1., max_batches=None):\n",
    "    losses = []\n",
    "    with tqdm.tqdm(total=(max_batches if max_batches else len(dl_train)), file=sys.stdout) as pbar:\n",
    "        for idx_batch, batch in enumerate(dl_train, start=1):\n",
    "            x, x_len = batch.src\n",
    "            y, y_len =  batch.trg\n",
    "\n",
    "            # Forward pass: encoder and decoder\n",
    "            # Output y_hat is the translated sequence\n",
    "            y_hat = model(x, y, p_tf, src_len=x_len)\n",
    "            S, B, V = y_hat.shape\n",
    "\n",
    "            # y[:,i] is <sos>, w_1, w_2, ..., w_k, <eos>, <pad>, ...\n",
    "            # y_hat is   w_1', w_2', ..., w_k', <eos>', <pad>', ...\n",
    "            # based on the above, get ground truth y\n",
    "            y_gt = y[1:, :].reshape(S*B)  # drop <sos>\n",
    "            y_hat = y_hat.reshape(S*B, V)\n",
    "\n",
    "            # Calculate loss compared to ground truth y\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(y_hat, y_gt)\n",
    "            loss.backward()\n",
    "\n",
    "            # Prevent exploding gradients\n",
    "            if clip_grad > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            pbar.update(); pbar.set_description(f'train loss={losses[-1]:.3f}')\n",
    "            if max_batches and idx_batch >= max_batches:\n",
    "                break\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_seq2seq(model, dl_test):\n",
    "    accuracies = []\n",
    "    with tqdm.tqdm(total=len(dl_test), file=sys.stdout) as pbar:\n",
    "        for idx_batch, batch in enumerate(dl_test):\n",
    "            x, x_len = batch.src\n",
    "            y, y_len =  batch.trg\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_hat = model(x, y, p_tf=0, src_len=x_len)  # Note: no teacher forcing in eval\n",
    "\n",
    "            S, B, V = y_hat.shape\n",
    "\n",
    "            y_gt = y[1:, :].reshape(S*B)  # drop <sos>\n",
    "            y_hat = torch.argmax(y_hat.reshape(S*B, V), dim=1) # greedy-sample\n",
    "\n",
    "            # Compare prediction to ground truth\n",
    "            accuracies.append(torch.sum(y_gt == y_hat) / S)\n",
    "\n",
    "            pbar.update(); pbar.set_description(f'eval acc={accuracies[-1]}')\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EPOCH 1/2, p_tf=1.00 ===\n",
      "train loss=5.483: 100%|██████████| 25/25 [00:21<00:00,  1.15it/s]\n",
      "eval acc=9: 100%|██████████| 8/8 [00:01<00:00,  5.60it/s] \n",
      "=== EPOCH 2/2, p_tf=0.95 ===\n",
      "train loss=5.001: 100%|██████████| 25/25 [00:20<00:00,  1.25it/s]\n",
      "eval acc=7: 100%|██████████| 8/8 [00:01<00:00,  5.64it/s] \n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "# Use small model so that training is fast, just an example\n",
    "EMB_DIM = 64\n",
    "HID_DIM = 128\n",
    "NUM_LAYERS = 3\n",
    "GRAD_CLIP = 1.\n",
    "EPOCHS = 2\n",
    "BATCHES_PER_EPOCH=25\n",
    "\n",
    "dl_train, dl_valid, dl_test = BucketIterator.splits((ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE)\n",
    "\n",
    "enc = Seq2SeqEncoder(V_src, EMB_DIM, NUM_LAYERS, HID_DIM)\n",
    "dec = Seq2SeqDecoder(V_tgt, EMB_DIM, NUM_LAYERS, HID_DIM)\n",
    "seq2seq_model = Seq2Seq(enc, dec)\n",
    "\n",
    "optimizer = torch.optim.Adam(seq2seq_model.parameters(), lr=1e-2)\n",
    "\n",
    "# Note: We don't compute loss from padding tokens!\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "for idx_epoch in range(EPOCHS):\n",
    "    # Linearly decay amount of teacher forcing for the first 20 epochs (example)\n",
    "    p_tf = 1 - min((idx_epoch / 20), 1)\n",
    "    \n",
    "    print(f'=== EPOCH {idx_epoch+1}/{EPOCHS}, p_tf={p_tf:.2f} ===')\n",
    "    losses += train_seq2seq(seq2seq_model, dl_train, optimizer, loss_fn, p_tf, GRAD_CLIP, BATCHES_PER_EPOCH)\n",
    "    accuracies += eval_seq2seq(seq2seq_model, dl_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "ax[0].plot(losses); ax[0].set_title('train loss'); ax[0].set_xlabel('iteration'); ax[0].grid(True)\n",
    "ax[1].plot(accuracies); ax[1].set_title('eval accuracy'); ax[1].set_xlabel('iteration'); ax[1].grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we use this a **Generative** model?\n",
    "\n",
    "I.e., can we use this model to generate coherent sentences in English?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically we could... But how can we sample the input context vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this architecture the encoder's **last** hidden state must encode all the information the decoder needs for translation.\n",
    "\n",
    "**Local** information, i.e. the encoder outputs and intermediate hidden states is discarded.\n",
    "\n",
    "Can we use this local info to help the decoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement an Additive attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "class MLPAttention(nn.Module):\n",
    "    def __init__(self, q_dim, k_dim, v_dim, h_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.wk = nn.Linear(k_dim, h_dim, bias=False)\n",
    "        self.wq = nn.Linear(q_dim, h_dim, bias=False)\n",
    "        self.v  = nn.Linear(h_dim, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor, seq_len: Tensor=None):\n",
    "        \"\"\"\n",
    "        :param q: Queries tensor of shape (B, Q, q_dim)\n",
    "        :param k: Keys tensor of shape (B, K, k_dim)\n",
    "        :param v: Values tensor of shape (B, K, v_dim)\n",
    "        :param seq_len: Sequence lengths tensor of shape (B,).\n",
    "        :return: Attended values tensor, of shape (B, Q, v_dim).\n",
    "        \"\"\"\n",
    "        # (B, K, k_dim) -> (B, K, h_dim) -> (B, 1, K, h_dim)\n",
    "        wk_k = self.wk(k).unsqueeze(1)\n",
    "        \n",
    "        # (B, Q, q_dim)  -> (B, Q, h_dim)  -> (B, Q, 1, h_dim)\n",
    "        wq_q = self.wq(q).unsqueeze(2)\n",
    "        \n",
    "        # (B, Q, K, h_dim)\n",
    "        z1 = torch.tanh(wq_q + wk_k)\n",
    "        \n",
    "        # (B, Q, K, 1) -> (B, Q, K)\n",
    "        z2 = self.v(z1).squeeze(dim=-1)\n",
    "        \n",
    "        # Mask z2 before applying softmax: only seq_len keys are non-padding in each of the B samples\n",
    "        if seq_len is not None:\n",
    "            B, Q, K = z2.shape\n",
    "            idx = torch.arange(K).expand_as(z2)    # (B,Q,K) containing indices 0..K-1\n",
    "            mask = idx >= seq_len.reshape(B, 1, 1) # mask selects indices greater than seq_len\n",
    "            z2[mask] = float('-inf')               # set selected to -inf to prevent influence on softmax\n",
    "        \n",
    "        a = torch.softmax(z2, dim=-1)\n",
    "        a = self.dropout(a)\n",
    "        \n",
    "        # (B, Q, K) * (B, K, v_dim) = (B, Q, v_dim)\n",
    "        return torch.bmm(a, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q (B, Q, q_dim):\n",
      " tensor([[[1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# Create one query of dim 3 (but in a batch of 2)\n",
    "q = torch.ones((2, 1, 3), dtype=torch.float)\n",
    "print('q (B, Q, q_dim):\\n', q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k (B, K, k_dim):\n",
      " tensor([[[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]]])\n",
      "v (B, K, v_dim):\n",
      " tensor([[[ 0.,  4.,  8., 12., 16.],\n",
      "         [ 1.,  5.,  9., 13., 17.],\n",
      "         [ 2.,  6., 10., 14., 18.],\n",
      "         [ 3.,  7., 11., 15., 19.]],\n",
      "\n",
      "        [[20., 24., 28., 32., 36.],\n",
      "         [21., 25., 29., 33., 37.],\n",
      "         [22., 26., 30., 34., 38.],\n",
      "         [23., 27., 31., 35., 39.]]])\n"
     ]
    }
   ],
   "source": [
    "# Create 4 key-value pairs\n",
    "k = torch.ones(2, 4, 2, dtype=torch.float)\n",
    "print('k (B, K, k_dim):\\n', k)\n",
    "v = torch.arange(40, dtype=torch.float).reshape(2, 5, 4).transpose(1, 2)\n",
    "print('v (B, K, v_dim):\\n', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = MLPAttention(q_dim=3, k_dim=2, v_dim=5, h_dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.5000,  5.5000,  9.5000, 13.5000, 17.5000]],\n",
      "\n",
      "        [[21.5000, 25.5000, 29.5000, 33.5000, 37.5000]]],\n",
      "       grad_fn=<BmmBackward>)\n",
      "(B, Q, v_dim) = torch.Size([2, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "o = attn(q, k, v, seq_len=None)\n",
    "print(o)\n",
    "print(f'(B, Q, v_dim) = {o.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output is a sequence of length 1 because we had one query.\n",
    "\n",
    "Let's try with `seq_len=1`, i.e. only the first token in the input is considered valid (not padding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  4.,  8., 12., 16.]],\n",
       "\n",
       "        [[20., 24., 28., 32., 36.]]], grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(q, k, v, seq_len=torch.tensor([1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `seq_len=1`, the output exactly equals the first value `v[:,0,:]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5000,  4.5000,  8.5000, 12.5000, 16.5000]],\n",
       "\n",
       "        [[21.5000, 25.5000, 29.5000, 33.5000, 37.5000]]],\n",
       "       grad_fn=<BmmBackward>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn(q, k, v, seq_len=torch.tensor([2, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/seq2seq_attention.svg\" width=1000></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoderAttn(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_layers, h_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # The Q, K, V dims are all h_dim because of how we parametrized encoder and decoder\n",
    "        self.attn = MLPAttention(h_dim, h_dim, h_dim, h_dim, dropout)\n",
    "        \n",
    "        # Note: GRU input dim now includes both embedding and attention output!\n",
    "        self.rnn = nn.GRU(embedding_dim + h_dim*num_layers, h_dim, num_layers=num_layers, dropout=dropout)\n",
    "        \n",
    "        self.out_fc = nn.Linear(h_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, h0, enc_h, src_len, **kw):\n",
    "        # x shape: (S, B)\n",
    "        # h0: (L, B, H) the initial hidden state\n",
    "        # enc_h: (S, B, H) all outputs from encoder (key-values for attention)\n",
    "        # src_len: (B,) the length without padding of the encoder's sequence\n",
    "        S, B = x.shape\n",
    "        embedded = self.embedding(x) # embedded shape: (S, B, E)\n",
    "        \n",
    "        # Apply attention: Query is prev hidden state; key/vals are enc outputs without positions of padding\n",
    "        q  = h0.transpose(0, 1)    # (B, L, H)\n",
    "        kv = enc_h.transpose(0, 1) # (B, S, H)\n",
    "        a  = self.attn(q, kv, kv, seq_len=src_len)  # (B, L, H)\n",
    "        \n",
    "        # Create RNN input by concatenating attention-based context with the embedded inputs\n",
    "        # Note that when used with Seq2Seq we have S=1 so the expand is not needed\n",
    "        # (L, B, H) -> (1, B, L*H) -> (S, B, L*H)\n",
    "        a = a.reshape(1, B, -1).expand(S, -1, -1)\n",
    "        rnn_input = torch.cat((embedded, a), dim=2) # (S, B, E + L*H)\n",
    "        \n",
    "        h, ht = self.rnn(rnn_input, h0)\n",
    "        \n",
    "        # Project H back to the vocab size V, to get a score per word\n",
    "        out = self.out_fc(h)\n",
    "        \n",
    "        # Out shapes: (S, B, V) and (L, B, H)\n",
    "        return out, ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EPOCH 1/2, p_tf=1.00 ===\n",
      "train loss=5.448: 100%|██████████| 25/25 [00:42<00:00,  1.69s/it]\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'src_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-188-c5aae0aca369>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'=== EPOCH {idx_epoch+1}/{EPOCHS}, p_tf={p_tf:.2f} ==='\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_seq2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq2seq_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRAD_CLIP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCHES_PER_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0maccuracies\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0meval_seq2seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq2seq_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-138-52fb99b63601>\u001b[0m in \u001b[0;36meval_seq2seq\u001b[0;34m(model, dl_test)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Note: no teacher forcing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-84cdb9ed0128>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_src, x_tgt, p_tf, **kw)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# Feed the decoder sequences of length 1 & save new context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_h\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# dec_output is (1, B, V)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mdec_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'src_len'"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "# Use small model so that training is fast, just an example\n",
    "EMB_DIM = 64\n",
    "HID_DIM = 128\n",
    "NUM_LAYERS = 3\n",
    "GRAD_CLIP = 1.\n",
    "EPOCHS = 2\n",
    "BATCHES_PER_EPOCH=25\n",
    "\n",
    "dl_train, dl_valid, dl_test = BucketIterator.splits((ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE)\n",
    "\n",
    "enc = Seq2SeqEncoder(V_src, EMB_DIM, NUM_LAYERS, HID_DIM)\n",
    "dec = Seq2SeqDecoderAttn(V_tgt, EMB_DIM, NUM_LAYERS, HID_DIM)\n",
    "seq2seq_model = Seq2Seq(enc, dec)\n",
    "\n",
    "optimizer = torch.optim.Adam(seq2seq_model.parameters(), lr=1e-2)\n",
    "\n",
    "# Note: We don't compute loss from padding tokens!\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "for idx_epoch in range(EPOCHS):\n",
    "    # Linearly decay amount of teacher forcing fro the first 10 epochs (example)\n",
    "    p_tf = 1 - min((idx_epoch / 20), 1)\n",
    "    \n",
    "    print(f'=== EPOCH {idx_epoch+1}/{EPOCHS}, p_tf={p_tf:.2f} ===')\n",
    "    losses += train_seq2seq(seq2seq_model, dl_train, optimizer, loss_fn, p_tf, GRAD_CLIP, BATCHES_PER_EPOCH)\n",
    "    accuracies += eval_seq2seq(seq2seq_model, dl_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n",
    "ax[0].plot(losses); ax[0].set_title('train loss'); ax[0].set_xlabel('iteration'); ax[0].grid(True)\n",
    "ax[1].plot(accuracies); ax[1].set_title('eval accuracy'); ax[1].set_xlabel('iteration'); ax[1].grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Image credits**\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from:\n",
    "\n",
    "- K. Xu et al. 2015, http://proceedings.mlr.press/v37/xuc15.html\n",
    "- Sutskever et al. 2014, https://arxiv.org/abs/1409.3215\n",
    "- Zhang et al., Dive into Deep Learning, 2019\n",
    "- Peter Bloem, http://www.peterbloem.nl/blog/transformers\n",
    "- Chris Olah, https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "- Ben Trevett, http://bentrevett.com\n",
    "- MIT 6.S191"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
