{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "# Tutorial 5: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:  **TODO**\n",
    "\n",
    "- Backpropagation\n",
    "- Optimization\n",
    "- Automatic differentiation\n",
    "- PyTorch backward functions\n",
    "- Bi-level differentiable optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descent-based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we have seen, training deep neural network is performed iteratively using descent-based optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The general scheme is,\n",
    "\n",
    "1. Initialize parameters to some $\\vec{\\Theta}^0 \\in \\set{R}^P$, and set $k\\leftarrow 0$.\n",
    "2. While not converged:\n",
    "    1. Choose a direction $\\vec{d}^k\\in\\set{R}^P$\n",
    "    2. Choose a step size $\\eta_k\\in\\set{R}$\n",
    "    3. Update: $\\vec{\\Theta}^{k+1} \\leftarrow \\vec{\\Theta}^k + \\eta_k \\vec{d}^k$\n",
    "    4. $k\\leftarrow k+1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which descent direction to choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one which maximally decreases the loss function $L(\\vec{\\Theta})$:\n",
    "\n",
    "$$\n",
    "\\vec{d} =\\arg\\min_{\\vec{d'}} L(\\vec{\\Theta}+\\vec{d'})-L(\\vec{\\Theta})\n",
    "\\approx\n",
    "\\arg\\min_{\\vec{d'}}\\nabla L(\\vec{\\Theta})^\\top\\vec{d'}, \\\n",
    "\\mathrm{s.t.} \\norm{\\vec{d}}_p=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choice of norm determines $\\vec{d}$. For example,\n",
    "- $p=1$: Coordinate descent: direction of the largest gradient component.\n",
    "- $p=2$: Gradient descent: $\\vec{d}=-\\nabla L(\\vec{\\Theta})$.\n",
    "\n",
    "|$p=1$|$p=2$|\n",
    "|---|---|\n",
    "|<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e3/Coordinate_descent.svg\" width=\"300\" /> | <img src=\"https://upload.wikimedia.org/wikipedia/commons/f/ff/Gradient_descent.svg\" width=\"300\" />| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks and mitigations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Susceptible to initialization**\n",
    "\n",
    "Initializing near local minima can prevent finding better ones.\n",
    "\n",
    "<center><img src=\"imgs/sgd-init.png\" width=\"500\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can use stochastic gradient to get a different loss surface every iteration.\n",
    "\n",
    "\n",
    "<center><img src=\"imgs/sgd-loss.png\" width=\"500\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sensitive to learning rate**\n",
    "\n",
    "<center><img src=\"imgs/sgd-lr.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Line search (1D minimization):\n",
    "$$\n",
    "\\eta_k = \\arg\\min_{\\eta'} L(\\vec{\\Theta}^k+\\eta'\\vec{d}^k)\n",
    "$$\n",
    "\n",
    "- Adaptive LR optimizers, e.g. Adam\n",
    "\n",
    "- LR scheduling\n",
    "<center><img src=\"imgs/sgd-lr-schedule.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zig-zags in narrow \"ravines\"**\n",
    "\n",
    "<center><img src=\"imgs/sgd-zigzag.png\" width=\"300\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Momentum: Use previous gradients to build \"speed\" in the common direction and cancel-out oscillations in opposite directions.\n",
    "\n",
    "- BatchNorm: Normalizes activations to zero-mean and unit variance (reduces curvature)\n",
    "\n",
    "- Second-order methods: Use quadratic local approximation of the loss surface, instead of linear.\n",
    "    - Newton's method: $\\vec{d}_k=\\mat{H}_k^{-1}\\vec{g}_k = \\nabla^2 L(\\vec{\\Theta}_k)^{-1}\\nabla L(\\vec{\\Theta}_k)$.\n",
    "    - Various other methods which use some estimate of the Hessian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The back-propagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the above optimization methods have a crucial thing in common: They require calculation of gradients of the loss w.r.t. to the parameters.\n",
    "\n",
    "In practical settings when training neural networks we have many different parameters tensors we would like to update separately. Thus, we require the gradient of the loss w.r.t. each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back-propagation is an efficient way to calculate these gradients using the chain rule.\n",
    "\n",
    "We represent the application of a model as a **computation graph**.\n",
    "For example, a simple linear regression model can be represented as:\n",
    "\n",
    "<center><img src=\"imgs/backprop-graph.png\" width=\"350\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that in this graph we have $N$ variables $\\vec{v}^i,\\ 1\\leq i \\leq N$  and functions $f_i$ which compute them from other variables.\n",
    "\n",
    "The graph is directional, thus assume $\\vec{v}^1, \\vec{v}^2,\\dots,\\vec{v}^N$ represents a topological order of the graph (parents before children).\n",
    "\n",
    "Define also the notation $\\delta\\vec{v}\\triangleq \\pderiv{L}{\\vec{v}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward pass can therefore we written as:\n",
    "\n",
    "1. For $i=1,2,\\dots,N$:\n",
    "  1. Graph parents of current node: $$\\mathcal{P}_i \\leftarrow \\left\\{\\vec{v}^j ~\\middle\\vert~ \\vec{v}^j \\text{ parent of } \\vec{v}^i\\right\\}$$ \n",
    "  2. Function value of current node: $$\\vec{v}^i\\leftarrow f_i(\\mathcal{P}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in the backward pass we traverse the graph in reverse and apply the chain rule:\n",
    "\n",
    "1. Set $\\delta\\vec{v}^N=1$.\n",
    "2. For $i=N,N-1,\\dots,1$:\n",
    "  1. Graph children of current node: $$\\mathcal{C}_i \\leftarrow \\left\\{\\vec{v}^j ~\\middle\\vert~ \\vec{v}^j \\text{ child of } \\vec{v}^i\\right\\}$$  \n",
    "  2. Chain rule: $$\\delta\\vec{v}^i\\leftarrow \\sum_{\\vec{v}^j\\in\\mathcal{C}_i} \\delta\\vec{v}^j\\pderiv{\\vec{v}^j}{\\vec{v}^i}$$\n",
    "  \n",
    "Note that the expression $\\delta\\vec{v}^j\\pderiv{\\vec{v}^j}{\\vec{v}^i}$ is a \"vector\"-Jacobian product (VJP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation easily lends itself to a modular and efficient implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modularity:\n",
    "- Nodes in the computation graph only need to know how to calculate their own derivatives.\n",
    "- This is then passed to the parent nodes, which can do the same.\n",
    "\n",
    "\n",
    "<center><img src=\"imgs/backprop-modular.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efficiency:\n",
    "\n",
    "- We only need to compute each $\\delta\\vec{v}^i$ once.\n",
    "- We in practice never construct the Jacobian, and instead calculate the VJP directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern automatic-differentiation packages such as PyTorch's `autograd` utilize exactly these tricks to implement backprop in an extremely powerful way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Image credits**\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from:\n",
    "\n",
    "- Dr. Roger Grosse, UToronto, cs321\n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
