{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "# CS236605: Deep Learning\n",
    "# Tutorial 5: Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus far, our models have been composed of fully connected (linear) layers or convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fully connected layers\n",
    "    - Each layer $l$ operates on the output of the previous layer ($\\vec{y}_{l-1}$) and calculates,\n",
    "        $$\n",
    "        \\vec{y}_l = \\varphi\\left( \\mat{W}_l \\vec{y}_{l-1} + \\vec{b}_l \\right),~\n",
    "        \\mat{W}_l\\in\\set{R}^{n_{l}\\times n_{l-1}},~ \\vec{b}_l\\in\\set{R}^{n_l}.\n",
    "        $$\n",
    "    - FC's have completely pre-fixed input and output dimensions.\n",
    "    \n",
    "    <img src=\"img/mlp.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convolutional layers\n",
    "    - Each layer operates on an input tensor $\\vec{x}$ containing $M$ feature maps. The $k$-th feature map of the output tensor $\\vec{y}$ is:\n",
    "        $$\n",
    "        \\vec{y}^k = \\sum_{m=1}^{M} \\vec{w}^{km}\\ast\\vec{x}^m+b^k,\\ k\\in[1,K]\n",
    "        $$\n",
    "      Where $\\ast$ denotes convolution, and $K$ is the number of output feature maps.\n",
    "      \n",
    "      <img src=\"img/cnn_filters.png\" width=\"500\"/>\n",
    "    - This time the weight dimensions are not dependent on the input dimensions.\n",
    "    - Weights are shared across the spatial dimensions of the input.\n",
    "    - Output dimension changes based on input dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However,\n",
    "- Models based on these types of layers lack **persistent state**. \n",
    "- The current output is not affected by **previous inputs** (or outputs).\n",
    "\n",
    "How can we model a dynamical system?\n",
    "E.g., a linear system such as\n",
    "$$\\vec{y}_t = a_0 + a_1 \\vec{y}_{t-1}+\\dots+a_P \\vec{y}_{t-P} + b_0 \\vec{x}_t+\\dots+b_{t-Q}\\vec{x}_{t-Q}$$\n",
    "\n",
    "Many use cases and examples: text translation, sentiment classification, scene analysis in video, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RNN layer is similar to a regular FC layer, but it has two inputs:\n",
    "- Current sample, $\\vec{x}_t \\in\\set{R}^{d_{i}}$.\n",
    "- Previous **state**, $\\vec{h}_{t-1}\\in\\set{r}^{d_{h}}$.\n",
    "\n",
    "and it produces two outputs which depend on both:\n",
    "- Current layer output, $\\vec{y}_t\\in\\set{R}^{d_o}$.\n",
    "- Current **state**, $\\vec{h}_{t}\\in\\set{r}^{d_{h}}$.\n",
    "\n",
    "<img src=\"img/rnn_cell.png\" width=\"300\"/>\n",
    "\n",
    "Crucially,\n",
    "- The function $\\varphi(\\cdot)$ itself is not time-dependent (but is parametrized).\n",
    "- The same layer (function) is applied at successive time steps, propagating the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic RNN can be defined as follows.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\forall t \\geq 0:\\\\\n",
    "\\vec{h}_t &= \\varphi_h\\left( \\mat{W}_{hh} \\vec{h}_{t-1} + \\mat{W}_{xh} \\vec{x}_t + \\vec{b}_h\\right) \\\\\n",
    "\\vec{y}_t &= \\varphi_y\\left(\\mat{W}_{hy}\\vec{h}_t + \\vec{b}_y \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where,\n",
    "- $\\vec{x}_t \\in\\set{R}^{d_{i}}$ is the input at time $t$.\n",
    "- $\\vec{h}_{t-1}\\in\\set{R}^{d_{h}}$ is the **hidden state** of a fixed dimension.\n",
    "- $\\vec{y}_t\\in\\set{R}^{d_o}$ is the output at time $t$.\n",
    "- $\\mat{W}_{hh}\\in\\set{R}^{d_h\\times d_h}$, $\\mat{W}_{xh}\\in\\set{R}^{d_h\\times d_i}$, $\\mat{W}_{hy}\\in\\set{R}^{d_o\\times d_h}$, $\\vec{b}_h\\in\\set{R}^{d_h}$ \n",
    "- $\\varphi_h$ and $\\varphi_y$ are some non-linear functions. In many cases $\\varphi_y$ is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $\\vec{b}_y\\in\\set{R}^{d_o}$ are the model weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling time-dependence\n",
    "\n",
    "If we imagine **unrolling** a single RNN layer through time,\n",
    "<img src=\"img/rnn_unrolled.png\" width=\"800\" />\n",
    "\n",
    "We can see how late outputs can now be influenced by early inputs, through the hidden state.\n",
    "\n",
    "How would **backpropagation** work, though?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layered RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs layers can be stacked to build a deep RNN model.\n",
    "\n",
    "<img src=\"img/rnn_layered.png\" width=\"800\"/>\n",
    "\n",
    "- As with MLPs, adding depth allows us to model intricate hierarchical features.\n",
    "- However, now we also have a time dimension which makes the representation time-dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above equaitions, let's create a simple layer RNN with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_dim, phi_h=torch.tanh, phi_y=torch.sigmoid):\n",
    "        super().__init__()\n",
    "        self.phi_h, self.phi_y = phi_h, phi_y\n",
    "        \n",
    "        self.fc_xh = nn.Linear(in_dim, h_dim, bias=False)\n",
    "        self.fc_hh = nn.Linear(h_dim, h_dim, bias=True)\n",
    "        self.fc_hy = nn.Linear(h_dim, out_dim, bias=True)\n",
    "        \n",
    "    def forward(self, xt, h_prev=None):\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(xt.shape[0], self.fc_hh.in_features)\n",
    "        \n",
    "        ht = self.phi_h(self.fc_xh(xt) + self.fc_hh(h_prev))\n",
    "        \n",
    "        yt = self.fc_hy(ht)\n",
    "        \n",
    "        if self.phi_y is not None:\n",
    "            yt = self.phi_y(yt)\n",
    "        \n",
    "        return yt, ht\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLayer(\n",
       "  (fc_xh): Linear(in_features=1024, out_features=128, bias=False)\n",
       "  (fc_hh): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc_hy): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate our model\n",
    "\n",
    "N = 3 # batch size\n",
    "in_dim, h_dim, out_dim = 1024, 128, 1\n",
    "\n",
    "rnn = RNNLayer(in_dim, h_dim, out_dim)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1: tensor([[0.4339],\n",
      "        [0.4022],\n",
      "        [0.5431]], grad_fn=<SigmoidBackward>)\n",
      "y2: tensor([[0.4161],\n",
      "        [0.4258],\n",
      "        [0.4816]], grad_fn=<SigmoidBackward>)\n",
      "y3: tensor([[0.5188],\n",
      "        [0.5181],\n",
      "        [0.5371]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Manually \"run\" a few time steps\n",
    "\n",
    "# t=1\n",
    "x1 = torch.randn(N, in_dim)\n",
    "y1, h1 = rnn(x1)\n",
    "print(f'y1: {y1}')\n",
    "\n",
    "# t=2\n",
    "x2 = torch.randn(N, in_dim)\n",
    "y2, h2 = rnn(x2, h1)\n",
    "print(f'y2: {y2}')\n",
    "\n",
    "# t=3\n",
    "x3 = torch.randn(N, in_dim)\n",
    "y3, h3 = rnn(x3, h2)\n",
    "print(f'y3: {y3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1]) torch.Size([3, 128])\n"
     ]
    }
   ],
   "source": [
    "print(y3.shape, h3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "\n",
    "from torch.utils.data.dataset import Subset\n",
    "import numpy as np\n",
    "\n",
    "#train_data, test_data = datasets.IMDB.splits(TEXT, LABEL, root=data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField()#(dtype=torch.float)\n",
    "\n",
    "train_data, valid_data, test_data = datasets.SST.splits(TEXT, LABEL, root=data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 8544\n",
      "Number of validation examples: 1101\n",
      "Number of testing examples: 2210\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['The', 'film', 'aims', 'to', 'be', 'funny', ',', 'uplifting', 'and', 'moving', ',', 'sometimes', 'all', 'at', 'once', '.'], 'label': 'positive'}\n",
      "{'text': ['An', 'ugly', ',', 'revolting', 'movie', '.'], 'label': 'negative'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data[111]))\n",
    "print(vars(train_data[7777]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocab simply gives each unique work (token) an index an maps it to a 1-hot vector\n",
    "MAX_VOCAB_SIZE = 25_000\n",
    "TEXT.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 17200\n",
      "Unique tokens in LABEL vocabulary: 3\n",
      "[('.', 8041), (',', 7131), ('the', 6087), ('and', 4474), ('of', 4446), ('a', 4423), ('to', 3024), ('-', 2739), (\"'s\", 2544), ('is', 2540), ('that', 1916), ('in', 1817), ('it', 1781), ('The', 1265), ('as', 1203), ('film', 1156), ('but', 1076), ('with', 1071), ('movie', 999), ('for', 977)]\n",
      "['<unk>', '<pad>', '.', ',', 'the', 'and', 'of', 'a', 'to', '-']\n",
      "defaultdict(<function _default_unk_index at 0x127e27ea0>, {'positive': 0, 'negative': 1, 'neutral': 2})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")\n",
    "\n",
    "print(TEXT.vocab.freqs.most_common(20))\n",
    "print(TEXT.vocab.itos[:10])\n",
    "\n",
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6455,    69,  1805,    15],\n",
      "        [  496,  1277,    38,    81],\n",
      "        [   12,  3896,   670,  1562],\n",
      "        [   85,    35,    24,    25],\n",
      "        [   68,    33,   566,   202],\n",
      "        [ 1099,     4,    53,   148],\n",
      "        [16380,   140,     6,    16],\n",
      "        [   19,     3,  7845,     4],\n",
      "        [    4,    34,  3431,  1513],\n",
      "        [  108,     4,     5,  1947],\n",
      "        [    6,    83,  4319,    11],\n",
      "        [  149,    35,  1639,    87],\n",
      "        [    5,    11,     6,  1453],\n",
      "        [ 7598,     7,  1494, 13573],\n",
      "        [    7,   325,     2,    21],\n",
      "        [   39,    60,     1,    80],\n",
      "        [ 8624,   657,     1,  2207],\n",
      "        [ 2486,     3,     1, 13568],\n",
      "        [  974,    71,     1,     2],\n",
      "        [  192,    14,     1,     1],\n",
      "        [  205,    10,     1,     1],\n",
      "        [16345,  1277,     1,     1],\n",
      "        [   82,  3896,     1,     1],\n",
      "        [    6,    12,     1,     1],\n",
      "        [   47,    25,     1,     1],\n",
      "        [ 1687,   261,     1,     1],\n",
      "        [ 8244,     2,     1,     1],\n",
      "        [   50,     1,     1,     1],\n",
      "        [  730,     1,     1,     1],\n",
      "        [12230,     1,     1,     1],\n",
      "        [    2,     1,     1,     1]])\n",
      "tensor([0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "# data iterators\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    device = device)\n",
    "\n",
    "\n",
    "batch = next(iter(train_iterator))\n",
    "\n",
    "# batch.text is a sentence_length by batch_size tensor\n",
    "print(batch.text)\n",
    "print(batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, in_dim, embedding_dim, h_dim, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding converts from token index to dense tensor\n",
    "        self.embedding = nn.Embedding(in_dim, embedding_dim)\n",
    "        \n",
    "        # Our custom RNN layer without phi_y outputs a class score\n",
    "        self.rnn = RNNLayer(embedding_dim, h_dim, out_dim, phi_y=None)\n",
    "        \n",
    "        # To convert class scores to log-probability\n",
    "        self.log_softmax = nn.LogSoftmax(dim=0)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        # Loop over tokens in the sentence\n",
    "        ht = None\n",
    "        for xt in embedded:\n",
    "            yt, ht = self.rnn(xt, ht)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        #assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        #return self.fc(hidden.squeeze(0))\n",
    "        yt_log_proba = self.log_softmax(yt)\n",
    "        return yt_log_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(17200, 100)\n",
       "  (rnn): RNNLayer(\n",
       "    (fc_xh): Linear(in_features=100, out_features=256, bias=False)\n",
       "    (fc_hh): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (fc_hy): Linear(in_features=256, out_features=3, bias=True)\n",
       "  )\n",
       "  (log_softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 3\n",
    "\n",
    "model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4530, -1.2225, -1.5000],\n",
      "        [-1.3820, -1.4393, -1.3328],\n",
      "        [-1.3566, -1.4515, -1.3604],\n",
      "        [-1.3566, -1.4516, -1.3604]], grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "print(model(batch.text))\n",
    "print(batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,812,163 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Training\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
    "loss_fn = nn.NLLLoss()\n",
    "model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, loss=1.4017422533035278, accuracy=1.59\n",
      "Epoch #1, loss=1.4019185101985931, accuracy=1.54\n",
      "Epoch #2, loss=1.40732672393322, accuracy=1.495\n",
      "Epoch #3, loss=1.403628532886505, accuracy=1.575\n",
      "Epoch #4, loss=1.3962826710939407, accuracy=1.635\n",
      "Epoch #5, loss=1.398765323162079, accuracy=1.63\n",
      "Epoch #6, loss=1.391654960513115, accuracy=1.53\n",
      "Epoch #7, loss=1.3938999819755553, accuracy=1.635\n",
      "Epoch #8, loss=1.40216306746006, accuracy=1.52\n",
      "Epoch #9, loss=1.404263315796852, accuracy=1.515\n"
     ]
    }
   ],
   "source": [
    "for epoch_idx in range(10):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    num_correct = 0\n",
    "    max_batches = 200\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        x, y = batch.text, batch.label\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred_log_proba = model(x)\n",
    "        \n",
    "        loss = loss_fn(y_pred_log_proba, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        y_pred = torch.argmax(y_pred_log_proba, dim=1)\n",
    "        num_correct += torch.sum(y_pred == y).float().item()\n",
    "        \n",
    "        if batch_idx == max_batches-1:\n",
    "            break\n",
    "        \n",
    "    print(f\"Epoch #{epoch_idx}, loss={epoch_loss /(max_batches)}, accuracy={num_correct /(max_batches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2500)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(y_pred==y).float()/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Image credits**\n",
    "\n",
    "Images in this tutorial were taken and/or adapted from:\n",
    "\n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
