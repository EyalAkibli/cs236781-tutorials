{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "# CS236605: Deep Learning\n",
    "# Tutorial 5: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "**TODO: Update**\n",
    "\n",
    "- How convolutional layers work\n",
    "- Pooling layers\n",
    "- Network architecture\n",
    "- Spatial classification\n",
    "- Vanishing gradient problem\n",
    "- Skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.join(os.getenv('HOME'), '.pytorch-datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "![img](https://qph.fs.quoracdn.net/main-qimg-330e8b2941bc0164211bbdc7d5c693f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Composed of multiple **layers**.\n",
    "\n",
    "Each layer $j$ consists of $n_j$ regular perceptrons (\"neurons\") which calculate:\n",
    "$$\n",
    "\\vec{y}_j = \\varphi\\left( \\mat{W}_j \\vec{y}_{j-1} + \\vec{b}_j \\right),~\n",
    "\\mat{W}_j\\in\\set{R}^{n_{j}\\times n_{j-1}},~ \\vec{b}_j\\in\\set{R}^{n_j}.\n",
    "$$\n",
    "\n",
    "- Note that both input and output are **vetors**. We can think of the above equation as describing a layer of **multiple perceptrons**.\n",
    "- We'll henceforth refer to such layers as **fully-connected** or FC layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given an input sample $\\vec{x}^i$, the computed function of an $L$-layer MLP is:\n",
    "$$\n",
    "\\vec{y}_L^i= \\varphi \\left(\n",
    "\\mat{W}_L \\varphi \\left( \\cdots\n",
    "\\varphi \\left( \\mat{W}_1 \\vec{x}^i + \\vec{b}_1 \\right)\n",
    "\\cdots \\right)\n",
    "+ \\vec{b}_L \\right)\n",
    "$$\n",
    "\n",
    "- Universal approximator theorem: an MLP with $L>1$, can approximate (almost) any function given enough parameters (Cybenko, 1989).\n",
    "- This expression is fully differentiable w.r.t. parameters using the Chain Rule.\n",
    "    - In practice, many challenges to calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression\n",
    "\n",
    "<img src=\"https://docs.microsoft.com/en-us/azure/machine-learning/studio/media/algorithm-choice/image2.png\" alt=\"regression\" width=\"600\"/>\n",
    "\n",
    "- Output: $\\hat{\\vec{y}^i} = \\vec{y}^i_L$\n",
    "- Quadratic loss: $\\sum_i (\\vec{y}^i - \\hat{\\vec{y}^i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification\n",
    "<img src=\"https://ml.berkeley.edu/blog/assets/tutorials/1/image_3.svg\" width=\"400\" alt=\"classification\">\n",
    "\n",
    "- Output: $\\hat{\\vec{y}^i} = \\mathrm{softmax}(\\vec{y}^i_L)$ (class probabilities)\n",
    "- Cross entropy loss: $\\sum_i - {\\vectr{y}}^i \\log(\\hat{\\vec{y}^i})$\n",
    "\n",
    "To explore ConvNets we'll now be focusing our attention mainly on the task of classifying images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations of MLPs for image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of parameters increases quadratically with image size due to connectivity.\n",
    "    - 28x28 MNIST image: 784 weights per neuron in the first layer\n",
    "    - 1000x1000x3 color image: 3M weights **per neuron**\n",
    "    \n",
    "    <img src=\"img/vanilla_dnn_scale.png\" width=\"400\" alt=\"scale\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Huge number of parameters greatly increases risk of overfitting\n",
    "    - MLP with 1 hidden layer, 3, 6 and 20 Neurons\n",
    "    \n",
    "    <img src=\"img/overfit_1HL_3-6-20N.jpg\" width=\"500\" alt=\"overfit1\">\n",
    "    \n",
    "    - MLP with 1, 2 and 4 hidden layers, 3 neurons each\n",
    "    \n",
    "    <img src=\"img/overfit_1-2-4HL_3N.jpg\" width=\"500\" alt=\"overfit1\">\n",
    "\n",
    "- FC layers are highly sensitivity to translation, while image features are inherently translation-invariant.\n",
    "\n",
    "Despite all these limitations we still want to use deep netural nets because they allow us to **learn hierarchical features** from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional layer is similar to an MLP FC layer but with three improtant distinctions:\n",
    "1. Each neuron is only **connected to a small region** of the previous layer's output.\n",
    "1. The neurons are stacked in a **3D** grid (insead of 1D).\n",
    "1. Neurons that are at the same depth in the grid **share the same weights** (parameters $\\mat{W},~\\vec{b}$).\n",
    "\n",
    "![cnn_layer](img/cnn_layer.jpeg)\n",
    "\n",
    "In the above image, the colors of the neurons represent their weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two important things to understand about convolutional layers:\n",
    "- They operate on and produce **volumes** (3D tensors).\n",
    "\n",
    "   <img src=\"img/cnn_layers.jpeg\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each neuron is spatially local, but operates on the **full depth** dimension of it's input layer.\n",
    "\n",
    "   <img src=\"img/depthcol.jpeg\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation as filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each neuron in a given depth-slice of operates on a small region of the input layer, we can think of the combined **output of that depth-slice** as the **convolution between a filter and the input volume**.\n",
    "\n",
    "<img src=\"img/cnn_filters.png\" width=\"500\" />\n",
    "<img src=\"img/filter_resp.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have multiple depth-slices per convolutional layer, the layer computes multiple convolutions of the same input with different kernels (filters).\n",
    "\n",
    "Each 2D slice of an input and output volume is known as **feature map** or a **channel**.\n",
    "\n",
    "[Visualization of a convolutional filter](http://cs231n.github.io/assets/conv-demo/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters & dimentions\n",
    "\n",
    "Assume an input volume of shape $(C_{\\mathrm{in}}, H_{\\mathrm{in}}, W_{\\mathrm{in}})$, i.e. channels, height, width.\n",
    "\n",
    "Define,\n",
    "\n",
    "1. Number of kernels, $K \\geq 1$.\n",
    "2. Spatial extent (size) of each kernel, $F \\geq 1$. \n",
    "3. Stride $S\\geq 1$: spatial distance between consecutive applications of a kernel.\n",
    "4. Padding $P\\geq 0$: Number of \"pixels\" to zero-pad around each input feature map.\n",
    "5. Dilation $D \\geq 1$: Spacing between kernel elements when applying to input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $P=0,~S=1,~D=1$ | $P=1,~S=1,~D=1$ | $P=1,~S=2,~D=1$ | $P=0,~S=1,~D=2$ |\n",
    "|-----------------|-----------------|-----------------| --------------- |\n",
    "|<img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif\" width=\"200\"/>| <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides.gif\" width=\"200\"/> | <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides.gif\" width=\"200\"/> | <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif\" width=\"200\"/> |\n",
    "\n",
    "In the above animations, **blue** maps are inputs,\n",
    "**green** maps are outputs and\n",
    "the **shaded** area is the kernel with $F=3$.\n",
    "\n",
    "We can see that the second combination, $F=3,~P=1,~S=1,~D=1$, leads to identical sizes of input and output feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then,\n",
    "\n",
    "- Each convolution kernel will be a tensor of shape $(C_{\\mathrm{in}}, F, F)$.\n",
    "- The ouput volume dimensions will be:\n",
    "\n",
    "  $$\\begin{align}\n",
    "  H_{\\mathrm{out}} &= \\left\\lfloor \\frac{H_{\\mathrm{in}} + 2P - D\\cdot(F-1) -1}{S} \\right\\rfloor + 1\\\\\n",
    "  W_{\\mathrm{out}} &= \\left\\lfloor \\frac{W_{\\mathrm{in}} + 2P - D\\cdot(F-1) -1}{S} \\right\\rfloor + 1\\\\\n",
    "  C_{\\mathrm{out}} &= K\\\\\n",
    "  \\end{align}$$\n",
    "\n",
    "- The number of parameters in the layer will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\underbrace{K}_{\\mathrm{kernels}} \\cdot \\left(\n",
    "\\underbrace{C_{\\mathrm{in}} \\cdot F^2}_{\\mathrm{kernel\\ size}} + \\underbrace{1}_{\\mathrm{bias\\ term}}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Input image is 1000x1000x3, and the first conv layer has $10$ kernels of size 5x5.\n",
    "The number of parameters in the first layer will be: $ 10 \\cdot 3 \\cdot 5^2 + 10 = 760 $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch `Conv2d` layer example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as tvtf\n",
    "\n",
    "tf = tvtf.Compose([tvtf.ToTensor()])\n",
    "ds_cifar10 = torchvision.datasets.CIFAR10(data_dir, download=True, train=True, transform=tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 shape with batch dim: torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Load first CIFAR10 image\n",
    "x0,y0 = ds_cifar10[0]\n",
    "# add batch dim\n",
    "x0 = x0.unsqueeze(0)\n",
    "print('x0 shape with batch dim:', x0.shape)\n",
    "\n",
    "def num_params(layer):\n",
    "    return sum([p.numel() for p in layer.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1: 280 parameters\n",
      "conv2: 7220 parameters\n",
      "Input image shape:       torch.Size([1, 3, 32, 32])\n",
      "After first conv layer:  torch.Size([1, 10, 32, 32])\n",
      "After second conv layer: torch.Size([1, 20, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# First conv layer: works on input image volume\n",
    "conv1 = nn.Conv2d(in_channels=x0.size(1), out_channels=10, padding=1, kernel_size=3, stride=1)\n",
    "print(f'conv1: {num_params(conv1)} parameters')\n",
    "\n",
    "# Second conv layer: works on output volume of first layer\n",
    "conv2 = nn.Conv2d(in_channels=10, out_channels=20, padding=0, kernel_size=6, stride=2)\n",
    "print(f'conv2: {num_params(conv2)} parameters')\n",
    "\n",
    "print(f'{\"Input image shape:\":25s}{x0.shape}')\n",
    "print(f'{\"After first conv layer:\":25s}{conv1(x0).shape}')\n",
    "print(f'{\"After second conv layer:\":25s}{conv2(conv1(x0)).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: observe that the width and height dimensions of the input image were never specified!\n",
    "more on the significance of that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the size of feature maps between the convolutional layers, it's common to use **pooling** layers.\n",
    "\n",
    "Pooling layers have some hyperparameters:\n",
    "1. Spatial extent (size) of each pooling kernel, $F \\geq 2$. \n",
    "1. Stride $S\\geq 2$: spatial distance between consecutive applications.\n",
    "1. Operation (e.g. max, average, $p$-norm)\n",
    "\n",
    "... but no trainable parameters.\n",
    "\n",
    "**Example**: $\\max$-pooling with $F=2,~S=2$ performing a factor-2 downsample:\n",
    "\n",
    "<img src=\"img/maxpool.png\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why downsample feature maps after convolutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, to reduce the number of features to process in the next layer.\n",
    "\n",
    "But more crucially,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To increase the **receptive field** of the original image that each layer works with.\n",
    "- We want successive conv layers to be affected by increasingly larger parts of the input image.\n",
    "- This allows us to learn a hierarchy of visual features.\n",
    "\n",
    "<img src=\"img/feature_hierarchy.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch `Pool2d` layer example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After second conv layer: torch.Size([1, 20, 14, 14])\n",
      "After max-pool:          torch.Size([1, 20, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "print(f'{\"After second conv layer:\":25s}{conv2(conv1(x0)).shape}')\n",
    "print(f'{\"After max-pool:\":25s}{pool(conv2(conv1(x0))).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic way to build an architecture of a deep convolutional neural net, is to repeat groups of **conv-relu** layers, sprinkle in some **pooling** in between and top it all off with a nice **FC-softmax** combo.\n",
    "\n",
    "<img src=\"img/arch.png\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above image, all the **conv** blocks are actually **conv-relu** (or some other nonlinearity). The rightmost architecture is called VGG, and used to be a relevant architecture for ImageNet classification.\n",
    "\n",
    "There are many other things to consider and add:\n",
    "- Size of conv kernels\n",
    "- Number of consecutive convolutions\n",
    "- Use of batch norm to speed up training\n",
    "- Dropout for improved generalization\n",
    "- Depthwise convolutions instead of FC layers\n",
    "- Skip connections (we'll see later)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch network architecture example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement **LeNet**, arguably the first successful CNN model for MNIST (LeCun, 1998).\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*1TI1aGBZ4dybR6__DI9dzA.png\" width=\"800\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "LeNet(x0)= tensor([[-0.0320,  0.0509,  0.0192,  0.0559, -0.0295,  0.1238, -0.0317,  0.0795,\n",
      "         -0.0425,  0.0319]], grad_fn=<ThAddmmBackward>)\n",
      "shape= torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        class_scores = self.classifier(features)\n",
    "        return class_scores\n",
    "\n",
    "net = LeNet()\n",
    "print(net)\n",
    "print('LeNet(x0)=', net(x0))\n",
    "print('shape=', net(x0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Fully-convolutional Networks\n",
    "\n",
    "Notice how we never actually specified the input image size.\n",
    "\n",
    "**Does this mean we can use the network on images of any size**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**No**, because of the FC layers at the end.\n",
    "\n",
    "Here, let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "size mismatch, m1: [1 x 2704], m2: [400 x 120] at /Users/soumith/miniconda2/conda-bld/pytorch_1532624435833/work/aten/src/TH/generic/THTensorMath.cpp:2070\n"
     ]
    }
   ],
   "source": [
    "large_image = torch.randn(1,3,32*2,32*2)\n",
    "try:\n",
    "    net(large_image)\n",
    "except RuntimeError as e:\n",
    "    print(e, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However,\n",
    "- Only the FC layers at the end require actual knowledge of exact image sizes.\n",
    "- We replace them with... Convolutions, of course\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNetFullyConv(\n",
      "  (feature_extractor): Sequential(\n",
      "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(120, 84, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(84, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LeNetFullyConv(LeNet):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Remember: the last feature map volume has shape (16,5,5)\n",
    "        # Override the classifier with depthwise convolutions\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Conv2d(16, 120, 5), # note: no padding or strides!\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(120, 84, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(84, 10, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        # note: no need to reshape the features now\n",
    "        class_scores = self.classifier(features)\n",
    "        return class_scores\n",
    "\n",
    "net_fully_conv = LeNetFullyConv()\n",
    "print(net_fully_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's forward the original-sized image and the larger image through the network and observe the output shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regular image output shape: torch.Size([1, 10, 1, 1])\n",
      "large   image output shape: torch.Size([1, 10, 9, 9])\n"
     ]
    }
   ],
   "source": [
    "print('regular image output shape:', net_fully_conv(x0).shape)\n",
    "print('large   image output shape:', net_fully_conv(large_image).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's the meaning of the output after conversion to fully convolutional?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now a **spatial classification map**.\n",
    "\n",
    "<img src=\"img/fully_conv.png\" width=\"600\" />\n",
    "\n",
    "Long, J., Shelhamer, E., & Darrell, T. (2015). Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image credits**\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from:\n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017\n",
    "- Deep Learning with Python, Francios Chollet, Manning 2018\n",
    "- Stanford cs231n course site\n",
    "- https://github.com/vdumoulin/conv_arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
