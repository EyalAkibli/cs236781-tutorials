{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "# CS236605: Deep Learning\n",
    "# Tutorial 5: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "![img](https://qph.fs.quoracdn.net/main-qimg-330e8b2941bc0164211bbdc7d5c693f3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Composed of multiple **layers**.\n",
    "\n",
    "Each layer $j$ consists of $n_j$ regular perceptrons (\"neurons\") which calculate:\n",
    "$$\n",
    "\\vec{y}_j = \\varphi\\left( \\mat{W}_j \\vec{y}_{j-1} + \\vec{b}_j \\right),~\n",
    "\\mat{W}_j\\in\\set{R}^{n_{j}\\times n_{j-1}},~ \\vec{b}_j\\in\\set{R}^{n_j}.\n",
    "$$\n",
    "\n",
    "- Note that both input and output are **vetors**. We can think of the above equation as describing a layer of **multiple perceptrons**.\n",
    "- We'll henceforth refer to such layers as **fully-connected** or FC layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given an input sample $\\vec{x}^i$, the computed function of an $L$-layer MLP is:\n",
    "$$\n",
    "\\vec{y}_L^i= \\varphi \\left(\n",
    "\\mat{W}_L \\varphi \\left( \\cdots\n",
    "\\varphi \\left( \\mat{W}_1 \\vec{x}^i + \\vec{b}_1 \\right)\n",
    "\\cdots \\right)\n",
    "+ \\vec{b}_L \\right)\n",
    "$$\n",
    "\n",
    "- Universal approximator theorem: an MLP with $L>1$, can approximate (almost) any function given enough parameters (Cybenko, 1989).\n",
    "- This expression is fully differentiable w.r.t. parameters using the Chain Rule.\n",
    "    - In practice, many challenges to calculate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression\n",
    "\n",
    "<img src=\"https://docs.microsoft.com/en-us/azure/machine-learning/studio/media/algorithm-choice/image2.png\" alt=\"regression\" width=\"600\"/>\n",
    "\n",
    "- Output: $\\hat{\\vec{y}^i} = \\vec{y}^i_L$\n",
    "- Quadratic loss: $\\sum_i (\\vec{y}^i - \\hat{\\vec{y}^i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification\n",
    "<img src=\"https://ml.berkeley.edu/blog/assets/tutorials/1/image_3.svg\" width=\"400\" alt=\"classification\">\n",
    "\n",
    "- Output: $\\hat{\\vec{y}^i} = \\mathrm{softmax}(\\vec{y}^i_L)$ (class probabilities)\n",
    "- Cross entropy loss: $\\sum_i - {\\vectr{y}}^i \\log(\\hat{\\vec{y}^i})$\n",
    "\n",
    "To explore ConvNets we'll now be focusing our attention mainly on the task of classifying images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations of MLPs for image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of parameters increases quadratically with image size due to connectivity.\n",
    "    - 28x28 MNIST image: 784 weights per neuron in the first layer\n",
    "    - 1000x1000x3 color image: 3M weights **per neuron**\n",
    "    \n",
    "    <img src=\"img/vanilla_dnn_scale.png\" width=\"400\" alt=\"scale\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Huge number of parameters greatly increases risk of overfitting\n",
    "    - MLP with 1 hidden layer, 3, 6 and 20 Neurons\n",
    "    \n",
    "    <img src=\"img/overfit_1HL_3-6-20N.jpg\" width=\"500\" alt=\"overfit1\">\n",
    "    \n",
    "    - MLP with 1, 2 and 4 hidden layers, 3 neurons each\n",
    "    \n",
    "    <img src=\"img/overfit_1-2-4HL_3N.jpg\" width=\"500\" alt=\"overfit1\">\n",
    "\n",
    "- FC layers are highly sensitivity to translation, while image features are inherently translation-agnostic\n",
    "\n",
    "Despite all these limitations we still want to use deep netural nets because they allow us to **learn hierarchical features** from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional layer is similar to an MLP FC layer but with three improtant distinctions:\n",
    "1. Each neuron is only **connected to a small region** of the previous layer's output.\n",
    "1. The neurons are stacked in a **3D** grid (insead of 1D).\n",
    "1. Neurons that are at the same depth in the grid **share the same weights** (parameters $\\mat{W},~\\vec{b}$).\n",
    "\n",
    "![cnn_layer](img/cnn_layer.jpeg)\n",
    "\n",
    "In the above image, the colors of the neurons represent their weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two important things to understand about convolutional layers:\n",
    "- They operate on and produce **volumes** (3D tensors).\n",
    "\n",
    "   <img src=\"img/cnn_layers.jpeg\" width=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each neuron is spatially local, but operates on the **full depth** dimension of it's input layer.\n",
    "\n",
    "   <img src=\"img/depthcol.jpeg\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation as filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each neuron in a given depth-slice of operates on a small region of the input layer, we can think of the combined **output of that depth-slice** as the **convolution between a filter and the input volume**.\n",
    "\n",
    "<img src=\"img/cnn_filters.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have multiple depth-slices per convolutional layer, the layer computes multiple convolutions of the same input with different kernels (filters).\n",
    "\n",
    "Each 2D slice of an input and output volume is known as **feature map** or a **channel**.\n",
    "\n",
    "[Visualization of a convolutional filter](http://cs231n.github.io/assets/conv-demo/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters & dimentions\n",
    "\n",
    "Assume an input volume of shape $(C_{\\mathrm{in}}, H_{\\mathrm{in}}, W_{\\mathrm{in}})$, i.e. channels, height, width.\n",
    "\n",
    "Define,\n",
    "\n",
    "1. Number of kernels, $K \\geq 1$.\n",
    "2. Spatial extent (size) of each kernel, $F \\geq 1$. \n",
    "3. Stride $S\\geq 1$: spatial distance between consecutive applications of a kernel.\n",
    "4. Padding $P\\geq 0$: Number of \"pixels\" to zero-pad around each input feature map.\n",
    "5. Dilation $D \\geq 1$: Spacing between kernel elements when applying to input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| $P=0,~S=1,~D=1$ | $P=1,~S=1,~D=1$ | $P=1,~S=2,~D=1$ | $P=0,~S=1,~D=2$ |\n",
    "|-----------------|-----------------|-----------------| --------------- |\n",
    "|<img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif\" width=\"200\"/>| <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/same_padding_no_strides.gif\" width=\"200\"/> | <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/padding_strides.gif\" width=\"200\"/> | <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/dilation.gif\" width=\"200\"/> |\n",
    "\n",
    "In the above animations, **blue** maps are inputs,\n",
    "**cyan** maps are outputs and\n",
    "the **shaded** area is the kernel with $F=3$.\n",
    "\n",
    "We can see that the second combination, $F=3,~P=1,~S=1,~D=1$, leads to identical sizes of input and output feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then,\n",
    "\n",
    "- Each convolution kernel will be a tensor of shape $(C_{\\mathrm{in}}, F, F)$.\n",
    "- The ouput volume dimensions will be:\n",
    "\n",
    "  $$\\begin{align}\n",
    "  H_{\\mathrm{out}} &= \\left\\lfloor \\frac{H_{\\mathrm{in}} + 2P - D\\cdot(F-1) -1}{S} \\right\\rfloor + 1\\\\\n",
    "  W_{\\mathrm{out}} &= \\left\\lfloor \\frac{W_{\\mathrm{in}} + 2P - D\\cdot(F-1) -1}{S} \\right\\rfloor + 1\\\\\n",
    "  C_{\\mathrm{out}} &= K\\\\\n",
    "  \\end{align}$$\n",
    "\n",
    "- The number of parameters in the layer will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\underbrace{K \\cdot C_{\\mathrm{in}} \\cdot F^2}_{\\mathrm{weights}} +\n",
    "\\underbrace{K}_{\\mathrm{biases}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: Input image is 1000x1000x3, and the first conv layer has $10$ kernels of size 5x5.\n",
    "The number of parameters in the first layer will be: $ 10 \\cdot 3 \\cdot 5^2 + 10 = 760 $. No dependency on the width and height of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch `Conv2d` layer example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as tvtf\n",
    "\n",
    "data_dir = os.path.join(os.getenv('HOME'), '.pytorch-datasets')\n",
    "tf = tvtf.Compose([tvtf.ToTensor()])\n",
    "ds_cifar10 = torchvision.datasets.CIFAR10(data_dir, download=True, train=True, transform=tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 shape with batch dim: torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Load first CIFAR10 image\n",
    "x0,y0 = ds_cifar10[0]\n",
    "# add batch dim\n",
    "x0 = x0.unsqueeze(0)\n",
    "print('x0 shape with batch dim:', x0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape:       torch.Size([1, 3, 32, 32])\n",
      "After first conv layer:  torch.Size([1, 10, 32, 32])\n",
      "After second conv layer: torch.Size([1, 20, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# First conv layer: works on input image volume\n",
    "conv1 = nn.Conv2d(in_channels=x0.size(1), out_channels=10, padding=1, kernel_size=3, stride=1)\n",
    "\n",
    "# Second conv layer: works on output volume of first layer\n",
    "conv2 = nn.Conv2d(in_channels=10, out_channels=20, padding=0, kernel_size=6, stride=2)\n",
    "\n",
    "print(f'{\"Input image shape:\":25s}{x0.shape}')\n",
    "print(f'{\"After first conv layer:\":25s}{conv1(x0).shape}')\n",
    "print(f'{\"After second conv layer:\":25s}{conv2(conv1(x0)).shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: observe that the width and height dimensions of the input image were never specified!\n",
    "more on the significance of that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image credits**\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from:\n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017\n",
    "- Deep Learning with Python, Francios Chollet, Manning 2018\n",
    "- Stanford cs231n course site\n",
    "- https://github.com/vdumoulin/conv_arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
