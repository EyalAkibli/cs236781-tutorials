{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "# Tutorial 5: Sequence Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- What RNNs are and how they work\n",
    "- Implementing basic RNNs models\n",
    "- Application example: sentiment analysis of movie reviews\n",
    "- Temporal Convolution Networks as an RNN alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:29:58.937598Z",
     "iopub.status.busy": "2020-10-22T03:29:58.937024Z",
     "iopub.status.idle": "2020-10-22T03:29:59.751930Z",
     "shell.execute_reply": "2020-10-22T03:29:59.752731Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:29:59.757659Z",
     "iopub.status.busy": "2020-10-22T03:29:59.757078Z",
     "iopub.status.idle": "2020-10-22T03:29:59.779906Z",
     "shell.execute_reply": "2020-10-22T03:29:59.780499Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus far, our models have been composed of fully connected (linear) layers or convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Fully connected layers\n",
    "    - Each layer $l$ operates on the output of the previous layer ($\\vec{y}_{l-1}$) and calculates,\n",
    "        $$\n",
    "        \\vec{y}_l = \\varphi\\left( \\mat{W}_l \\vec{y}_{l-1} + \\vec{b}_l \\right),~\n",
    "        \\mat{W}_l\\in\\set{R}^{n_{l}\\times n_{l-1}},~ \\vec{b}_l\\in\\set{R}^{n_l}.\n",
    "        $$\n",
    "    - FC's have completely pre-fixed input and output dimensions.\n",
    "    \n",
    "    <center><img src=\"img/mlp.png\" width=\"600\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Convolutional layers\n",
    "    - Each layer operates on an input tensor $\\vec{x}$ containing $M$ feature maps. The $k$-th feature map of the output tensor $\\vec{y}$ is:\n",
    "        $$\n",
    "        \\vec{y}^k = \\sum_{m=1}^{M} \\vec{w}^{km}\\ast\\vec{x}^m+b^k,\\ k\\in[1,K]\n",
    "        $$\n",
    "      Where $\\ast$ denotes convolution, and $K$ is the number of output feature maps.\n",
    "      \n",
    "      <center><img src=\"img/cnn_filters.png\" width=\"500\"/></center>\n",
    "      \n",
    "    - This time the weight dimensions are not dependent on the input dimensions.\n",
    "    - Weights are shared across the spatial dimensions of the input.\n",
    "    - Output dimension changes based on input dimension.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However,\n",
    "- Models based on these types of layers lack **persistent state**. \n",
    "- The current output is not affected by **previous inputs** (or outputs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How can we model a dynamical system?\n",
    "E.g., a linear system such as\n",
    "\n",
    "$$\\vec{y}_t = a_0 + a_1 \\vec{y}_{t-1}+\\dots+a_P \\vec{y}_{t-P} + b_0 \\vec{x}_t+\\dots+b_{t-Q}\\vec{x}_{t-Q}$$\n",
    "\n",
    "Many use cases and examples: text translation, sentiment analysis, scene classification in video, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An RNN layer is similar to a regular FC layer, but it has two inputs:\n",
    "- Current sample, $\\vec{x}_t \\in\\set{R}^{d_{i}}$.\n",
    "- Previous **state**, $\\vec{h}_{t-1}\\in\\set{r}^{d_{h}}$.\n",
    "\n",
    "and it produces two outputs which depend on both:\n",
    "- Current layer output, $\\vec{y}_t\\in\\set{R}^{d_o}$.\n",
    "- Current **state**, $\\vec{h}_{t}\\in\\set{r}^{d_{h}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/rnn_cell.png\" width=\"400\"/></center>\n",
    "\n",
    "Crucially,\n",
    "- The function $\\varphi(\\cdot)$ itself is not time-dependent (but is parametrized).\n",
    "- The same layer (function) is applied at successive time steps, propagating the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A basic RNN can be defined as follows.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\forall t \\geq 0:\\\\\n",
    "\\vec{h}_t &= \\varphi_h\\left( \\mat{W}_{hh} \\vec{h}_{t-1} + \\mat{W}_{xh} \\vec{x}_t + \\vec{b}_h\\right) \\\\\n",
    "\\vec{y}_t &= \\varphi_y\\left(\\mat{W}_{hy}\\vec{h}_t + \\vec{b}_y \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where,\n",
    "- $\\vec{x}_t \\in\\set{R}^{d_{i}}$ is the input at time $t$.\n",
    "- $\\vec{h}_{t-1}\\in\\set{R}^{d_{h}}$ is the **hidden state** of a fixed dimension.\n",
    "- $\\vec{y}_t\\in\\set{R}^{d_o}$ is the output at time $t$.\n",
    "- $\\mat{W}_{hh}\\in\\set{R}^{d_h\\times d_h}$, $\\mat{W}_{xh}\\in\\set{R}^{d_h\\times d_i}$, $\\mat{W}_{hy}\\in\\set{R}^{d_o\\times d_h}$, $\\vec{b}_h\\in\\set{R}^{d_h}$ and $\\vec{b}_y\\in\\set{R}^{d_o}$ are the model weights and biases.\n",
    "- $\\varphi_h$ and $\\varphi_y$ are some non-linear functions. In many cases $\\varphi_y$ is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modeling time-dependence\n",
    "\n",
    "If we imagine **unrolling** a single RNN layer through time,\n",
    "<center><img src=\"img/rnn_unrolled.png\" width=\"1200\" /></center>\n",
    "\n",
    "We can see how late outputs can now be influenced by early inputs, through the hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RNN models are very flexible in terms of input and output meaning.\n",
    "\n",
    "Common applications include image captioning, sentiment analysis, machine translation and more. \n",
    "\n",
    "<center><img src=\"img/rnn_use_cases.jpeg\" width=\"1200\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How would **backpropagation** work, though?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/bptt.png\" width=\"1000\"></center>\n",
    "\n",
    "1. Calculated loss from each output and accumulate\n",
    "2. Calculate Gradient of loss w.r.t. each parameter at each timestep\n",
    "3. For each parameter, accumulate gradients from all timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is known as **Backpropagation through time**, or BPTT.\n",
    "\n",
    "$$\n",
    "\\pderiv{L_t}{\\mat{W}} = \\sum_{k=1}^{t}\n",
    "\\pderiv{L_t}{\\hat y_t} \\cdot\n",
    "\\pderiv{\\hat y_t}{\\vec{h}_t} \\cdot\n",
    "\\pderiv{\\vec{h}_t}{\\vec{h}_k} \\cdot\n",
    "\\pderiv{\\vec{h}_k}{\\mat{W}}\n",
    "$$\n",
    "\n",
    "But how far back do we go? What's the limiting factor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We're limited in depth by vanishing and exploding gradients controlled by the eigenvalues of $\\mat{W}$.\n",
    "\n",
    "One pragmatic solution is to limit the number of timesteps involved in the backpropagation.\n",
    "\n",
    "<center><img src=\"img/tbptt.png\" width=\"1000\"></center>\n",
    "\n",
    "This is known as **Truncated backpropagation through time**, or TBPTT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multi-layered (deep) RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RNNs layers can be stacked to build a deep RNN model.\n",
    "\n",
    "<center><img src=\"img/rnn_layered.png\" width=\"1200\"/></center>\n",
    "\n",
    "- As with MLPs, adding depth allows us to model intricate hierarchical features.\n",
    "- However, now we also have a time dimension which makes the representation time-dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Based on the above equations, let's create a simple RNN layer  with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:29:59.787247Z",
     "iopub.status.busy": "2020-10-22T03:29:59.786653Z",
     "iopub.status.idle": "2020-10-22T03:29:59.808975Z",
     "shell.execute_reply": "2020-10-22T03:29:59.809661Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_dim, phi_h=torch.tanh, phi_y=torch.sigmoid):\n",
    "        super().__init__()\n",
    "        self.phi_h, self.phi_y = phi_h, phi_y\n",
    "        \n",
    "        self.fc_xh = nn.Linear(in_dim, h_dim, bias=False)\n",
    "        self.fc_hh = nn.Linear(h_dim, h_dim, bias=True)\n",
    "        self.fc_hy = nn.Linear(h_dim, out_dim, bias=True)\n",
    "        \n",
    "    def forward(self, xt, h_prev=None):\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(xt.shape[0], self.fc_hh.in_features)\n",
    "        \n",
    "        ht = self.phi_h(self.fc_xh(xt) + self.fc_hh(h_prev))\n",
    "        \n",
    "        yt = self.fc_hy(ht)\n",
    "        \n",
    "        if self.phi_y is not None:\n",
    "            yt = self.phi_y(yt)\n",
    "        \n",
    "        return yt, ht\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:29:59.813921Z",
     "iopub.status.busy": "2020-10-22T03:29:59.813121Z",
     "iopub.status.idle": "2020-10-22T03:29:59.838737Z",
     "shell.execute_reply": "2020-10-22T03:29:59.839395Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLayer(\n",
       "  (fc_xh): Linear(in_features=1024, out_features=10, bias=False)\n",
       "  (fc_hh): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (fc_hy): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate our model\n",
    "\n",
    "N = 3 # batch size\n",
    "in_dim, h_dim, out_dim = 1024, 10, 1\n",
    "\n",
    "rnn = RNNLayer(in_dim, h_dim, out_dim)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:29:59.843276Z",
     "iopub.status.busy": "2020-10-22T03:29:59.842697Z",
     "iopub.status.idle": "2020-10-22T03:29:59.872793Z",
     "shell.execute_reply": "2020-10-22T03:29:59.872147Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1: tensor([[0.5213],\n",
      "        [0.5620],\n",
      "        [0.4896]], grad_fn=<SigmoidBackward>)\n",
      "h1: tensor([[ 0.0588,  0.3526,  0.6714, -0.0943,  0.0776,  0.6714,  0.2496,  0.0098,\n",
      "         -0.8337, -0.4559],\n",
      "        [ 0.6358,  0.7648,  0.2558, -0.7108, -0.1683,  0.5659, -0.8304,  0.7617,\n",
      "         -0.7814, -0.8468],\n",
      "        [-0.5123, -0.8542,  0.4543, -0.4178, -0.4875,  0.2785,  0.2190,  0.9113,\n",
      "         -0.3511,  0.6048]], grad_fn=<TanhBackward>)\n",
      "\n",
      "y2: tensor([[0.5148],\n",
      "        [0.4038],\n",
      "        [0.3830]], grad_fn=<SigmoidBackward>)\n",
      "h2: tensor([[ 0.2656,  0.0332,  0.5322, -0.8608,  0.0190,  0.7811, -0.6733,  0.4618,\n",
      "         -0.3671, -0.6653],\n",
      "        [-0.2583,  0.1175,  0.2149, -0.4023, -0.1527, -0.4869,  0.0600, -0.2069,\n",
      "         -0.1992, -0.5492],\n",
      "        [-0.6827, -0.5860, -0.3934, -0.7404, -0.4677, -0.0076, -0.0793,  0.7699,\n",
      "         -0.7487, -0.7405]], grad_fn=<TanhBackward>)\n",
      "\n",
      "y3: tensor([[0.4094],\n",
      "        [0.4700],\n",
      "        [0.3826]], grad_fn=<SigmoidBackward>)\n",
      "h3: tensor([[ 0.1381,  0.0947,  0.4443, -0.3240,  0.3320, -0.1141, -0.2251, -0.0431,\n",
      "         -0.7440, -0.9700],\n",
      "        [-0.3997,  0.2851,  0.4468, -0.5937,  0.4601, -0.0796,  0.0319, -0.0027,\n",
      "          0.7706,  0.5392],\n",
      "        [-0.4423,  0.2801, -0.6786,  0.5398,  0.6066, -0.2879, -0.2073, -0.0164,\n",
      "         -0.7193,  0.3681]], grad_fn=<TanhBackward>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manually \"run\" a few time steps\n",
    "\n",
    "# t=1\n",
    "x1 = torch.randn(N, in_dim)\n",
    "y1, h1 = rnn(x1)\n",
    "print(f'y1: {y1}')\n",
    "print(f'h1: {h1}\\n')\n",
    "\n",
    "# t=2\n",
    "x2 = torch.randn(N, in_dim)\n",
    "y2, h2 = rnn(x2, h1)\n",
    "print(f'y2: {y2}')\n",
    "print(f'h2: {h2}\\n')\n",
    "\n",
    "# t=3\n",
    "x3 = torch.randn(N, in_dim)\n",
    "y3, h3 = rnn(x3, h2)\n",
    "print(f'y3: {y3}')\n",
    "print(f'h3: {h3}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1: Sentiment analysis for movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The task: Given a review about a movie written by some user, decide whether it's **positive**, **negative** or **neutral**.\n",
    "\n",
    "<center><img src=\"img/sentiment_analysis.png\" width=\"500\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Classically this is considered a challenging task if approached based on keywords alone.\n",
    "\n",
    "Consider:\n",
    "\n",
    "     \"This movie was actually neither that funny, nor super witty.\"\n",
    "     \n",
    "To comprehend such a sentence, it's intuitive to see that some \"state\" must be kept when \"reading\" it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset\n",
    "\n",
    "We'll use the [`torchtext`](https://github.com/pytorch/text) package, which provides useful tools for working ith textual data, and also includes some built-in datasets and dataloaders (similar to `torchvision`).\n",
    "\n",
    "Out dataset will be the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/treebank.html) (SST) dataset, which contains ~10,000 **labeled** movie reviews.\n",
    "\n",
    "The label of each review is either \"positive\", \"neutral\" or \"negative\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Loading and tokenizing text samples\n",
    "\n",
    "The `torchtext.data.Field` class takes care of splitting text into unique \"tokens\"\n",
    "(~words) and converting it a numerical representation as a sequence of numbers representing\n",
    "the tokens in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:29:59.877398Z",
     "iopub.status.busy": "2020-10-22T03:29:59.876645Z",
     "iopub.status.idle": "2020-10-22T03:30:10.801099Z",
     "shell.execute_reply": "2020-10-22T03:30:10.801599Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aviv/miniconda3/envs/temp2_cs236781/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/aviv/miniconda3/envs/temp2_cs236781/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import torchtext.data\n",
    "\n",
    "# torchtext Field objects parse text (e.g. a review) and create a tensor representation\n",
    "\n",
    "# This Field object will be used for tokenizing the movie reviews text\n",
    "review_parser = torchtext.data.Field(\n",
    "    sequential=True, use_vocab=True, lower=True,\n",
    "    init_token='<sos>', eos_token='<eos>', dtype=torch.long,\n",
    "    tokenize='spacy', tokenizer_language='en_core_web_sm'\n",
    ")\n",
    "\n",
    "# This Field object converts the text labels into numeric values (0,1,2)\n",
    "label_parser = torchtext.data.Field(\n",
    "    is_target=True, sequential=False, unk_token=None, use_vocab=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:10.805517Z",
     "iopub.status.busy": "2020-10-22T03:30:10.804806Z",
     "iopub.status.idle": "2020-10-22T03:30:18.271556Z",
     "shell.execute_reply": "2020-10-22T03:30:18.272306Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aviv/miniconda3/envs/temp2_cs236781/lib/python3.8/site-packages/torchtext/data/example.py:94: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
      "/Users/aviv/miniconda3/envs/temp2_cs236781/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
      "/Users/aviv/miniconda3/envs/temp2_cs236781/lib/python3.8/site-packages/torchtext/data/example.py:94: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 8544\n",
      "Number of test     samples: 2210\n"
     ]
    }
   ],
   "source": [
    "import torchtext.datasets\n",
    "\n",
    "# Load SST, tokenize the samples and labels\n",
    "# ds_X are Dataset objects which will use the parsers to return tensors\n",
    "ds_train, ds_valid, ds_test = torchtext.datasets.SST.splits(\n",
    "    review_parser, label_parser, root=data_dir\n",
    ")\n",
    "\n",
    "n_train = len(ds_train)\n",
    "print(f'Number of training samples: {n_train}')\n",
    "print(f'Number of test     samples: {len(ds_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lets print some examples from our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:18.279113Z",
     "iopub.status.busy": "2020-10-22T03:30:18.278584Z",
     "iopub.status.idle": "2020-10-22T03:30:18.319759Z",
     "shell.execute_reply": "2020-10-22T03:30:18.319203Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample#0111 [positive]:\n",
      " the film aims to be funny , uplifting and moving , sometimes all at once .\n",
      "\n",
      "sample#4321 [neutral ]:\n",
      " the most anti - human big studio picture since 3000 miles to graceland .\n",
      "\n",
      "sample#7777 [negative]:\n",
      " an ugly , revolting movie .\n",
      "\n",
      "sample#0000 [positive]:\n",
      " the rock is destined to be the 21st century 's new ` ` conan '' and that he 's going to make a splash even greater than arnold schwarzenegger , jean - claud van damme or steven segal .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in ([111, 4321, 7777, 0]):\n",
    "    example = ds_train[i]\n",
    "    label = example.label\n",
    "    review = str.join(\" \", example.text)\n",
    "    print(f'sample#{i:04d} [{label:8s}]:\\n {review}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Building a vocabulary\n",
    "\n",
    "The `Field` object can build a **vocabulary** for us,\n",
    "which is simply a bi-directional mapping between a unique index and a token.\n",
    "\n",
    "We'll only include words from the training set in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:18.324528Z",
     "iopub.status.busy": "2020-10-22T03:30:18.324016Z",
     "iopub.status.idle": "2020-10-22T03:30:18.433410Z",
     "shell.execute_reply": "2020-10-22T03:30:18.433935Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in training samples: 15482\n",
      "Number of tokens in training labels: 3\n"
     ]
    }
   ],
   "source": [
    "review_parser.build_vocab(ds_train)\n",
    "label_parser.build_vocab(ds_train)\n",
    "\n",
    "print(f\"Number of tokens in training samples: {len(review_parser.vocab)}\")\n",
    "print(f\"Number of tokens in training labels: {len(label_parser.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:18.437552Z",
     "iopub.status.busy": "2020-10-22T03:30:18.436796Z",
     "iopub.status.idle": "2020-10-22T03:30:18.479819Z",
     "shell.execute_reply": "2020-10-22T03:30:18.480325Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 20 tokens:\n",
      " ['<unk>', '<pad>', '<sos>', '<eos>', '.', 'the', ',', 'a', 'and', 'of', 'to', '-', 'is', \"'s\", 'it', 'that', 'in', 'as', 'but', 'film']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'first 20 tokens:\\n', review_parser.vocab.itos[:20], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note the **special tokens**, `<unk>`, `<pad>`, `<sos>` and `<eos>` at indexes `0-3`.\n",
    "These were automatically created by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:18.483605Z",
     "iopub.status.busy": "2020-10-22T03:30:18.483127Z",
     "iopub.status.idle": "2020-10-22T03:30:18.525804Z",
     "shell.execute_reply": "2020-10-22T03:30:18.526328Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word=film            index=19\n",
      "word=actor           index=492\n",
      "word=schwarzenegger  index=3404\n",
      "word=spielberg       index=715\n"
     ]
    }
   ],
   "source": [
    "# Show that some words exist in the vocab\n",
    "for w in ['film', 'actor', 'schwarzenegger', 'spielberg']:\n",
    "    print(f'word={w:15s} index={review_parser.vocab.stoi[w]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:18.529558Z",
     "iopub.status.busy": "2020-10-22T03:30:18.529060Z",
     "iopub.status.idle": "2020-10-22T03:30:18.572264Z",
     "shell.execute_reply": "2020-10-22T03:30:18.573110Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels vocab:\n",
      " {'positive': 0, 'negative': 1, 'neutral': 2}\n"
     ]
    }
   ],
   "source": [
    "print(f'labels vocab:\\n', dict(label_parser.vocab.stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Data loaders (iterators)\n",
    "\n",
    "The `torchtext` package comes with `Iterator`s, similar to the `DataLoaders` we previously worked with.\n",
    "\n",
    "A key issue when working with text sequences is that each sample is of a different length.\n",
    "\n",
    "So, how can we work with **batches** of data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:18.576603Z",
     "iopub.status.busy": "2020-10-22T03:30:18.576120Z",
     "iopub.status.idle": "2020-10-22T03:30:18.617616Z",
     "shell.execute_reply": "2020-10-22T03:30:18.618466Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aviv/miniconda3/envs/temp2_cs236781/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "# BucketIterator is supposed to created batches with samples of similar length\n",
    "# to minimize the number of <pad> tokens in the batch.\n",
    "dl_train, dl_valid, dl_test = torchtext.data.BucketIterator.splits(\n",
    "    (ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE,\n",
    "    shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Lets look at a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:18.623474Z",
     "iopub.status.busy": "2020-10-22T03:30:18.622956Z",
     "iopub.status.idle": "2020-10-22T03:30:18.673880Z",
     "shell.execute_reply": "2020-10-22T03:30:18.674393Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \n",
      " tensor([[   2,    2,    2,    2],\n",
      "        [  23,   26,  142,    7],\n",
      "        [  12,  125, 1544,  315],\n",
      "        [  32, 1059,    4, 1143],\n",
      "        [   7, 4070,    3,  207],\n",
      "        [3686, 2996,    1,    9],\n",
      "        [ 393,   21,    1, 1447],\n",
      "        [ 693,   23,    1,  510],\n",
      "        [ 317,  112,    1,    8],\n",
      "        [   6,  177,    1,  381],\n",
      "        [ 513,   26,    1,    4],\n",
      "        [   7,   71,    1,    3],\n",
      "        [ 817, 2361,    1,    1],\n",
      "        [   6,   36,    1,    1],\n",
      "        [ 961, 1741,    1,    1],\n",
      "        [   6, 5770,    1,    1],\n",
      "        [  16, 4159,    1,    1],\n",
      "        [  11,   30,    1,    1],\n",
      "        [6959,    5,    1,    1],\n",
      "        [  33, 1885,    1,    1],\n",
      "        [   4,    8,    1,    1],\n",
      "        [   3,  141,    1,    1],\n",
      "        [   1,   29,    1,    1],\n",
      "        [   1, 8073,    1,    1],\n",
      "        [   1,    4,    1,    1],\n",
      "        [   1,    3,    1,    1]]) torch.Size([26, 4])\n",
      "\n",
      "y = \n",
      " tensor([1, 2, 1, 0]) torch.Size([4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aviv/miniconda3/envs/temp2_cs236781/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(dl_train))\n",
    "\n",
    "X, y = batch.text, batch.label\n",
    "print('X = \\n', X, X.shape, end='\\n\\n')\n",
    "print('y = \\n', y, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What are we looking at?\n",
    "\n",
    "Our sample tensor `X` is of shape `(sentence_length, batch_size)`.\n",
    "\n",
    "Note that:\n",
    "1. `sentence_length` changes every batch!\n",
    "2. Sequence dimension first (not batch). This is the convention with RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model\n",
    "\n",
    "We'll now create our sentiment analysis model based on the simple `RNNLayer` we've implemented above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The model will:\n",
    "- Take an input batch of tokenized sentences.\n",
    "- Compute a dense word-embedding of each token.\n",
    "- Process the sentence **sequentially** through the RNN layer.\n",
    "- Produce a `(B, 3)` tensor, which we'll interpret as class probabilities for each sentence in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is a **word embedding**? How do we get one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Embeddings encode tokens as tensors in a way that maintain some **semantic** meaning for our task.\n",
    "\n",
    "Generally we should take a pre-trained embedding depending on our task type.\n",
    "\n",
    "Here we'll train an Embedding together with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:18.677900Z",
     "iopub.status.busy": "2020-10-22T03:30:18.677404Z",
     "iopub.status.idle": "2020-10-22T03:30:18.717241Z",
     "shell.execute_reply": "2020-10-22T03:30:18.717845Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 0, 3, 4, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0130, -0.2335,  0.6219, -0.6392,  1.4204,  1.4514, -1.3536,  0.8401],\n",
       "        [ 0.7453, -0.9093, -0.3346,  1.2128, -0.5464, -1.0639,  0.9769,  1.5244],\n",
       "        [-0.0130, -0.2335,  0.6219, -0.6392,  1.4204,  1.4514, -1.3536,  0.8401],\n",
       "        [-0.9731,  1.6934,  0.9723, -0.1298,  0.3641,  0.8168,  0.9631,  0.8763],\n",
       "        [-0.0130, -0.2335,  0.6219, -0.6392,  1.4204,  1.4514, -1.3536,  0.8401],\n",
       "        [-0.0130, -0.2335,  0.6219, -0.6392,  1.4204,  1.4514, -1.3536,  0.8401]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(num_embeddings=5, embedding_dim=8)\n",
    "\n",
    "token_idx = torch.randint(low=0, high=5, size=(6,))\n",
    "print(token_idx)\n",
    "embedding_layer(token_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "OK, model time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:18.723038Z",
     "iopub.status.busy": "2020-10-22T03:30:18.722513Z",
     "iopub.status.idle": "2020-10-22T03:30:18.760626Z",
     "shell.execute_reply": "2020-10-22T03:30:18.760068Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, in_dim, embedding_dim, h_dim, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.Embedding converts from token index to dense tensor\n",
    "        self.embedding = nn.Embedding(in_dim, embedding_dim)\n",
    "        \n",
    "        # Our own Vanilla RNN layer, without phi_y so it outputs a class score\n",
    "        self.rnn = RNNLayer(embedding_dim, h_dim, out_dim, phi_y=None)\n",
    "        \n",
    "        # To convert class scores to log-probability we'll apply log-softmax\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # X shape: (S, B) Note batch dim is not first!\n",
    "        \n",
    "        embedded = self.embedding(X) # embedded shape: (S, B, E)\n",
    "        \n",
    "        # Loop over (batch of) tokens in the sentence(s)\n",
    "        ht = None\n",
    "        for xt in embedded:\n",
    "            yt, ht = self.rnn(xt, ht) # yt is (B, D_out)\n",
    "        \n",
    "        # Class scores to log-probability\n",
    "        yt_log_proba = self.log_softmax(yt)\n",
    "        \n",
    "        return yt_log_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this model, what should the `in_dim` be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:18.764202Z",
     "iopub.status.busy": "2020-10-22T03:30:18.763670Z",
     "iopub.status.idle": "2020-10-22T03:30:18.815239Z",
     "shell.execute_reply": "2020-10-22T03:30:18.815848Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(15482, 100)\n",
       "  (rnn): RNNLayer(\n",
       "    (fc_xh): Linear(in_features=100, out_features=128, bias=False)\n",
       "    (fc_hh): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (fc_hy): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       "  (log_softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(review_parser.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 3\n",
    "\n",
    "model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Test a manual forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:18.819505Z",
     "iopub.status.busy": "2020-10-22T03:30:18.818956Z",
     "iopub.status.idle": "2020-10-22T03:30:18.866917Z",
     "shell.execute_reply": "2020-10-22T03:30:18.867544Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model(X) = \n",
      " tensor([[-1.5253, -0.9144, -0.9631],\n",
      "        [-1.3313, -1.1396, -0.8773],\n",
      "        [-1.5027, -0.9393, -0.9505],\n",
      "        [-1.5026, -0.9393, -0.9505]], grad_fn=<LogSoftmaxBackward>) torch.Size([4, 3])\n",
      "labels =  tensor([1, 2, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f'model(X) = \\n', model(X), model(X).shape)\n",
    "print(f'labels = ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How big is our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:18.871489Z",
     "iopub.status.busy": "2020-10-22T03:30:18.870937Z",
     "iopub.status.idle": "2020-10-22T03:30:18.909011Z",
     "shell.execute_reply": "2020-10-22T03:30:18.909852Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RNN model has 1,577,899 trainable weights.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The RNN model has {count_parameters(model):,} trainable weights.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why so many? We used only one RNN layer.\n",
    "\n",
    "Where are most of the weights?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training\n",
    "\n",
    "Let's complete the example by showing the regular pytorch-style train loop with this model.\n",
    "\n",
    "We'll run only a few epochs on a small subset just to test that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:18.918419Z",
     "iopub.status.busy": "2020-10-22T03:30:18.917645Z",
     "iopub.status.idle": "2020-10-22T03:30:18.961643Z",
     "shell.execute_reply": "2020-10-22T03:30:18.960680Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, dataloader, max_epochs=4, max_batches=200):\n",
    "    for epoch_idx in range(max_epochs):\n",
    "        total_loss, num_correct = 0, 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            X, y = batch.text, batch.label\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred_log_proba = model(X)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(y_pred_log_proba, y)\n",
    "            loss.backward()\n",
    "\n",
    "            # Weight updates\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            total_loss += loss.item()\n",
    "            y_pred = torch.argmax(y_pred_log_proba, dim=1)\n",
    "            num_correct += torch.sum(y_pred == y).float().item()\n",
    "\n",
    "            if batch_idx == max_batches-1:\n",
    "                break\n",
    "                \n",
    "        print(f\"Epoch #{epoch_idx}, loss={total_loss /(max_batches):.3f}, accuracy={num_correct /(max_batches*BATCH_SIZE):.3f}, elapsed={time.time()-start_time:.1f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:18.974575Z",
     "iopub.status.busy": "2020-10-22T03:30:18.973386Z",
     "iopub.status.idle": "2020-10-22T03:30:34.950101Z",
     "shell.execute_reply": "2020-10-22T03:30:34.950661Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, loss=1.084, accuracy=0.406, elapsed=3.6 sec\n",
      "Epoch #1, loss=1.069, accuracy=0.431, elapsed=3.7 sec\n",
      "Epoch #2, loss=1.058, accuracy=0.391, elapsed=4.2 sec\n",
      "Epoch #3, loss=1.059, accuracy=0.426, elapsed=4.4 sec\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "rnn_model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Recall: LogSoftmax + NLL is equiv to CrossEntropy on the class scores\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "train(rnn_model, optimizer, loss_fn, dl_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Limitations\n",
    "\n",
    "As usual this is a very naïve model, just for demonstration.\n",
    "It lacks many tricks of the NLP trade, such was pre-trained embeddings,\n",
    "gated RNN units, deep or bi-directional models, dropout, etc.\n",
    "\n",
    "Don't expect SotA results :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Temporal Convolution Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "RNNs are sometimes useful but they have some serious drawbacks:\n",
    "\n",
    "1. Vanilla RNNs are very hard to train on long sequences (large `S`) and thus are almost never used.\n",
    "Instead, more complex RNNs like LSTMs and GRUs are employed (See HW3 :).\n",
    "\n",
    "2. They require processing the input **sequentially** which results in poor performance for both training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But CNNs are great! Can we use them instead?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "OK, but how do we deal with:\n",
    "1. Input and output must both be sequences of same length, which should be arbitrary.\n",
    "1. Output at time $t$ only depends on inputs up to time $t$.\n",
    "1. Long-term dependencies across time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Input and output must both be sequences of same length, which should be arbitrary.<br>\n",
    "   $\\Longrightarrow$ 1D fully-convolutional architecture with appropriate **padding**.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Output at time $t$ only depends on inputs up to time $t$.<br>\n",
    "    $\\Longrightarrow$ **Causal convolutions**, where an output at time $t$ is convolved only with elements from time $t$ and earlier in the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Long-term dependencies across time.<br>\n",
    "    $\\Longrightarrow$ Increase receptive field using **depth** (facilitated by residual connections) and **dilated convolutions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Combining these techniques gives us the TCN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"img/tcn.png\" width=\"1400\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's implement a first a **general** TCN residual block and then a full TCN model for our Sentiment Analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:34.958007Z",
     "iopub.status.busy": "2020-10-22T03:30:34.957237Z",
     "iopub.status.idle": "2020-10-22T03:30:35.000204Z",
     "shell.execute_reply": "2020-10-22T03:30:35.000772Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, padding, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=padding, dilation=dilation),\n",
    "            nn.ReLU(), nn.Dropout(dropout)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size, stride=1, padding=padding, dilation=dilation),\n",
    "            nn.ReLU(), nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            self.channels_adapter = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.channels_adapter = None\n",
    "            \n",
    "        self.padding = padding\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Main branch\n",
    "        out = self.conv1(x)\n",
    "        out = out[..., :-self.padding] # maintain causality\n",
    "        out = self.conv2(out)\n",
    "        out = out[..., :-self.padding]\n",
    "\n",
    "        # Skip-connection (residual branch)\n",
    "        skip = x if not self.channels_adapter else self.channels_adapter(x)\n",
    "            \n",
    "        out = torch.relu(out + skip)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:35.006954Z",
     "iopub.status.busy": "2020-10-22T03:30:35.006231Z",
     "iopub.status.idle": "2020-10-22T03:30:35.049633Z",
     "shell.execute_reply": "2020-10-22T03:30:35.050204Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentTCN(nn.Module):\n",
    "    def __init__(self, in_dim: int, embedding_dim: int, layer_channels: list, out_dim: int, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        assert len(layer_channels) > 0\n",
    "\n",
    "        self.embedding = nn.Embedding(in_dim, embedding_dim)\n",
    "        \n",
    "        tcn_channels = [embedding_dim] + layer_channels + [out_dim]\n",
    "        layers = []\n",
    "        for i, (c_in, c_out) in enumerate(zip(tcn_channels[:-1], tcn_channels[1:])):\n",
    "            # Exponentially-increasing dilation\n",
    "            dilation = 2 ** i\n",
    "            # Control padding to maintain output size\n",
    "            padding = (kernel_size - 1) * dilation\n",
    "            \n",
    "            layers.append(\n",
    "                TCNBlock(c_in, c_out, kernel_size, dilation=dilation, padding=padding, dropout=dropout)\n",
    "            )\n",
    "            \n",
    "        self.blocks = nn.Sequential(*layers)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, **kw): # x is (S, B)\n",
    "        # First we need to embed our sequence.\n",
    "        # Note how we treat the E as channels for the convulutions.\n",
    "        x_emb = self.embedding(x) # (S, B, E)\n",
    "        x_emb = torch.transpose(x_emb, 0, 1) # (B, S, E)\n",
    "        x_emb = torch.transpose(x_emb, 1, 2) # (B, E, S)\n",
    "        \n",
    "        # Process the entire sequence at once\n",
    "        y_seq = self.blocks(x_emb) # (B, D_out, S)\n",
    "        \n",
    "        # Output predictions\n",
    "        yt = y_seq[..., -1] # (B, D_out)\n",
    "        yt_log_proba = self.log_softmax(yt)\n",
    "        return yt_log_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:35.053498Z",
     "iopub.status.busy": "2020-10-22T03:30:35.052764Z",
     "iopub.status.idle": "2020-10-22T03:30:35.112802Z",
     "shell.execute_reply": "2020-10-22T03:30:35.113802Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentTCN(\n",
       "  (embedding): Embedding(15482, 100)\n",
       "  (blocks): Sequential(\n",
       "    (0): TCNBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv1d(100, 32, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (channels_adapter): Conv1d(100, 32, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "    (1): TCNBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): TCNBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv1d(32, 3, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv1d(3, 3, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (channels_adapter): Conv1d(32, 3, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (log_softmax): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcn = SentimentTCN(INPUT_DIM, EMBEDDING_DIM, [32, 32], OUTPUT_DIM, kernel_size=3)\n",
    "tcn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice where the `channel_adapter`s are. What's their purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:35.119056Z",
     "iopub.status.busy": "2020-10-22T03:30:35.118196Z",
     "iopub.status.idle": "2020-10-22T03:30:35.159401Z",
     "shell.execute_reply": "2020-10-22T03:30:35.160358Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TCN model has 1,570,796 trainable weights.\n"
     ]
    }
   ],
   "source": [
    "print(f'The TCN model has {count_parameters(tcn):,} trainable weights.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try a forward pass with this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:35.165269Z",
     "iopub.status.busy": "2020-10-22T03:30:35.164344Z",
     "iopub.status.idle": "2020-10-22T03:30:35.213431Z",
     "shell.execute_reply": "2020-10-22T03:30:35.214309Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8909, -1.2270, -1.2157],\n",
       "        [-0.7391, -1.3638, -1.3213],\n",
       "        [-0.8258, -1.2744, -1.2640],\n",
       "        [-0.8708, -1.2509, -1.2203]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcn(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And finally, let's train the new model using the exact same setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-22T03:30:35.220077Z",
     "iopub.status.busy": "2020-10-22T03:30:35.219109Z",
     "iopub.status.idle": "2020-10-22T03:30:46.935023Z",
     "shell.execute_reply": "2020-10-22T03:30:46.935541Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, loss=1.081, accuracy=0.434, elapsed=2.9 sec\n",
      "Epoch #1, loss=1.084, accuracy=0.410, elapsed=2.9 sec\n",
      "Epoch #2, loss=1.077, accuracy=0.415, elapsed=3.0 sec\n",
      "Epoch #3, loss=1.079, accuracy=0.425, elapsed=2.9 sec\n"
     ]
    }
   ],
   "source": [
    "tcn_model = SentimentTCN(INPUT_DIM, EMBEDDING_DIM, [32, 32], OUTPUT_DIM, kernel_size=3)\n",
    "optimizer = optim.Adam(tcn_model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "train(tcn_model, optimizer, loss_fn, dl_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice how:\n",
    "1. The model took sequences of different length every batch, just like an RNN.\n",
    "2. Sequences were processed in **parallel** w.r.t. time, unlike RNN.\n",
    "3. Receptive field was controlled by architecture, not by sequence length, unlike RNN.\n",
    "4. No need for BPTT, unlike RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Image credits**\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from:\n",
    "\n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017\n",
    "- Andrej Karpathy, http://karpathy.github.io\n",
    "- MIT 6.S191\n",
    "- Stanford cs231n\n",
    "- S. Bai et al. 2018, http://arxiv.org/abs/1803.01271"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
