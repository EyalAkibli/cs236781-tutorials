{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "# Tutorial 7: Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory Reminders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Intuitively, some parts of the input may be more important than others.\n",
    "\n",
    "An **Attention** mechanism, allows the model to \"focus\" on, i.e. give a *greater weight* to\n",
    "different parts of the input or some other intermetiate part of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example from an image captioning [paper](https://arxiv.org/pdf/1502.03044.pdf) (K. Xu et al. 2015):\n",
    "\n",
    "<img src=\"img/attn_ic1.png\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/attn_ic2.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Input soft attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One place to apply attention is to the **input features**.\n",
    "\n",
    "In the context of our RNN model, we can change it's hidden state update to:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{a}_t &= \\sigma\\left( \\mat{W}_{ha} \\vec{h}_{t-1} + \\mat{W}_{xa} \\vec{x}_t+ \\vec{b}_a\\right) \\\\\n",
    "\\vec{g}_t &= \\mathrm{softmax}(\\alpha \\vec{a}_t) \\\\\n",
    "\\vec{h}_t &= \\varphi_h\\left( \\mat{W}_{hh} \\vec{h}_{t-1} + \\mat{W}_{xh} (\\vec{x}_t \\odot \\vec{g}_t)+ \\vec{b}_h\\right) \\\\\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLayerInputAttn(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, out_dim, phi_h=torch.tanh, phi_y=torch.sigmoid):\n",
    "        super().__init__()\n",
    "        self.phi_h, self.phi_y = phi_h, phi_y\n",
    "        \n",
    "        # Attention parameters\n",
    "        self.fc_xa = nn.Linear(in_dim, in_dim, bias=False)\n",
    "        self.fc_ha = nn.Linear(h_dim, in_dim, bias=True)\n",
    "        \n",
    "        # Regular RNN parameters\n",
    "        self.fc_xh = nn.Linear(in_dim, h_dim, bias=False)\n",
    "        self.fc_hh = nn.Linear(h_dim, h_dim, bias=True)\n",
    "        self.fc_hy = nn.Linear(h_dim, out_dim, bias=True)\n",
    "        \n",
    "    def forward(self, xt, h_prev=None):\n",
    "        if h_prev is None:\n",
    "            h_prev = torch.zeros(xt.shape[0], self.fc_hh.in_features)\n",
    "            \n",
    "        # Calculate the attention gating gt: a weight for each feature of x\n",
    "        at = torch.sigmoid(self.fc_xa(xt) + self.fc_ha(h_prev))\n",
    "        gt = torch.softmax(at, dim=1)\n",
    "        \n",
    "        # Apply regular RNN with gated input\n",
    "        ht = self.phi_h(self.fc_xh(xt * gt) + self.fc_hh(h_prev))\n",
    "        \n",
    "        yt = self.fc_hy(ht)\n",
    "        \n",
    "        if self.phi_y is not None:\n",
    "            yt = self.phi_y(yt)\n",
    "        \n",
    "        return yt, ht\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can interpret this as a soft (differentiable) gating of the input.\n",
    "\n",
    "This makes sense for image captioning, where we want to emphasize image regions based on their feature maps.\n",
    "\n",
    "What about our sentiment analysis task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Self attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another place to apply attention in the context of RNNs is to the **hidden states**.\n",
    "\n",
    "In an ICLR 2017 [paper](https://arxiv.org/pdf/1703.03130.pdf), Lin et al. proposed\n",
    "an attention for sentiment analysis.\n",
    "\n",
    "<img src=\"img/self_attn_sa.png\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problem with applying attention to the hidden state vectors, is that their number changes each batch,\n",
    "depdending on the sentence length.\n",
    "\n",
    "This approach creates a **sentence embedding** $M$ of a fixed size:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mat{H}_T &= \\sigma\\left[ \\vectr{h}_1; \\dots; \\vectr{h}_T \\right] \\in\\set{R}^{T\\times d_h}\\\\\n",
    "\\mat{A} &= \\mathrm{softmax}\\left(\\mat{W}_{s2} \\tanh\\left( \\mat{W}_{s1} \\mattr{H}_T \\right) \\right),\\ \n",
    "\\mat{W}_{s1}\\in\\set{R}^{d_a \\times d_h},\\ \\mat{W}_{s2}\\in\\set{R}^{r \\times d_a} \\\\\n",
    "\\mat{M} &= \\mat{A}\\mat{H}_T \\in\\set{R}^{r\\times d_h}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "The sentence embedding $M$ is then fed into an FC classifier to produce the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Self excercise:* Modify our `SentimentRNN` and add the Self-Attantion layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5363, 0.4637, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4384, 0.5616, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5111, 0.4889, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3149, 0.6851, 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.2433, 0.3085, 0.4482, 0.0000, 0.0000],\n",
       "         [0.2871, 0.3667, 0.3463, 0.0000, 0.0000],\n",
       "         [0.2354, 0.3299, 0.4346, 0.0000, 0.0000],\n",
       "         [0.3343, 0.4579, 0.2078, 0.0000, 0.0000]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [1.0000, 0.0000, 0.0000, 0.0000, 0.0000]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "def valid_softmax2d(X: Tensor, valid_len: Tensor=None):\n",
    "    if valid_len is None:\n",
    "        return torch.softmax(X, dim=-1)\n",
    "    \n",
    "    assert X.ndim == 2\n",
    "    assert valid_len.ndim == 1\n",
    "    assert X.shape[0] == valid_len.shape[0]\n",
    "    \n",
    "    B, S = X.shape\n",
    "    mask = torch.arange(S)[None,:] < valid_len[:,None]\n",
    "    \n",
    "    X = X.to(torch.float)\n",
    "    X[~mask] = float('-inf')\n",
    "    return torch.softmax(X, dim=-1)\n",
    "    \n",
    "valid_softmax2d(torch.ones(3,4), valid_len=torch.tensor([3, 2, 1]))\n",
    "\n",
    "#%%\n",
    "\n",
    "def valid_softmax3d(X: Tensor, valid_len: Tensor=None):\n",
    "    \"\"\"\n",
    "    Applied masked softmax on the last dimension of a 3d tensor.\n",
    "    :param X: A 3d Tensor fo shape (B,E,S).\n",
    "    :param valid_len: A tensor of shape (B,) representing the valid length \n",
    "    for each sample. Can be None, which means softmax will be applied \n",
    "    without any masking.\n",
    "    :return: X after setting elements after the valid_length to -inf \n",
    "    on the last dimension and applying softmax.\n",
    "    \"\"\"\n",
    "    assert X.ndim == 3\n",
    "    B, E, S = X.shape\n",
    "    X2d = X.view(B*E, S)\n",
    "    \n",
    "    if valid_len is not None:\n",
    "        valid_len = valid_len.repeat_interleave(E, dim=0)\n",
    "        \n",
    "    X2d_s = valid_softmax2d(X2d, valid_len)\n",
    "    return X2d_s.view(B,E,S)\n",
    "\n",
    "#%%\n",
    "\n",
    "X = torch.rand(3,4,5)\n",
    "v = torch.tensor([2,3,1])\n",
    "\n",
    "valid_softmax3d(X, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPAttention(nn.Module):\n",
    "    def __init__(self, q_dim, k_dim, v_dim, h_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.wk = nn.Linear(k_dim, h_dim, bias=False)\n",
    "        self.wq = nn.Linear(q_dim, h_dim, bias=False)\n",
    "        self.v  = nn.Linear(h_dim, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor, valid_len: Tensor=None):\n",
    "        \"\"\"\n",
    "        :param q: Queries tensor of shape (B, Q, q_dim)\n",
    "        :param k: Keys tensor of shape (B, KV, k_dim)\n",
    "        :param v: Values tensor of shape (B, KV, v_dim)\n",
    "        :param valid_len: Sequence lengths tensor of shape (B,).\n",
    "        :return: Attended values tensor, of shape (B, Q, v_dim).\n",
    "        \"\"\"\n",
    "        # (B, KV, k_dim) -> (B, KV, h_dim) -> (B, 1, KV, h_dim)\n",
    "        wk_k = self.wk(k).unsqueeze(1)\n",
    "        \n",
    "        # (B, Q, q_dim)  -> (B, Q, h_dim)  -> (B, Q, 1, h_dim)\n",
    "        wq_q = self.wq(q).unsqueeze(2)\n",
    "        \n",
    "        # (B, Q, KV, h_dim)\n",
    "        z1 = torch.tanh(wq_q + wk_k)\n",
    "        \n",
    "        # (B, Q, KV, 1) -> (B, Q, KV)\n",
    "        z2 = self.v(z1).squeeze(dim=-1)\n",
    "        \n",
    "        a = valid_softmax3d(z2, valid_len)\n",
    "        a = self.dropout(a)\n",
    "        \n",
    "        # (B, Q, KV) * (B, KV, v_dim) = (B, Q, v_dim)\n",
    "        return torch.bmm(a, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  1.1111,  2.2222,  3.3333]],\n",
      "\n",
      "        [[ 7.4074,  8.3333,  9.2593, 10.1852]]], grad_fn=<BmmBackward>) torch.Size([2, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "keys = torch.ones(2, 10, 2, dtype=torch.float) \n",
    "vals = torch.arange(40, dtype=torch.float).reshape(1, 10, 4).repeat_interleave(2, dim=0)\n",
    "ques = torch.ones(2, 1, 2, dtype=torch.float)\n",
    "\n",
    "mlp_attn = MLPAttention(ques.shape[-1], keys.shape[-1], vals.shape[-1], 100, dropout=0.1)\n",
    "out = mlp_attn(ques, keys, vals, valid_len=torch.tensor([1, 6]))\n",
    "print(out, out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 8544\n",
      "Number of test     samples: 2210\n"
     ]
    }
   ],
   "source": [
    "import torchtext.data\n",
    "import torchtext.datasets\n",
    "\n",
    "# torchtext Field objects parse text (e.g. a review) and create a tensor representation\n",
    "# This Field object will be used for tokenizing the movie reviews text\n",
    "review_parser = torchtext.data.Field(\n",
    "    sequential=True, use_vocab=True, lower=True,\n",
    "    init_token='<sos>', eos_token='<eos>', dtype=torch.long,\n",
    "    tokenize='spacy', tokenizer_language='en_core_web_sm'\n",
    ")\n",
    "\n",
    "# This Field object converts the text labels into numeric values (0,1,2)\n",
    "label_parser = torchtext.data.Field(\n",
    "    is_target=True, sequential=False, unk_token=None, use_vocab=True\n",
    ")\n",
    "\n",
    "\n",
    "# Load SST, tokenize the samples and labels\n",
    "# ds_X are Dataset objects which will use the parsers to return tensors\n",
    "ds_train, ds_valid, ds_test = torchtext.datasets.SST.splits(\n",
    "    review_parser, label_parser, root=data_dir\n",
    ")\n",
    "\n",
    "review_parser.build_vocab(ds_train)\n",
    "label_parser.build_vocab(ds_train)\n",
    "\n",
    "n_train = len(ds_train)\n",
    "print(f'Number of training samples: {n_train}')\n",
    "print(f'Number of test     samples: {len(ds_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "# BucketIterator creates batches with samples of similar length\n",
    "# to minimize the number of <pad> tokens in the batch.\n",
    "dl_train, dl_valid, dl_test = torchtext.data.BucketIterator.splits(\n",
    "    (ds_train, ds_valid, ds_test), batch_size=BATCH_SIZE,\n",
    "    shuffle=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    def __init__(self, in_dim, embedding_dim, h_dim, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.Embedding converts from token index to dense tensor\n",
    "        self.embedding = nn.Embedding(in_dim, embedding_dim)\n",
    "        \n",
    "        # PyTorch multilayer GRU RNN\n",
    "        self.rnn = nn.GRU(embedding_dim, h_dim, num_layers=2, bias=False)\n",
    "        \n",
    "        # Our custom MLP attention\n",
    "        self.attn = MLPAttention(h_dim, h_dim, h_dim, h_dim)\n",
    "        \n",
    "        # Output layer to create class scores\n",
    "        self.out_fc = nn.Linear(h_dim, out_dim, bias=True)\n",
    "        \n",
    "        # To convert class scores to log-probability we'll apply log-softmax\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # X shape: (S, B) Note batch dim is not first!\n",
    "        S, B = X.shape\n",
    "        embedded = self.embedding(X) # embedded shape: (S, B, E)\n",
    "        \n",
    "        # GRU returs all hidden states (S, B, H)\n",
    "        h, _ = self.rnn(embedded)\n",
    "        \n",
    "        # Transpose to (B, S, H)\n",
    "        h = h.transpose(0, 1)\n",
    "        \n",
    "        # Apply self-attention to hidden states -> (B, S, H) and transpose -> (B, H, S)\n",
    "        # This gives us S self-weighted hidden states\n",
    "        a = self.attn(h, h, h)\n",
    "        a = a.transpose(-2, -1)\n",
    "        \n",
    "        # Create sentence embedding: apply attention to hidden states\n",
    "        # (B, H, S) * (B, S, H) -> (B, H, H) -> (B, H)\n",
    "        # m = torch.bmm(a, h).view(B, -1)\n",
    "        \n",
    "        # Create sentence embedding: average weighted hidden states over sequence\n",
    "        # (B, H, S) -> (B, H)\n",
    "        m = torch.mean(a, dim=2)\n",
    "        \n",
    "        # Create output scores: (B, out_dim)\n",
    "#         print('a.shape', a.shape)\n",
    "#         print('h.shape', h.shape)\n",
    "#         print('m.shape', m.shape)\n",
    "        \n",
    "        yhat = self.out_fc(m)\n",
    "        \n",
    "        # Class scores to log-probability\n",
    "        yhat_log_proba = self.log_softmax(yhat)\n",
    "        \n",
    "        return yhat_log_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentRNN(\n",
       "  (embedding): Embedding(15482, 100)\n",
       "  (rnn): GRU(100, 128, num_layers=2, bias=False)\n",
       "  (attn): MLPAttention(\n",
       "    (wk): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (wq): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (v): Linear(in_features=128, out_features=1, bias=False)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (out_fc): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (log_softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM = len(review_parser.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 3\n",
    "\n",
    "model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, y0 = next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51, 4])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1210, -0.9384, -1.2630],\n",
      "        [-1.0988, -0.9599, -1.2595],\n",
      "        [-1.1440, -1.0450, -1.1094],\n",
      "        [-1.1052, -0.9209, -1.3068]], grad_fn=<LogSoftmaxBackward>) torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "yhat0 = model(x0)\n",
    "print(yhat0, yhat0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, dataloader, max_epochs=4, max_batches=200):\n",
    "    for epoch_idx in range(max_epochs):\n",
    "        total_loss, num_correct = 0, 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            X, y = batch.text, batch.label\n",
    "\n",
    "            # Forward pass\n",
    "            y_pred_log_proba = model(X)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(y_pred_log_proba, y)\n",
    "            loss.backward()\n",
    "\n",
    "            # Weight updates\n",
    "            optimizer.step()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            total_loss += loss.item()\n",
    "            y_pred = torch.argmax(y_pred_log_proba, dim=1)\n",
    "            num_correct += torch.sum(y_pred == y).float().item()\n",
    "\n",
    "            if batch_idx == max_batches-1:\n",
    "                break\n",
    "                \n",
    "        print(f\"Epoch #{epoch_idx}, loss={total_loss /(max_batches):.3f}, accuracy={num_correct /(max_batches*BATCH_SIZE):.3f}, elapsed={time.time()-start_time:.1f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0, loss=1.048, accuracy=0.424, elapsed=7.7 sec\n",
      "Epoch #1, loss=1.040, accuracy=0.469, elapsed=7.9 sec\n",
      "Epoch #2, loss=1.025, accuracy=0.471, elapsed=7.7 sec\n",
      "Epoch #3, loss=1.028, accuracy=0.476, elapsed=8.4 sec\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "rnn_model = SentimentRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "\n",
    "optimizer = optim.Adam(rnn_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Recall: LogSoftmax + NLL is equiv to CrossEntropy on the class scores\n",
    "loss_fn = nn.NLLLoss()\n",
    "\n",
    "train(rnn_model, optimizer, loss_fn, dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = rnn.LSTM(num_hiddens, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        X = self.embedding(X)  # X shape: (batch_size, seq_len, embed_size)\n",
    "        # RNN needs first axes to be timestep, i.e., seq_len\n",
    "        X = X.swapaxes(0, 1)\n",
    "        state = self.rnn.begin_state(batch_size=X.shape[1], ctx=X.context)\n",
    "        out, state = self.rnn(X, state)\n",
    "        # out shape: (seq_len, batch_size, num_hiddens)\n",
    "        # state shape: (num_layers, batch_size, num_hiddens),\n",
    "        # where \"state\" contains the hidden state and the memory cell\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Image credits**\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from:\n",
    "\n",
    "- Zhang et al., Dive into Deep Learning, 2019\n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017\n",
    "- Andrej Karpathy, http://karpathy.github.io\n",
    "- MIT 6.S191\n",
    "- Stanford cs231n\n",
    "- K. Xu et al. 2015, https://arxiv.org/abs/1502.03044"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
