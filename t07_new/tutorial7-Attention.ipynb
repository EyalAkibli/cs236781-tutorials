{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\Tr}[0]{^\\top}\n",
    "\\newcommand{\\softmax}[1]{\\mathrm{softmax}\\left({#1}\\right)}\n",
    "$$\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "# Tutorial 7: Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the context of learning from **sequences** of inputs, we have seen RNNs as a model capable of learning a transformation of one sequence into another.\n",
    "\n",
    "<center><img src=\"img/rnn_unrolled.png\" width=\"1000\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\forall t \\geq 0:\\\\\n",
    "\\vec{h}_t &= \\varphi_h\\left( \\mat{W}_{hh} \\vec{h}_{t-1} + \\mat{W}_{xh} \\vec{x}_t + \\vec{b}_h\\right) \\\\\n",
    "\\vec{y}_t &= \\varphi_y\\left(\\mat{W}_{hy}\\vec{h}_t + \\vec{b}_y \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, RNNs (even the fancy ones) have some major drawbacks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Input must be processed sequentially.\n",
    "2. Hard to train on long sequences (needs BPTT).\n",
    "3. Difficult to learn long-term dependencies, e.g. between late outputs and early inputs. The **hidden state** has the burden of \"remembering\" the \"meaning\" of the entire sequence so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we want to translate text from English to French. The general approach using RNNs is to design a Sequence-to-sequence (**Seq2Seq**) Encoder-Decoder architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/seq2seq.svg\" width=\"1000\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such an architecture the **last** hidden state must encode all the information the decoder needs for translation.\n",
    "\n",
    "**Local** information, i.e. the encoder outputs and intermediate hidden states is discarded.\n",
    "\n",
    "Can we use this local info to help the decoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning contexts, **attention** is a term used for a family of related mechanisms which, in general, learn to predict some probability distribution over a sequence of elements.\n",
    "\n",
    "Intuitively, this allows a model to \"pay more attention\" to elements from the sequence which get a higher probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent versions of attention mechanisms can be defined formally as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given:\n",
    "- $n$ **key-value** pairs: $\\left\\{\\left(\\vec{k}_i, \\vec{v}_i\\right)\\right\\}_{i=1}^{n}$, where $\\vec{k}_i\\in\\set{R}^{d_k}$, $\\vec{v}_i\\in\\set{R}^{d_v}$\n",
    "- A **query**, $\\vec{q} \\in\\set{R}^{d_q}$\n",
    "- Some similarity function between keys and queries, $s: \\set{R}^{d_k}\\times \\set{R}^{d_q} \\mapsto \\set{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An attention mechanism computes a weighted sum of the **values**,\n",
    "\n",
    "$$\n",
    "\\vec{o} = \\sum_{i=1}^{n} a_i \\vec{v}_i\\ \\in \\set{R}^{d_v},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where attention weights $a_i$ are computed according the the similarity between the **query** and each **key**:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "b_i &= s(\\vec{k}_i, \\vec{q}) \\\\\n",
    "\\vec{b} &= \\left[  b_1, \\dots, b_n \\right]\\Tr \\\\\n",
    "\\vec{a} &= \\softmax{\\vec{b}}.\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplicative attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One basic type of attention mechanism uses a simple **dot product** as the similarity function.\n",
    "\n",
    "Widely-used by models based on the **Transformer** architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume $d_k=d_q=d$, then\n",
    "\n",
    "$$\n",
    "s(\\vec{k},\\vec{q})= \\frac{\\vectr{k}\\vec{q}}{\\sqrt{d}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why scale by $\\sqrt{d}$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the factor at which the dot-product grows due to the dimensionality. E.g.,\n",
    "\n",
    "$$\n",
    "\\norm{\\vec{1}_d}_2 = \\norm{[1,\\dots,1]\\Tr}_2 = \\sqrt{d\\cdot 1^2} =\\sqrt{d}.\n",
    "$$\n",
    "\n",
    "This helps keep the softmax values from becoming very small when the dimension is large, and therefore helps prevents tiny gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now deal with $m$ queries simultaneously by stacking them in a matrix $\\mat{Q} \\in \\set{R}^{m\\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we'll stack the keys and values in their own matrices, $\\mat{K}\\in\\set{R}^{n\\times d}$, $\\mat{V}\\in\\set{R}^{n\\times d_v}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can compute the attention weights for all queries in parallel:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mat{B} &= \\frac{1}{\\sqrt{d}} \\mat{Q}\\mattr{K}  \\ \\in\\set{R}^{m\\times n} \\\\\n",
    "\\mat{A} &= \\softmax{\\mat{B}},\\ \\mathrm{dim}=1 \\\\\n",
    "\\mat{O} &= \\mat{A}\\mat{V} \\ \\in\\set{R}^{m\\times d_v}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the softmax is applied per-row, and so each row $i$ of $\\mat{A}$ contains the attention weights for the $i$th query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also notice that in this formulation, we **input a sequence** of $m$ queries and get an **output sequence** of $m$ weighed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive attention based on an MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common type of attention mechanism uses an MLP to **learn** the similarity function $s(\\vec{k},\\vec{q})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this type of attention, the similarity function is \n",
    "\n",
    "$$\n",
    "s(\\vec{k},\\vec{q}) = \\vectr{v} \\tanh(\\mat{W}_k\\vec{k} + \\mat{W}_q\\vec{q}),\n",
    "$$\n",
    "\n",
    "where $\\mat{W}_k\\in\\set{R}^{h\\times d_k}$, $\\mat{W}_q\\in\\set{R}^{h\\times d_q}$ and $\\vec{v}\\in\\set{R}^{h}$ are trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Seq2Seq language translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "field_args = dict(tokenize='spacy', init_token = '<sos>', eos_token = '<eos>', lower = False) \n",
    "src_field = Field(tokenizer_language=\"de_core_news_sm\", **field_args)\n",
    "tgt_field = Field(tokenizer_language=\"en_core_web_sm\", **field_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_valid, ds_test = Multi30k.splits(\n",
    "    root=data_dir, exts=('.de', '.en'), fields=(src_field, tgt_field)\n",
    ")\n",
    "\n",
    "src_field.build_vocab(ds_train)\n",
    "tgt_field.build_vocab(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18659"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_field.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9799"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tgt_field.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample#0000:\n",
      "\tDE: zwei junge weiße männer sind im freien in der nähe vieler büsche .\n",
      "\tEN: two young , white males are outside near many bushes .\n",
      "\n",
      "sample#0010:\n",
      "\tDE: eine ballettklasse mit fünf mädchen , die nacheinander springen .\n",
      "\tEN: a ballet class of five girls jumping in sequence .\n",
      "\n",
      "sample#0100:\n",
      "\tDE: männliches kleinkind in einem roten hut , das sich an einem geländer festhält .\n",
      "\tEN: toddler boy in a red hat holding on to some railings .\n",
      "\n",
      "sample#1000:\n",
      "\tDE: ein junger mann in einem weißen hemd , der tomaten schneidet .\n",
      "\tEN: a young man in a white shirt cutting tomatoes .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in ([0, 10, 100, 1000]):\n",
    "    example = ds_train[i]\n",
    "    src = str.join(\" \", example.src)\n",
    "    tgt = str.join(\" \", example.trg)\n",
    "    print(f'sample#{i:04d}:\\n\\tDE: {src}\\n\\tEN: {tgt}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Image credits**\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from:\n",
    "\n",
    "- Zhang et al., Dive into Deep Learning, 2019\n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017\n",
    "- Andrej Karpathy, http://karpathy.github.io\n",
    "- MIT 6.S191\n",
    "- Stanford cs231n\n",
    "- K. Xu et al. 2015, https://arxiv.org/abs/1502.03044"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
