{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\Tr}[0]{^\\top}\n",
    "\\newcommand{\\softmax}[1]{\\mathrm{softmax}\\left({#1}\\right)}\n",
    "$$\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "# Tutorial 7: Seq2Seq and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Attention Mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the context of learning from **sequences** of inputs, we have seen RNNs as a model capable of learning a transformation of one sequence into another.\n",
    "\n",
    "<center><img src=\"img/rnn_unrolled.png\" width=\"1000\" /></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\forall t \\geq 0:\\\\\n",
    "\\vec{h}_t &= \\varphi_h\\left( \\mat{W}_{hh} \\vec{h}_{t-1} + \\mat{W}_{xh} \\vec{x}_t + \\vec{b}_h\\right) \\\\\n",
    "\\vec{y}_t &= \\varphi_y\\left(\\mat{W}_{hy}\\vec{h}_t + \\vec{b}_y \\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, RNNs (even the fancy ones) have some major drawbacks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Input must be processed sequentially.\n",
    "2. Hard to train on long sequences (needs BPTT).\n",
    "3. Difficult to learn long-term dependencies, e.g. between late outputs and early inputs. The **hidden state** has the burden of \"remembering\" the \"meaning\" of the entire sequence so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we want to translate text from English to French. The general approach using RNNs is to design a Sequence-to-sequence (**Seq2Seq**) Encoder-Decoder architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/seq2seq.svg\" width=\"1000\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such an architecture the **last** hidden state must encode all the information the decoder needs for translation.\n",
    "\n",
    "**Local** information, i.e. the encoder outputs and intermediate hidden states is discarded.\n",
    "\n",
    "Can we use this local info to help the decoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In deep learning contexts, **attention** is a term used for a family of related mechanisms which, in general, learn to predict some probability distribution over a sequence of elements.\n",
    "\n",
    "Intuitively, this allows a model to \"pay more attention\" to elements from the sequence which get a higher probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recent versions of attention mechanisms can be defined formally as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given:\n",
    "- $n$ **key-value** pairs: $\\left\\{\\left(\\vec{k}_i, \\vec{v}_i\\right)\\right\\}_{i=1}^{n}$, where $\\vec{k}_i\\in\\set{R}^{d_k}$, $\\vec{v}_i\\in\\set{R}^{d_v}$\n",
    "- A **query**, $\\vec{q} \\in\\set{R}^{d_q}$\n",
    "- Some similarity function between keys and queries, $s: \\set{R}^{d_k}\\times \\set{R}^{d_q} \\mapsto \\set{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An attention mechanism computes a weighted sum of the **values**,\n",
    "\n",
    "$$\n",
    "\\vec{o} = \\sum_{i=1}^{n} a_i \\vec{v}_i\\ \\in \\set{R}^{d_v},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where attention weights $a_i$ are computed according the the similarity between the **query** and each **key**:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "b_i &= s(\\vec{k}_i, \\vec{q}) \\\\\n",
    "\\vec{b} &= \\left[  b_1, \\dots, b_n \\right]\\Tr \\\\\n",
    "\\vec{a} &= \\softmax{\\vec{b}}.\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplicative attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One basic type of attention mechanism uses a simple **dot product** as the similarity function.\n",
    "\n",
    "Widely-used by models based on the **Transformer** architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume $d_k=d_q=d$, then\n",
    "\n",
    "$$\n",
    "s(\\vec{k},\\vec{q})= \\frac{\\vectr{k}\\vec{q}}{\\sqrt{d}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why scale by $\\sqrt{d}$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the factor at which the dot-product grows due to the dimensionality. E.g.,\n",
    "\n",
    "$$\n",
    "\\norm{\\vec{1}_d}_2 = \\norm{[1,\\dots,1]\\Tr}_2 = \\sqrt{d\\cdot 1^2} =\\sqrt{d}.\n",
    "$$\n",
    "\n",
    "This helps keep the softmax values from becoming very small when the dimension is large, and therefore helps prevent tiny gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now deal with $m$ queries simultaneously by stacking them in a matrix $\\mat{Q} \\in \\set{R}^{m\\times d}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we'll stack the keys and values in their own matrices, $\\mat{K}\\in\\set{R}^{n\\times d}$, $\\mat{V}\\in\\set{R}^{n\\times d_v}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can compute the attention weights for all queries in parallel:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mat{B} &= \\frac{1}{\\sqrt{d}} \\mat{Q}\\mattr{K}  \\ \\in\\set{R}^{m\\times n} \\\\\n",
    "\\mat{A} &= \\softmax{\\mat{B}},\\ \\mathrm{dim}=1 \\\\\n",
    "\\mat{O} &= \\mat{A}\\mat{V} \\ \\in\\set{R}^{m\\times d_v}.\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the softmax is applied per-row, and so each row $i$ of $\\mat{A}$ contains the attention weights for the $i$th query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also notice that in this formulation, we **input a sequence** of $m$ queries and get an **output sequence** of $m$ weighed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive attention based on an MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common type of attention mechanism uses an MLP to **learn** the similarity function $s(\\vec{k},\\vec{q})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this type of attention, the similarity function is \n",
    "\n",
    "$$\n",
    "s(\\vec{k},\\vec{q}) = \\vectr{v} \\tanh(\\mat{W}_k\\vec{k} + \\mat{W}_q\\vec{q}),\n",
    "$$\n",
    "\n",
    "where $\\mat{W}_k\\in\\set{R}^{h\\times d_k}$, $\\mat{W}_q\\in\\set{R}^{h\\times d_q}$ and $\\vec{v}\\in\\set{R}^{h}$ are trainable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice that we're adding projected versions of the key and query and applying a 2-layer MLP.\n",
    "- Both projections and the output layer are trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Seq2Seq language translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field\n",
    "\n",
    "# Common args for field objects\n",
    "field_args = dict(tokenize='spacy',\n",
    "                  init_token = '<sos>',\n",
    "                  eos_token = '<eos>',\n",
    "                  include_lengths=True,\n",
    "                  lower = True) \n",
    "\n",
    "# Field for processing German source\n",
    "src_field = Field(tokenizer_language=\"de_core_news_sm\", **field_args)\n",
    "\n",
    "# Field for processing English target\n",
    "tgt_field = Field(tokenizer_language=\"en_core_web_sm\", **field_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_valid, ds_test = Multi30k.splits(\n",
    "    root=data_dir, exts=('.de', '.en'), fields=(src_field, tgt_field)\n",
    ")\n",
    "\n",
    "VOCAB_MIN_FREQ = 2\n",
    "src_field.build_vocab(ds_train, min_freq=VOCAB_MIN_FREQ)\n",
    "tgt_field.build_vocab(ds_train, min_freq=VOCAB_MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source vocab size: 7855\n",
      "target vocab size: 5893\n"
     ]
    }
   ],
   "source": [
    "V_src = len(src_field.vocab)\n",
    "print(f'source vocab size: {V_src}')\n",
    "\n",
    "V_tgt = len(tgt_field.vocab)\n",
    "print(f'target vocab size: {V_tgt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample#0001:\n",
      "\tDE: mehrere männer mit schutzhelmen bedienen ein antriebsradsystem .\n",
      "\tEN: several men in hard hats are operating a giant pulley system .\n",
      "\n",
      "sample#0010:\n",
      "\tDE: eine ballettklasse mit fünf mädchen , die nacheinander springen .\n",
      "\tEN: a ballet class of five girls jumping in sequence .\n",
      "\n",
      "sample#0100:\n",
      "\tDE: männliches kleinkind in einem roten hut , das sich an einem geländer festhält .\n",
      "\tEN: toddler boy in a red hat holding on to some railings .\n",
      "\n",
      "sample#1000:\n",
      "\tDE: ein junger mann in einem weißen hemd , der tomaten schneidet .\n",
      "\tEN: a young man in a white shirt cutting tomatoes .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in ([1, 10, 100, 1000]):\n",
    "    example = ds_train[i]\n",
    "    src = str.join(\" \", example.src)\n",
    "    tgt = str.join(\" \", example.trg)\n",
    "    print(f'sample#{i:04d}:\\n\\tDE: {src}\\n\\tEN: {tgt}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<sos>', '<eos>', 'a', '.', 'in', 'the', 'on', 'man']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_field.vocab.itos[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<sos>', '<eos>', '.', 'ein', 'einem', 'in', 'eine', ',']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_field.vocab.itos[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>=0, <pad>=1\n"
     ]
    }
   ],
   "source": [
    "UNK_TOKEN = tgt_field.vocab.stoi['<unk>']\n",
    "PAD_TOKEN = tgt_field.vocab.stoi['<pad>']\n",
    "\n",
    "print(f'<unk>={UNK_TOKEN}, <pad>={PAD_TOKEN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "dl_train, dl_valid, dl_test = BucketIterator.splits((ds_train, ds_valid, ds_test), batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0:\n",
      " tensor([[   2,    2,    2,    2],\n",
      "        [4658,    5,    5,    5],\n",
      "        [ 466,   66,  502,   13],\n",
      "        [   5,   25,   68, 1249],\n",
      "        [2103,   38,   19,   11],\n",
      "        [   7,    7,  626,    6],\n",
      "        [  14,    6,   12,  194],\n",
      "        [  78,  869,   14,   42],\n",
      "        [   0,    4, 3258,   19],\n",
      "        [   4,    3,    4,  295],\n",
      "        [   3,    1,    3,    7],\n",
      "        [   1,    1,    1,   14],\n",
      "        [   1,    1,    1,  152],\n",
      "        [   1,    1,    1,    4],\n",
      "        [   1,    1,    1,    3]]) torch.Size([15, 4])\n",
      "x0_len:\n",
      " tensor([11, 10, 11, 15]) torch.Size([4])\n",
      "y0:\n",
      " tensor([[   2,    2,    2,    2],\n",
      "        [2484,    4,    4,    4],\n",
      "        [ 456,   53,  510,    9],\n",
      "        [ 521,   33,  380,   10],\n",
      "        [ 201,   37,    4, 1292],\n",
      "        [  21,    6,  319,    4],\n",
      "        [2009,    4,    8,  180],\n",
      "        [   6,  123,    4,   60],\n",
      "        [   4,  135, 1276,    4],\n",
      "        [  59,    5, 1133,  309],\n",
      "        [ 402,    3,    5,    6],\n",
      "        [2917,    1,    3,    4],\n",
      "        [   5,    1,    1,  101],\n",
      "        [   3,    1,    1,    5],\n",
      "        [   1,    1,    1,    3]]) torch.Size([15, 4])\n",
      "y0_len:\n",
      " tensor([14, 11, 12, 15]) torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# dataloader returns a Batch object with .src and .trg attributes\n",
    "b0 = next(iter(dl_train))\n",
    "\n",
    "# The .src/.trg attributes contain tuples of sequences and their lengths\n",
    "# Get batches of sequences \n",
    "x0, x0_len = b0.src\n",
    "y0, y0_len =  b0.trg\n",
    "\n",
    "print('x0:\\n', x0, x0.shape)\n",
    "print('x0_len:\\n', x0_len, x0_len.shape)\n",
    "print('y0:\\n', y0, y0.shape)\n",
    "print('y0_len:\\n', y0_len, y0_len.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Seq2SeqEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_layers, h_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.Embedding converts from token index to dense tensor\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # PyTorch multilayer GRU RNN\n",
    "        self.rnn = nn.GRU(embedding_dim, h_dim, num_layers=num_layers, dropout=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (S, B) Note batch dim is not first!\n",
    "        S, B = x.shape\n",
    "        embedded = self.embedding(x) # embedded shape: (S, B, E)\n",
    "        \n",
    "        # GRU second output returns last hidden state from each layer (L, B, H)\n",
    "        _, ht = self.rnn(embedded)\n",
    "        return ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the encoder with a batch of English sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.3746, -0.3933, -0.9711,  ...,  0.5722,  0.0188, -0.8133],\n",
      "         [ 0.4752, -0.5076, -0.9792,  ...,  0.5750,  0.0752, -0.8176],\n",
      "         [ 0.3804, -0.3843, -0.9658,  ...,  0.5593,  0.0961, -0.8170],\n",
      "         [ 0.3967,  0.7725,  0.0905,  ...,  0.3933,  0.0243, -0.3996]],\n",
      "\n",
      "        [[-0.1673, -0.1089,  0.1749,  ..., -0.0644,  0.2285,  0.4482],\n",
      "         [-0.3082,  0.0909,  0.1338,  ..., -0.1165,  0.3426,  0.5760],\n",
      "         [-0.1632, -0.0732,  0.3275,  ..., -0.0462,  0.2715,  0.5270],\n",
      "         [ 0.4978,  0.2254,  0.0398,  ..., -0.1105, -0.0534, -0.2913]]],\n",
      "       grad_fn=<StackBackward>) torch.Size([2, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "EMB_DIM = 256\n",
    "HID_DIM = 128\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "enc = Seq2SeqEncoder(V_src, EMB_DIM, NUM_LAYERS, HID_DIM)\n",
    "ht = enc(x0)\n",
    "print(ht, ht.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_layers, h_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # nn.Embedding converts from token index to dense tensor\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # PyTorch multilayer GRU RNN\n",
    "        self.rnn = nn.GRU(embedding_dim, h_dim, num_layers=num_layers, dropout=dropout)\n",
    "        \n",
    "        # Output layer, note the output dimension!\n",
    "        self.out_fc = nn.Linear(h_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, context):\n",
    "        # x shape: (S, B)\n",
    "        # context: (L, B, H) the last hidden state from the encoder\n",
    "        S, B = x.shape\n",
    "        embedded = self.embedding(x) # embedded shape: (S, B, E)\n",
    "        \n",
    "        # Note initial hidden state is the context vector\n",
    "        # GRU first output returns all hidden states from last layer (S, B, H)\n",
    "        h, ht = self.rnn(embedded, context)\n",
    "        \n",
    "        # Transpose to shape (B, S, H)\n",
    "        # Project H back to the vocab size, to get a score per word\n",
    "        out = self.out_fc(h.transpose(0, 1))\n",
    "        \n",
    "        # Out shapes: (B, S, V) and (L, B, H)\n",
    "        return out, ht"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the decoder with the corresponding batch of German sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.0604e-02, -1.2762e-01, -6.7396e-02,  ..., -8.7237e-03,\n",
      "           2.5662e-01,  3.7591e-02],\n",
      "         [-4.8428e-02, -1.2934e-01, -3.0245e-02,  ..., -2.0415e-03,\n",
      "           2.6859e-01,  1.3676e-01],\n",
      "         [-1.4757e-01, -9.3835e-02,  9.3423e-02,  ..., -7.7210e-02,\n",
      "           2.7683e-01,  1.1571e-01],\n",
      "         ...,\n",
      "         [-2.8553e-01,  5.3404e-02,  1.2975e-01,  ..., -1.1142e-01,\n",
      "           2.4422e-01,  8.8269e-02],\n",
      "         [-2.3283e-01,  1.2989e-01,  1.2164e-01,  ..., -7.5037e-02,\n",
      "           1.9227e-01,  7.7251e-02],\n",
      "         [-1.3825e-01,  2.3209e-01,  1.5127e-02,  ..., -1.2760e-02,\n",
      "           1.8429e-01,  3.4283e-02]],\n",
      "\n",
      "        [[ 5.8610e-02, -1.0057e-01, -1.1802e-01,  ...,  1.0844e-01,\n",
      "           2.3447e-01,  7.1722e-02],\n",
      "         [-4.2139e-02, -5.2833e-02, -1.1637e-01,  ...,  8.6654e-02,\n",
      "           1.7782e-01,  2.0888e-02],\n",
      "         [-8.1408e-02,  4.9701e-02, -6.9711e-02,  ..., -2.2777e-02,\n",
      "           2.8267e-01, -6.7203e-03],\n",
      "         ...,\n",
      "         [-1.8613e-01,  2.8421e-01, -2.1063e-02,  ...,  1.3432e-01,\n",
      "           1.9191e-01,  6.5376e-03],\n",
      "         [-6.7525e-02,  1.8823e-01,  3.5835e-02,  ...,  1.3967e-01,\n",
      "           2.3738e-01,  1.9528e-02],\n",
      "         [-1.1597e-01,  2.8504e-01,  7.2414e-04,  ...,  1.4723e-01,\n",
      "           2.0547e-01,  2.5348e-02]],\n",
      "\n",
      "        [[ 1.7896e-02, -1.4433e-01, -4.8717e-02,  ..., -4.3769e-03,\n",
      "           2.0892e-01,  2.5083e-02],\n",
      "         [-9.5723e-02, -2.4426e-02, -6.8688e-02,  ..., -3.2653e-02,\n",
      "           2.8978e-01,  9.5839e-03],\n",
      "         [-2.1256e-01,  2.7054e-02,  2.9362e-03,  ...,  3.6060e-02,\n",
      "           2.1672e-01,  2.5072e-02],\n",
      "         ...,\n",
      "         [-1.9815e-01,  1.6209e-01,  6.3743e-05,  ...,  2.6363e-02,\n",
      "           1.7575e-01,  5.8466e-02],\n",
      "         [-1.0983e-01,  2.2883e-01, -3.7383e-02,  ...,  1.2027e-01,\n",
      "           2.0387e-01,  8.1567e-02],\n",
      "         [-1.1303e-01,  2.7074e-01, -1.7215e-02,  ...,  4.7854e-02,\n",
      "           2.0885e-01,  3.9976e-02]],\n",
      "\n",
      "        [[-1.4259e-02, -4.2428e-02, -1.1376e-01,  ..., -1.2337e-01,\n",
      "           2.2766e-01,  3.7472e-02],\n",
      "         [-9.4186e-02,  7.8266e-02, -1.5517e-01,  ..., -7.8328e-02,\n",
      "           2.1913e-01,  6.2962e-02],\n",
      "         [-1.9088e-01,  1.4151e-01, -9.8911e-02,  ..., -8.3536e-02,\n",
      "           2.8306e-01,  4.5400e-02],\n",
      "         ...,\n",
      "         [-1.6707e-01,  2.5900e-02, -1.1938e-01,  ...,  2.0946e-02,\n",
      "           1.8359e-01,  2.8283e-02],\n",
      "         [-2.1120e-01,  3.8658e-02, -8.4060e-02,  ...,  1.2217e-02,\n",
      "           1.7978e-01,  6.8199e-02],\n",
      "         [-1.7589e-01,  9.9371e-02, -3.3679e-02,  ...,  6.7488e-02,\n",
      "           1.4926e-01,  5.6560e-02]]], grad_fn=<AddBackward0>) torch.Size([4, 15, 5893])\n"
     ]
    }
   ],
   "source": [
    "dec = Seq2SeqDecoder(V_tgt, EMB_DIM, NUM_LAYERS, HID_DIM)\n",
    "yhat, _ = dec(y0, ht) # note different S\n",
    "print(yhat, yhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embedding_dim, num_layers, h_dim, **kw):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create encoder & decoder parts\n",
    "        self.enc = Seq2SeqEncoder(src_vocab_size, embedding_dim, num_layers, h_dim, **kw)\n",
    "        self.dec = Seq2SeqDecoder(tgt_vocab_size, embedding_dim, num_layers, h_dim, **kw)\n",
    "    \n",
    "    def forward(self, x_src, x_tgt, teacher_forcing=1.):\n",
    "        # input shapes: (S1, B), (S2, B)\n",
    "        S2, B = x_tgt.shape\n",
    "        \n",
    "        # Forward pass through encoder\n",
    "        # context is (L, B, H)\n",
    "        context = self.enc(x_src)\n",
    "        \n",
    "        # Loop over tokens in target sequence and feed them to the decoder\n",
    "        dec_input = x_tgt[0, range(B)] # (1, B)\n",
    "        dec_outputs = []\n",
    "        for t in range(1, S2):\n",
    "            # (B, 1, V)\n",
    "            dec_output, context = self.dec(dec_input, context)\n",
    "            dec_outputs.append(dec_output)\n",
    "            \n",
    "            # Next input, take either:\n",
    "            # - next target token (teacher forcing)\n",
    "            # - highest scoring output (greedy prediction of next token)\n",
    "            if teacher_forcing > torch.rand(1).item():\n",
    "                dec_input = x_tgt[t, range(B)] # (1, B)\n",
    "            else:\n",
    "                dec_input = torch.argmax(dec_output.squeeze(), dim=1, keepdim=True).transpose() # (1, B)\n",
    "            \n",
    "        \n",
    "        # Stack decoder outputs from all timesteps\n",
    "        y_hat = torch.cat(dec_outputs, dim=1)\n",
    "        \n",
    "        # Output shape: (B, S, V)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-ee086a7216fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mseq2seq_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_tgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMB_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHID_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2seq_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-dc337efc93b9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_src, x_tgt, teacher_forcing)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# (B, 1, V)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mdec_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mdec_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236781/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-99-41b17e07492c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# x shape: (S, B)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# context: (L, B, H) the last hidden state from the encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# embedded shape: (S, B, E)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "seq2seq_model = Seq2Seq(V_src, V_tgt, EMB_DIM, NUM_LAYERS, HID_DIM)\n",
    "yhat = seq2seq_model(x0, y0)\n",
    "print(yhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=torch.tensor([[1,3,2],[3,4,5]])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(t, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================\n",
      "                          Kernel Shape   Output Shape   Params  Mult-Adds\n",
      "Layer                                                                    \n",
      "0_enc.Embedding_embedding  [256, 7855]   [15, 4, 256]  2010880    2010880\n",
      "1_enc.GRU_rnn                        -   [15, 4, 128]   247296     245760\n",
      "2_dec.Embedding_embedding  [256, 5893]   [15, 4, 256]  1508608    1508608\n",
      "3_dec.GRU_rnn                        -   [15, 4, 128]   247296     245760\n",
      "4_dec.Linear_out_fc        [128, 5893]  [4, 15, 5893]   760197     754304\n",
      "-------------------------------------------------------------------------\n",
      "                       Totals\n",
      "Total params          4774277\n",
      "Trainable params      4774277\n",
      "Non-trainable params        0\n",
      "Mult-Adds             4765312\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "summary(seq2seq_model, x0, y0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_seq2seq(model, dl_train):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    # Note: We don't compute loss from padding tokens!\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "    \n",
    "    losses = []\n",
    "    for idx_batch, batch in enumerate(dl_train):\n",
    "        x, x_len = batch.src\n",
    "        y, y_len =  batch.trg\n",
    "        \n",
    "        # Forward pass\n",
    "        y_hat = model(x, y)\n",
    "        B, S, V = y_hat.shape\n",
    "        \n",
    "        # y[:,i] is <sos>, w_1, w_2, ..., w_k, <eos>, <pad>, ...\n",
    "        # y_hat is   w_1', w_2', ..., w_k', <eos>, <pad>, ...\n",
    "        # based on the above, get grout truth y\n",
    "        y_gt = y[1:, :].reshape(B*(S-1))\n",
    "        y_hat = y_hat[:,:-1,:].reshape(B*(S-1), V)\n",
    "        \n",
    "        # Calculate loss compared to ground truth\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(y_hat, y_gt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print(f'[{idx_batch+1}/{len(dl_train)}] loss: {loss.item():.4f} ' )\n",
    "        print('.', end='')\n",
    "        losses.append(loss)\n",
    "        if idx_batch == 100:\n",
    "            # print('This is just a demo, stopping...')\n",
    "            break\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................."
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 80\n",
    "dl_train, dl_valid, dl_test = BucketIterator.splits((ds_train, ds_valid, ds_test), batch_size = BATCH_SIZE)\n",
    "seq2seq_model = Seq2Seq(V_src, V_tgt, EMB_DIM, NUM_LAYERS, HID_DIM)\n",
    "losses = train_seq2seq(seq2seq_model, dl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15a70ebd0>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Image credits**\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from:\n",
    "\n",
    "- Zhang et al., Dive into Deep Learning, 2019\n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017\n",
    "- Andrej Karpathy, http://karpathy.github.io\n",
    "- MIT 6.S191\n",
    "- Stanford cs231n\n",
    "- K. Xu et al. 2015, https://arxiv.org/abs/1502.03044"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
