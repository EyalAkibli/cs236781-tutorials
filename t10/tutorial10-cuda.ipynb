{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\Tr}[0]{^\\top}\n",
    "\\newcommand{\\softmax}[1]{\\mathrm{softmax}\\left({#1}\\right)}\n",
    "$$\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "# Tutorial 10: CUDA Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- TODO\n",
    "- The CUDA programming model\n",
    "- Implementing CUDA kernels with `numba`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CUDA programming model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA is a parallel programming model and software environment that leverages the computational resources of NVIDIA GPU's for general-purpose numeric computation.\n",
    "\n",
    "It provides compilers, programming-language extensions, optimized software libraries and developer tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CUDA defines a programming model and a memory model\n",
    "- CUDA programs run 1000's of threds on on 100's of physical cores\n",
    "- Defines extensions to C language to write GPU code (But here we'll use Python :)\n",
    "- Allows heterogeneous computation:\n",
    "    - CPU runs sequential operations and invokes GPU\n",
    "    - GPU runs massively-parallel work\n",
    "    - Both can run concurrently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Device**: The GPU  \n",
    "**Host**: The machine controlling the GPU\n",
    "\n",
    "| Heterogeneous computing | Host-device communication |\n",
    "| --| --|\n",
    "| <center><img src=\"img/hetero.png\" width=\"300\" /></center> | <center><img src=\"img/host_device.png\" width=\"700\" /></center>|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A **Kernel** is a function that is *called from host* and *executes on device*\n",
    "- Generally, one kernel executes at a time on the entire device\n",
    "    - Actually, kernels can be queued into \"streams\"\n",
    "    - Kernels from different streams can overlap\n",
    "- A Kernel runs with using many concurrent threads\n",
    "- Each thread executes the *same code*\n",
    "\n",
    "<center><img src=\"img/kernel.png\" width=\"300\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel \"Geometry\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kernel launches as a 1d or 2d-**grid** of **thread blocks**\n",
    "- Each **block** contains multiple threads arranged in a 1d, 2d or 3d configuration\n",
    "- Threads within a block can synchronize (barrier) and share memory\n",
    "- Each thread has a **unique id** that is mostly used for\n",
    "    - Selecting in/out data (computing memory access locations)\n",
    "    - Control-flow decisions\n",
    "    \n",
    "<center><img src=\"img/kernel_geom.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that multi-dimensional grids and blocks are just for the convenience of the programmer.\n",
    "- Helps implement algorithms for 2d and 3d data\n",
    "- Nothing actually changes in the hardware execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is a kernel implemented and launched?\n",
    "\n",
    "- The CUDA C-extensions allow the programmer to define which code is compiled for CPU or GPU.\n",
    "- A special syntax (`<<< >>>`) allows the definition of kernel geometry when launching it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```c\n",
    "__global__ void MyKernel() {}      // call from host, execute on GPU\n",
    "__device__ float MyDeviceFunc() {} // call from GPU, execute on GPU\n",
    "__host__ int HostFunc() {}         // call from host, execute on host\n",
    "\n",
    "dim3 dimGrid(100, 50);  // 5000 thread blocks in the grid, in a 2D layout\n",
    "dim3 dimBlock(4, 8, 8); // 256 threads per block, in a 3D layout\n",
    "MyKernel <<< dimGrid, dimBlock >>> (...); // Launch kernel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Practically zero creation and switching overhead\n",
    "- Can launch kernels with thousands of threads, many more than physical cores (**\"oversubscribed\"**)\n",
    "    - When a thread is blocked due to memory latency, it's instantly swapped out with another waiting thread\n",
    "    - Instant thread switching hides memory latency\n",
    "- Even very simple kernels can generate performance benefit with massive parallelization\n",
    "- Scheduled together in \"warps\": groups of (usually 32) threads performing the same instruction (SIMT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Thread IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CUDA runtime provides **special variables** for determining the geometry of the currently executing kernel:\n",
    "- `gridDim`: Dimensions of the grid, in blocks. Can be 1d or 2d.\n",
    "- `blockDim`: Dimension of the block, in threads. Can be 1d, 2d, or 3d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CUDA runtime provides **special variables** for calculating the unique thread id:\n",
    "- `blockIdx`: Index of current block, within the grid. Can be 1d or 2d.\n",
    "- `threadIdx`: Index of current thread, within the block. Can be 1d, 2d, or 3d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: How can we use the above variables to obtain the unique thread id?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unique thread id for a 1d kernel geometry can be obtained with `blockIdx.x * blockDim.x + threadIdx.x`.\n",
    "\n",
    "<center><img src=\"img/thread_id_1d.png\" width=\"800\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key idea of CUDA\n",
    "\n",
    "- Write a single-threaded program with the **thread id** as a parameter.\n",
    "- Use thread id to select a subset of data to process.\n",
    "- Launch many threads, so that together they cover the entire dataset.\n",
    "- Code automatically to all available physical processors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Kernel transparently scale to device with a different number of physical processors\n",
    "- A thread block is executed within a one \"streaming multiprocessor\" (SM)\n",
    "    - Each SM has many thread processors AKA CUDA cores\n",
    "\n",
    "<center><img src=\"img/sm.png\" width=\"300\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hardware schedules thread blocks on any available multiprocessor\n",
    "- Source code defining kernel \"geometry\" stays the same regardless of hardware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, the same Kernel configuration can be launched on devices with a different number of multiprocessors:\n",
    "<center><img src=\"img/block_scheduling.png\" width=\"1000\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different types of memory are available to device threads.\n",
    "\n",
    "The most important ones are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Registers\n",
    "    - Per-thread access\n",
    "    - On chip $\\rightarrow$ extremely fast\n",
    "    - Persisted until thread terminates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Thread-local memory\n",
    "    - Stores per-thread local variables that cannot fit in the register memory\n",
    "    - Located in DRAM $\\rightarrow$ extremely slow\n",
    "    - Persisted until thread terminates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Shared memory\n",
    "    - Shared between threads in the same thread block\n",
    "    - Used for collaboration between threads in the same block\n",
    "    - On chip $\\rightarrow$ very fast\n",
    "    - Persisted until end of block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Global memory\n",
    "    - Can be access by any thread in any thread block\n",
    "    - Used to copy to/from host\n",
    "    - Located in DRAM $\\rightarrow$ extremely slow\n",
    "    - Persisted for the life of the application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Thread-local|Shared|Global|\n",
    "|-|-|-|\n",
    "|<img src=\"img/mem_local.png\" width=\"330\">| <img src=\"img/mem_shared.png\" width=\"273\">|<img src=\"img/mem_global.png\" width=\"500\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules of thumb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many blocks?\n",
    "- Should occupy every SM $\\rightarrow$ At least one block per SM\n",
    "- Should have something to run on SM if current block is waiting (e.g. sync) $\\rightarrow$ At least two blocks per SM\n",
    "- Should scale with same code if we upgrade hardware $\\rightarrow$ Many blocks per SM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many threads?\n",
    "- Many threads $\\rightarrow$ hides global memory latency\n",
    "- Too many threads $\\rightarrow$ exhaust registers and shared memory\n",
    "- Multiple of warp size\n",
    "- Typical selection: 64 to 256 per block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing CUDA Kernels with `numba`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is `numba`?\n",
    "\n",
    "Numba is a **just-in-time** (JIT) **function compiler**, focused on **numerical python**.\n",
    "It can be used to accelerate python code by generating efficient, **type-specialized** machine code.\n",
    "\n",
    "Numba supports all major OSes and a wide range of hardware (Intel x86/64, NVIDIA CUDA, ARM).\n",
    "It's developed and actively maintained by Anaconda Inc., and considered production ready.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explain the terms we used above:\n",
    "\n",
    "**Just-in-time**: Functions are compiled the first time they're called.  The compiler therefore knows the argument types.\n",
    "\n",
    "Bonus: This also allows Numba to be used interactively in a Jupyter notebook :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function compiler**:  Numba compiles Python functions, not entire applications.\n",
    "\n",
    "Numba does not replace the Python interpreter, it effectively transforms a function into a usually faster function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical python**: Numba supports only a subset of the python language. It works well with numerical types such as `int`, `float`, and `complex`, functions from the `math` and `cmath` modules and with `numpy` arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type-specialized**: Numba speeds up your function by generating a specialized implementation for the specific data types you are using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"img/numba_flowchart.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First steps with `numba` on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a \"Hello World\" style example: A trivial function that increments an array by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def inc_cpu(a):\n",
    "    for i in range(len(a)):\n",
    "        a[i] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `numba.jit` decorator to wrap our code in a `numba` object that will JIT and cache it when called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CPUDispatcher(<function inc_cpu at 0x7f05922c6e60>)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The inc_cpu variable no longer points to a regular python function, but a callable wrapper.\n",
    "inc_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the `nopython` option?\n",
    "- If `nopython=True`, `numba` will try to compile the entire function so that it can be run completely without the Python interpreter. This is usually what you want.\n",
    "- Otherwise, `numba` will try to compile the entire function, but if there are unsupported operations or types it will try to only extract loops and compile them as separate functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a million-element array and see how fast the python interpreter is using `%timeit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((10**6,), dtype=np.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.96 s ± 10.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([8., 8., 8., ..., 8., 8., 8.], dtype=float32)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run as regular python code (interpreted)\n",
    "%timeit inc_cpu.py_func(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had to call the function with `.py_func` to get the original function (before wrapping with the jitter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call it though the wrapper to time the compiled version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 µs ± 148 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([8119., 8119., 8119., ..., 8119., 8119., 8119.], dtype=float32)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run as jit-compiled machine code\n",
    "%timeit inc_cpu(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's about 5 orders of magnitude faster! Not bad for just adding a decorator function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 µs ± 82.3 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Run using numpy add(), this is like a + 1 but without allocating output array\n",
    "%timeit np.add(a, 1, out=a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, we get results similar to `numpy`'s optimized C code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note about benchmarking:\n",
    "\n",
    "The first time we called `inc_cpu` we paid a overhead price for the compilation.\n",
    "However, the `%timeit` magic returns the best result from multiple runs, so our results do not show this overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First steps with `numba` on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 CUDA devices\n",
      "id 0    b'GeForce RTX 2080 Ti'                              [SUPPORTED]\n",
      "                      compute capability: 7.5\n",
      "                           pci device id: 0\n",
      "                              pci bus id: 15\n",
      "Summary:\n",
      "\t1/1 devices are supported\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numba import cuda\n",
    "\n",
    "cuda.detect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rewrite our \"Hello World\" example as a CUDA kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def inc_gpu(a):\n",
    "    idx = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    \n",
    "    # Notice:\n",
    "    # 1. No loop\n",
    "    # 2. We assume more threads than array elements\n",
    "    if idx < a.shape[0]:\n",
    "        a[idx] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets invoke this kernel with a specific geometry containing more threads than array elements (over 1M threads!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182 µs ± 4.47 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([97341., 97341., 97341., ..., 97341., 97341., 97341.], dtype=float32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocksize = 256\n",
    "gridsize = math.ceil(a.shape[0] / blocksize)\n",
    "\n",
    "# Copy data to GPU memory\n",
    "d_a = cuda.to_device(a)\n",
    "\n",
    "# Run as a kernel on GPU\n",
    "# Note that we much synchronize to benchmark properly\n",
    "%timeit inc_gpu[gridsize, blocksize](d_a); cuda.synchronize()\n",
    "\n",
    "# Copying data back from device will also synchronize, i.e. wait for kernel to complete\n",
    "a = d_a.copy_to_host()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def elementwise_mult_kernel(a, b, out):\n",
    "    threads_per_block = cuda.blockDim.x\n",
    "    thread_idx_in_block = cuda.threadIdx.x\n",
    "    block_idx = cuda.blockIdx.x\n",
    "    \n",
    "    thread_idx_unique = thread_idx_in_block + block_idx * threads_per_block\n",
    "    \n",
    "    if thread_idx_unique >= len(a):\n",
    "        return\n",
    "    \n",
    "    i = thread_idx_unique \n",
    "    out[i] = a[i] * b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((10**6,), dtype=np.float32) * 2\n",
    "b = np.ones((10**6,), dtype=np.float32) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 6., 6., ..., 6., 6., 6.], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = np.zeros_like(a)\n",
    "elementwise_mult_kernel[1000, 1000](a, b, out)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 6., 6., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = np.zeros_like(a)\n",
    "elementwise_mult_kernel[10, 10](a, b, out)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 6., 6., 6., 6., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[95:105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we cover all data, regardless of kernel dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def elementwise_mult_kernel_2(a, b, out):\n",
    "    threads_per_block = cuda.blockDim.x\n",
    "    num_blocks = cuda.gridDim.x\n",
    "    \n",
    "    thread_idx_in_block = cuda.threadIdx.x\n",
    "    block_idx = cuda.blockIdx.x\n",
    "    \n",
    "    thread_idx_unique = thread_idx_in_block + block_idx * threads_per_block\n",
    "    \n",
    "    start = thread_idx_unique \n",
    "    end = len(a)\n",
    "    stride = threads_per_block * num_blocks # jump over all threads, in case we have more data than threads\n",
    "    \n",
    "    for i in range(start, end, stride):\n",
    "        out[i] = a[i] * b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 6., 6., ..., 6., 6., 6.], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = np.zeros_like(a)\n",
    "elementwise_mult_kernel_2[10, 10](a, b, out)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul_kernel(a, b, out):\n",
    "    i, j = cuda.grid(2)\n",
    "    imax, jmax = cuda.gridsize(2)\n",
    "    \n",
    "    for k in range(b.shape[0]):\n",
    "        out[i, j] += a[i,k] * b[k,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((2, 5), dtype=np.float32) * 2\n",
    "b = np.ones((5, 2), dtype=np.float32) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30., 30.],\n",
       "       [30., 30.]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = np.zeros((2,2), dtype=np.float32)\n",
    "matmul_kernel[1, (10, 10)](a, b, out)\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thread cooperation**\n",
    "\n",
    "Sometimes it's necessary for threads to cooperate, not everything can be parallel.\n",
    "- Unlimited cooperation among thousands of threads is not scalable performance wise\n",
    "- Solution: Only threads within same **block** can share memory\n",
    "- Balance between scalability and cooperation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
