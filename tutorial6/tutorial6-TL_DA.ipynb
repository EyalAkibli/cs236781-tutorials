{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\cset}[1]{\\mathcal{#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "\n",
    "# CS236605: Deep Learning\n",
    "# Tutorial 6: Transfer Learning and Domain Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- Transfer learning context(s)\n",
    "- Leveraging pre-trained models\n",
    "- Unsupervised domain adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.size'] = 20\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The supervised learning context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have a labeled dataset of $N$ labelled samples: $\\left\\{ (\\vec{x}^i,y^i) \\right\\}_{i=1}^N$, where\n",
    "- $\\vec{x}^i = \\left(x^i_1, \\dots, x^i_D\\right) \\in \\mathcal{X}$  is a **sample** or **feature vector**.\n",
    "- $y^i \\in \\mathcal{Y}$ is the **label**.\n",
    "- For classification with $C$ classes, $\\mathcal{Y} = \\{0,\\dots,C-1\\}$, so each $y^i$ is a **class label**.\n",
    "- Usually we assume each labeled sample $(\\vec{x}^i,y^i)$\n",
    "  is drawn from a joint distribution\n",
    "  $$P(\\rvec{X}, \\rvar{Y})=P(\\rvec{X})\\cdot P(\\rvar{Y}|\\rvec{X})$$\n",
    "    - We assume some marginal sample distribution $P(\\rvar{X})$ exists.\n",
    "    - We want to learn $P(\\rvar{Y}|\\rvec{X})$ from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So far, we considered mostly the traditional **supervised learning** setting:\n",
    "\n",
    "We assumed the **train** and **test** (which is supposed to represent future unseen data)\n",
    "sets are both from the same **distribution** and both labeled.\n",
    "\n",
    "We were able to assume this since we wanted to solve one task with one dataset, and we could\n",
    "therefore split our dataset into such sets.\n",
    "\n",
    "What happens when this is not the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transfer learning\n",
    "\n",
    "In the real world, we often don't have the perfect training set for our problem.\n",
    "\n",
    "What should we do when the supervised learning assumption is invalid?\n",
    "\n",
    "<img src=\"img/transfer_learning_digits.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Domains, targets and tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets start with some definitions to explain the problem.\n",
    "\n",
    "- Imagine we have a **feature space**, $\\mathcal{X}$\n",
    "    - For example, $\\mathcal{X}$ is the space of color images of size 32x32, each pixel in the range 0-255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10^7398.1131734380015\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# size of this \"limited\" feature space\n",
    "print(f'10^{math.log10(256**(32**2*3))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- As usual, we have a training set $X=\\{\\vec{x}^{(i)}\\}_{i=1}^{N},\\ \\vec{x}^{(i)}\\in\\cset{X}$.\n",
    "    - For example, CIFAR-10\n",
    "    \n",
    "    <img src=\"img/cifar10.png\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- There exists some **probability distribution** $P(X)$ (aka $P_{X}(\\vec{x})$) over our training set.\n",
    "    - Note that we don't care about the distribution over $\\cset{X}$.\n",
    "    - For example, if $X$ is CIFAR-10, the probability of an all-black image should be very low\n",
    "    - If classes are unbalanced, much different probabilities for members of large and small classes\n",
    "    \n",
    "    <img src=\"img/data_dist.jpg\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Our **label space**, $\\cset{Y}$ includes the possible labels for sample in our problem.\n",
    "    - For example $\\cset{Y}=\\{0,1\\}$ in binary classification.\n",
    "- We may have also $Y = \\{y^{(i)}\\}_{i=1}^{N}$, the set of labels for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We want to learn the target function $\\hat{y}=f(\\vec{x})$ which predicts a label given an image.\n",
    "    - From the probabilistic perspective, learn $P(\\hat{y}|\\vec{x})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Finally,\n",
    "- A learning **domain** $\\cset{D}$, is defined as $\\cset{D}=\\left\\{\\mathcal{X},P(X)\\right\\}$.\n",
    "- A learning **task** $\\cset{T}$ is defined as $\\cset{T}=\\{\\cset{Y},P(Y|X)\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transfer learning settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definition** (Pan & Yang, 2010):\n",
    "\n",
    "Given\n",
    "- A **source** domain $\\cset{D}_S$ and learning task $\\cset{T}_S$\n",
    "- A **target** domain $\\cset{D}_T$ and learning task $\\cset{T}_T$\n",
    "\n",
    "*Transfer learning* aims to improve the learning of the target function\n",
    "using *knowledge* in $\\cset{D}_S$ and $\\cset{T}_S$, when\n",
    "- $\\cset{D}_S \\neq \\cset{D}_T$, or\n",
    "- $\\cset{T}_S \\neq \\cset{T}_T$\n",
    "\n",
    "Usually also there are other constraints on the target domain, such as little or no labels available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When $\\cset{D}_S=\\cset{D}_T$ and $\\cset{T}_S=\\cset{T}_T$ we're in the regular supervised learning setting\n",
    "we have seen thus far.\n",
    "\n",
    "For example, splitting CIFAR-10 randomly into a train and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Same domain, different task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall, a learning **task** $\\cset{T}$ is defined as $\\cset{T}=\\{\\cset{Y},P(Y|X)\\}$.\n",
    "\n",
    "So there are two cases (not mutually exclusive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Case 1: The label spaces are different, $\\cset{Y}_S \\neq \\cset{Y}_T$\n",
    "\n",
    "For example, target domain has more classes.\n",
    "\n",
    "<img src=\"img/cifar10_100.png\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Case 2: The target conditional distributions are different, $P(Y_S|X_S)\\neq P(Y_T|X_T)$.\n",
    "\n",
    "This may be the case when the class-balance is very different in the source and target distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Same task, different domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall, a learning **domain** $\\cset{D}$, is defined as $\\cset{D}=\\left\\{\\mathcal{X},P(X)\\right\\}$.\n",
    "\n",
    "Again, two cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Case 1: Different feature spaces, $\\cset{X}_S \\neq \\cset{X}_T$.\n",
    "\n",
    "For example: $\\cset{X}_S$ is a space of grayscale images while $\\cset{X}_T$ is a space of color images;\n",
    "documents in different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Case 2: Different data distributions, $P(X_S)\\neq P(X_T)$.\n",
    "\n",
    "For example: source domain contains hand-drawn images, while target domain contains photographs;\n",
    "documents in the same language about different topics.\n",
    "\n",
    "<img src=\"img/tl_example.png\" width=\"400\"/>\n",
    "\n",
    "This is a very common scenario, and usually called **domain adaptation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "TL is a huge research field.\n",
    "\n",
    "<img src=\"img/pan_yang.png\" width=\"1000\" />\n",
    "\n",
    "In this tutorial we'll see two simple yet common examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1: Fine-tuning a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have trained trained a model in a source domain,\n",
    "and now we want to use it to speed up training for a different domain.\n",
    "\n",
    "In some applications, we may have have much less labeled data in the target domain, making it infeasible to train a deep model from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Common example: pre-train on ImageNet (1M+ images, 1000 classes), and then classify e.g. medical images.\n",
    "\n",
    "<img src=\"img/transfer-learning-medical.png\" width=\"700\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why would this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "CNNs capture hierarchical features, with deeper layers capturing higher-level, class-specific features\n",
    "(Zeiler & Fergus, 2013).\n",
    "\n",
    "<img src=\"img/zf1.png\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/zf2.png\" width=\"1400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, we can start from a pre-trained model and,\n",
    "- \"Fine-tune\" the convolutional filters, mainly in the deeper layers.\n",
    "- Change the classifier \"head\" (or completely remove it) to fit our task and train it from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision as tv\n",
    "\n",
    "# Load a deep CNN pretrained on ImageNet\n",
    "# Using ResNet18 just to reduce download size, use something deeper\n",
    "resnet18 = tv.models.resnet18(pretrained=True)\n",
    "resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Freeze all layers: disable gradient tracking\n",
    "for p in resnet18.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# \"Thaw\" last layer (or whatever is relevant for you)\n",
    "for p in resnet18.layer4.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "\n",
    "# Another way to freeze: zero learning rates for specific parameters\n",
    "opt = torch.optim.SGD([\n",
    "    dict(params=resnet18.layer1.parameters(), lr=0),\n",
    "    dict(params=resnet18.layer2.parameters(), lr=0),\n",
    "    dict(params=resnet18.layer3.parameters(), lr=0),\n",
    "    dict(params=resnet18.layer4.parameters(), lr=1e-4),\n",
    "    dict(params=resnet18.fc.parameters()),\n",
    "], lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Replace fully-connected part by some other classifier, e.g.\n",
    "\n",
    "cnn_features = resnet18.fc.in_features\n",
    "num_classes = 13\n",
    "\n",
    "resnet18.fc =  nn.Sequential(\n",
    "    nn.Linear(cnn_features, 100, bias=True),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, num_classes, bias=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as tvtf\n",
    "\n",
    "# Important nuance 1: need to scale our data same as ImageNet training data\n",
    "tf = tvtf.Compose([\n",
    "    tvtf.Resize(224),\n",
    "    tvtf.ToTensor(),\n",
    "    tvtf.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load our target domain data (CIFAR-10 used just as a simple example)\n",
    "ds_train = tv.datasets.CIFAR10(root=data_dir, download=True, train=True, transform=tf)\n",
    "ds_test = tv.datasets.CIFAR10(root=data_dir, download=True, train=False, transform=tf)\n",
    "\n",
    "batch_size = 8\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=True, num_workers=2)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3393,  0.3295, -0.1578,  0.0711,  0.3648,  0.0120,  0.1108, -0.2397,\n",
       "         -0.2669,  0.0617, -0.3212,  0.0277,  0.4796]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18(ds_train[0][0].unsqueeze(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Important nunance 2: Only parameters that track gradients can be passed into the optimizer\n",
    "params_non_frozen = filter(lambda p: p.requires_grad, resnet18.parameters())\n",
    "opt = optim.SGD(params_non_frozen, lr=0.05, momentum=0.9)\n",
    "\n",
    "# Finetuning usually means we want smaller than usual learning rates and \n",
    "# decaying them in order to keep improving the weights\n",
    "lr_sched = optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.05, patience=5,)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, loss_fn, opt, lr_sched, dl_train, dl_test):\n",
    "    # Same as regular classifier traning, just call lr_sched.step() every epoch.\n",
    "    # ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Unsupervised domain adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's consider a problem with different domains but an identical task:\n",
    "\n",
    "- Source domain: MNIST\n",
    "- Target domain: MNIST-M, a colored and textured version of MNIST\n",
    "\n",
    "Task in both cases is the usual 10-class digit classification.\n",
    "\n",
    "<img src=\"img/mnist_m.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Unsupervised** DA setting:\n",
    "We assume that there are **no available labels** for the target domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We need to force our CNN to learn features of the digis shapes only, not color distributions.\n",
    "\n",
    "Our approach, (based on Ganin et al. 2015):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Train a classifier for the **domain** of an image based on deep convolutonal features.\n",
    "- Try to maximize the loss of this classifier when training the CNN (**confusion loss**).\n",
    "- Simultaneously, minimize the classification loss on the source domain using the same convolutional features.\n",
    "- Train the digit classifier with source domain data, and the domain classifier with both domains' data.\n",
    "\n",
    "<img src=\"img/ganin_da.png\" width=\"1400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Source and target domain data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note: for the next block to run, you should manually [download](https://drive.google.com/open?id=0B_tExHiYS-0veklUZHFYT19KYjg) the MNIST-M dataset and unpack it into `data_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from tut6.data import MNISTMDataset\n",
    "\n",
    "image_size = 28\n",
    "batch_size = 4\n",
    "\n",
    "tf_source = tvtf.Compose([\n",
    "    tvtf.Resize(image_size),\n",
    "    tvtf.ToTensor(),\n",
    "    tvtf.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "])\n",
    "tf_target = tvtf.Compose([\n",
    "    tvtf.Resize(image_size),\n",
    "    tvtf.ToTensor(),\n",
    "    tvtf.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "ds_source = tv.datasets.MNIST(root=data_dir, train=True, transform=tf_source, download=True)\n",
    "dl_source = torch.utils.data.DataLoader(ds_source, batch_size)\n",
    "\n",
    "ds_target = MNISTMDataset(os.path.join(data_dir, 'mnist_m', 'mnist_m_train'),\n",
    "                          os.path.join(data_dir, 'mnist_m', 'mnist_m_train_labels.txt'),\n",
    "                         transform=tf_target)\n",
    "dl_target = torch.utils.data.DataLoader(ds_target, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tut6.plot_utils import dataset_first_n\n",
    "\n",
    "dataset_first_n(ds_source, 3, cmap='gray');\n",
    "dataset_first_n(ds_target, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our model will consist of three parts, as in the figure:\n",
    "- A \"deep\" CNN for image feature extraction (2x Conv, ReLU, MaxPool)\n",
    "- A digit-classification head (3x FC, ReLU)\n",
    "- A domain classification head (2x FC, ReLU), with **gradient reversal layer** (GRL).\n",
    "\n",
    "\n",
    "<img src=\"img/ganin_da2.png\" width=\"1400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall: GRL is no-op in forward pass, but applies $-\\lambda$ factor to gradient in the backward pass.\n",
    "\n",
    "How can we implement this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "# Autograd Function objects are what record operation history on tensors,\n",
    "# and define formulas for the forward and backprop.\n",
    "\n",
    "class GradientReversalFn(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        # Store context for backprop\n",
    "        ctx.alpha = alpha\n",
    "        \n",
    "        # Forward pass is a no-op\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Backward pass is just to -alpha the gradient\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        # Must return same number as inputs to forward()\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class DACNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=5),\n",
    "            nn.BatchNorm2d(64), nn.MaxPool2d(2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 50, kernel_size=5),\n",
    "            nn.BatchNorm2d(50), nn.Dropout2d(), nn.MaxPool2d(2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.class_classifier = nn.Sequential(\n",
    "            nn.Linear(50 * 4 * 4, 100), nn.BatchNorm1d(100), nn.Dropout2d(),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(100, 100), nn.BatchNorm1d(100),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(100, 10),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(50 * 4 * 4, 100), nn.BatchNorm1d(100),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(100, 2),\n",
    "            nn.LogSoftmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, grl_lambda=1.0):\n",
    "        # Handle single-channel input by expanding (repeating) the singleton dimention\n",
    "        x = x.expand(x.data.shape[0], 3, image_size, image_size)\n",
    "        \n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(-1, 50 * 4 * 4)\n",
    "        reverse_features = GradientReversalFn.apply(features, grl_lambda)\n",
    "        \n",
    "        class_pred = self.class_classifier(features)\n",
    "        domain_pred = self.domain_classifier(reverse_features)\n",
    "        return class_pred, domain_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Wait, but why let $\\lambda$ (`grl_lambda` in the code) change during training (e.g. every epoch)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In the beginning of training, the domain loss is extremely noisy since the CNN features are not good yet.\n",
    "- Therefore, lambda is gradulaly changed from 0 to 1 in the course of training.\n",
    "    $$\n",
    "    \\lambda_p = \\frac{2}{1+\\exp(-10\\cdot p)} -1,\n",
    "    $$\n",
    "    where $p\\in[0,1]$ is the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source domain:  torch.Size([4, 1, 28, 28]) torch.Size([4])\n",
      "target domain:  torch.Size([4, 3, 28, 28]) torch.Size([4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.3871, -2.0135, -2.6594, -2.3351, -2.6167, -2.5896, -1.9831, -2.3627,\n",
       "          -1.9416, -2.4720],\n",
       "         [-2.1184, -2.3197, -2.3218, -2.4503, -2.5570, -2.1713, -2.4280, -2.1596,\n",
       "          -2.3536, -2.2355],\n",
       "         [-2.2772, -2.3747, -2.3180, -2.3140, -2.6862, -2.6952, -1.7384, -2.6459,\n",
       "          -2.2026, -2.1704],\n",
       "         [-2.6372, -2.7048, -2.2429, -2.0046, -2.2479, -2.1448, -2.3381, -2.4679,\n",
       "          -2.3502, -2.1089]], grad_fn=<LogSoftmaxBackward>),\n",
       " tensor([[-0.8876, -0.5304],\n",
       "         [-0.8818, -0.5345],\n",
       "         [-0.9733, -0.4745],\n",
       "         [-0.6405, -0.7487]], grad_fn=<LogSoftmaxBackward>))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DACNN()\n",
    "\n",
    "x0_s, y0_s = next(iter(dl_source))\n",
    "x0_t, y0_t = next(iter(dl_target))\n",
    "\n",
    "print('source domain: ', x0_s.shape, y0_s.shape)\n",
    "print('target domain: ', x0_t.shape, y0_t.shape)\n",
    "\n",
    "model(x0_s)\n",
    "model(x0_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "n_epochs = 1\n",
    "\n",
    "# Setup optimizer as usual\n",
    "model = DACNN()\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "\n",
    "# Two losses functions this time\n",
    "loss_fn_class = torch.nn.NLLLoss()\n",
    "loss_fn_domain = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "dl_source = torch.utils.data.DataLoader(ds_source, batch_size)\n",
    "dl_target = torch.utils.data.DataLoader(ds_target, batch_size)\n",
    "\n",
    "# We'll train the same number of batches from both datasets\n",
    "max_batches = min(len(dl_source), len(dl_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0001 / 0001\n",
      "=================\n",
      "[1/58] class_loss: 2.3298 s_domain_loss: 0.7261 t_domain_loss: 0.6733 grl_lambda: 0.000 \n",
      "[2/58] class_loss: 2.2447 s_domain_loss: 0.7014 t_domain_loss: 0.6873 grl_lambda: 0.086 \n",
      "[3/58] class_loss: 2.1779 s_domain_loss: 0.6790 t_domain_loss: 0.7027 grl_lambda: 0.171 \n",
      "This is just a demo, stopping...\n"
     ]
    }
   ],
   "source": [
    "for epoch_idx in range(n_epochs):\n",
    "    print(f'Epoch {epoch_idx+1:04d} / {n_epochs:04d}', end='\\n=================\\n')\n",
    "    dl_source_iter = iter(dl_source)\n",
    "    dl_target_iter = iter(dl_target)\n",
    "\n",
    "    for batch_idx in range(max_batches):\n",
    "        optimizer.zero_grad()\n",
    "        # Training progress and GRL lambda\n",
    "        p = float(batch_idx + epoch_idx * max_batches) / (n_epochs * max_batches)\n",
    "        grl_lambda = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "\n",
    "        # Train on source domain\n",
    "        X_s, y_s = next(dl_source_iter)\n",
    "        y_s_domain = torch.zeros(batch_size, dtype=torch.long) # generate source domain labels\n",
    "\n",
    "        class_pred, domain_pred = model(X_s, grl_lambda)\n",
    "        loss_s_label = loss_fn_class(class_pred, y_s)\n",
    "        loss_s_domain = loss_fn_domain(domain_pred, y_s_domain)\n",
    "\n",
    "        # Train on target domain\n",
    "        X_t, _ = next(dl_target_iter) # ignore target domain class labels!\n",
    "        y_t_domain = torch.ones(batch_size, dtype=torch.long) # generate target domain labels\n",
    "\n",
    "        _, domain_pred = model(X_t, grl_lambda)\n",
    "        loss_t_domain = loss_fn_domain(domain_pred, y_t_domain)\n",
    "        \n",
    "        loss = loss_t_domain + loss_s_domain + loss_s_label\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'[{batch_idx+1}/{max_batches}] '\n",
    "              f'class_loss: {loss_s_label.item():.4f} ' f's_domain_loss: {loss_s_domain.item():.4f} '\n",
    "              f't_domain_loss: {loss_t_domain.item():.4f} ' f'grl_lambda: {grl_lambda:.3f} '\n",
    "             )\n",
    "        if batch_idx == 2:\n",
    "            print('This is just a demo, stopping...')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Image credits**\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from:\n",
    "\n",
    "- Pan & Yang, 2010, A Survey on Transfer Learning\n",
    "- Zeiler & Fergus, 2013, Visualizing and Understanding Convolutional Networks\n",
    "- Y. Ganin et al. 2015, Unsupervised Domain Adaptation by Backpropagation \n",
    "- M. Wulfmeier et al., https://arxiv.org/abs/1703.01461v2\n",
    "- Sebastian Ruder, http://ruder.io/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
